{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 2\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0708518, r0927391, r0925509, r0924356, r0912639</span>.","metadata":{"_cell_guid":"b47b15de-64a5-4fa9-a688-23d3efa9a2f4","_uuid":"0cc385a7-98f6-4883-96eb-7b89c7c9aa1c","papermill":{"duration":0.016533,"end_time":"2022-04-12T14:48:23.471825","exception":false,"start_time":"2022-04-12T14:48:23.455292","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Introduction and overview\nThis assignment consists of *three main parts*:\n* Image classification (Section 2)\n* Semantic segmentation (Section 3)\n* Adversarial attacks (Section 4)\n\nIn the first part, we train an end-to-end neural network for image classification. In the second part, we do the same for semantic segmentation. In the third part, we try to find and exploit the weaknesses of our classification and/or segmentation network. Finally, we reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Section 5).","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","papermill":{"duration":0.014397,"end_time":"2022-04-12T14:48:23.501501","exception":false,"start_time":"2022-04-12T14:48:23.487104","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.1 Deep learning\nWe start by loading the required packages for our notebook. We rely on [Tensorflow](https://www.tensorflow.org/) to build our deep learning architectures. A crash course on Tensorflow can be found [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO).","metadata":{"papermill":{"duration":0.014263,"end_time":"2022-04-12T14:48:23.530341","exception":false,"start_time":"2022-04-12T14:48:23.516078","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-15T12:32:03.901723Z","iopub.execute_input":"2023-05-15T12:32:03.902269Z","iopub.status.idle":"2023-05-15T12:32:03.938433Z","shell.execute_reply.started":"2023-05-15T12:32:03.902219Z","shell.execute_reply":"2023-05-15T12:32:03.935967Z"}}},{"cell_type":"code","source":"# Default packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:23.031856Z","iopub.execute_input":"2023-05-23T09:04:23.032807Z","iopub.status.idle":"2023-05-23T09:04:23.280158Z","shell.execute_reply.started":"2023-05-23T09:04:23.032748Z","shell.execute_reply":"2023-05-23T09:04:23.279094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deep learning specific packages\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\nfrom keras.applications import VGG16\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.backend import argmax\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\nfrom keras import backend as keras\nimport keras.losses\nfrom keras import regularizers #fixing overfitting with L2 regularization\nimport keras.backend as K\nfrom keras.preprocessing.image import *\n\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\n\nimport gc\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:23.282160Z","iopub.execute_input":"2023-05-23T09:04:23.282511Z","iopub.status.idle":"2023-05-23T09:04:31.341987Z","shell.execute_reply.started":"2023-05-23T09:04:23.282480Z","shell.execute_reply":"2023-05-23T09:04:31.340833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since training deep learning models is tedious and computationally expensive, we saved the weights of our models to a Kaggle dataset. In case one wants to train the models either way, toggle the next boolean.","metadata":{}},{"cell_type":"code","source":"TRAIN_MODELS = False\nLOAD_WEIGHTS = False","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:31.344070Z","iopub.execute_input":"2023-05-23T09:04:31.344946Z","iopub.status.idle":"2023-05-23T09:04:31.349633Z","shell.execute_reply.started":"2023-05-23T09:04:31.344912Z","shell.execute_reply":"2023-05-23T09:04:31.348398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 PASCAL VOC 2009\nFor this project, we will consider data from the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. The goal of using this dataset is to recongize objects embedded into realistic scenes, meaning that the images are not preprocessed to pre-segmented objects. Therefore, it is a supervised learning problem where the input data are images and the output data are the labels of the objects that are present in the image. There are twenty different objects present in the dataset:\n* Person: person\n* Animal: bird, cat, cow, dog, horse, sheep\n* Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\n* Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n\nIn this first section, we will consider the **classification** part of the competition, where the goal is to predict the presence/absence of a class in the images of the test set. In the second section, we consider the **segmentation** part of the competition, where the goal is instead to generate pixel-wise segmentations that determine the class of the object visible at each pixel (or whether the pixel is belonging to the background). \n\nLet us start by loading the data provided by the Kaggle competition.","metadata":{"papermill":{"duration":0.014416,"end_time":"2022-04-12T14:48:28.990998","exception":false,"start_time":"2022-04-12T14:48:28.976582","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Loading the training data\ntrain_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/train_set.csv', index_col=\"Id\")\nlabels = train_df.columns[0:20]\nlabels_str = labels.tolist()\ntrain_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\ntrain_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\nprint(\"The training set contains {} examples.\".format(len(train_df)))","metadata":{"_cell_guid":"1ce67f49-6bf6-4e5c-b5e4-576e893616a9","_uuid":"3b1c5fbb-757f-4349-b224-e281c540e1ad","papermill":{"duration":21.336481,"end_time":"2022-04-12T14:48:50.342062","exception":false,"start_time":"2022-04-12T14:48:29.005581","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T09:04:31.351395Z","iopub.execute_input":"2023-05-23T09:04:31.352196Z","iopub.status.idle":"2023-05-23T09:04:42.733554Z","shell.execute_reply.started":"2023-05-23T09:04:31.352099Z","shell.execute_reply":"2023-05-23T09:04:42.728483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the test data\ntest_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/test_set.csv', index_col=\"Id\")\ntest_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\ntest_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\nprint(\"The test set contains {} examples.\".format(len(test_df)))","metadata":{"papermill":{"duration":11.507733,"end_time":"2022-04-12T14:49:02.044233","exception":false,"start_time":"2022-04-12T14:48:50.536500","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T09:04:42.737767Z","iopub.execute_input":"2023-05-23T09:04:42.738512Z","iopub.status.idle":"2023-05-23T09:04:50.142283Z","shell.execute_reply.started":"2023-05-23T09:04:42.738467Z","shell.execute_reply":"2023-05-23T09:04:50.141160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we jump into the problem it is advised to consider the provided data and their meaning. The first row of the dataset is shown below:","metadata":{}},{"cell_type":"code","source":"train_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:50.143976Z","iopub.execute_input":"2023-05-23T09:04:50.144378Z","iopub.status.idle":"2023-05-23T09:04:53.749711Z","shell.execute_reply.started":"2023-05-23T09:04:50.144323Z","shell.execute_reply":"2023-05-23T09:04:53.748457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" The given dataframes contain 22 columns, where the first 20 correspond to the classes that are present in the Pascal VOC dataset. A value of $0$ denotes that the corresponding object is not present in the image, while a value of $1$ indicates that the object is present in the image.","metadata":{}},{"cell_type":"markdown","source":"As an extra, we use a colormap based on the labels of the Pascal dataset:","metadata":{}},{"cell_type":"code","source":"# Get discrete colorbar\ncmap = plt.cm.viridis\ncmaplist = [cmap(i) for i in range(cmap.N)]\n# create the new map\npascal_cmap = mpl.colors.LinearSegmentedColormap.from_list(\n    'Custom cmap', cmaplist, cmap.N)\npascal_norm = mpl.colors.BoundaryNorm(np.arange(0.5,20+1), cmap.N) ","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:53.751547Z","iopub.execute_input":"2023-05-23T09:04:53.751959Z","iopub.status.idle":"2023-05-23T09:04:53.768601Z","shell.execute_reply.started":"2023-05-23T09:04:53.751917Z","shell.execute_reply":"2023-05-23T09:04:53.767409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The remaining two columns provide the image and the segmentations. The segmentations have the same 2D shape as the images, but have only one channel. Here, the \"pixels\" consist of integers between 0 and 20, and denote whether a pixel is part of an object of the labels. We demonstrate this with an example:","metadata":{}},{"cell_type":"code","source":"# Get an example image\nind = 7\nexample_img = train_df.loc[ind][\"img\"]\nexample_seg = train_df.loc[ind][\"seg\"]\n# Show the image and the segmentation\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax1, ax2 = axs\n\n# Show the picture\nax1.imshow(example_img)\nax1.set_title(\"\\n\".join(f\"{label} (index {np.argwhere(labels==label)[0][0]})\" for label in labels if train_df.iloc[ind][label] == 1), fontsize=16)\n\n# Show segmentation with colorbar\nim = ax2.imshow(example_seg, cmap=pascal_cmap, norm=pascal_norm)\ncbar = fig.colorbar(im, ax=ax2, shrink=0.75, ticks=np.linspace(1, 20, 20))\ncbar.ax.set_yticklabels(labels)\nax2.set_title(\"Segmentation\", fontsize=16)\n\n# Remove ticks\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:53.770252Z","iopub.execute_input":"2023-05-23T09:04:53.771502Z","iopub.status.idle":"2023-05-23T09:04:54.245927Z","shell.execute_reply.started":"2023-05-23T09:04:53.771459Z","shell.execute_reply":"2023-05-23T09:04:54.244428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can also be convenient to see the **class distribution** in the provided dataset. Since images can have several class labels attached to them, as the example above shows, we just count the number of occurences of each class in the provided dataset and divide it by the size of the dataset, to see the percentage of occurence of each label. As the plot shows, there are a lot of images with persons. The other classes are more or less balanced balanced in distribution.","metadata":{}},{"cell_type":"code","source":"# Get class distributions\nclass_distr = np.array([np.sum(train_df[lab]) for lab in labels])/len(train_df)\n# Plot it\nplt.figure(figsize=(11,5))\nxt = [i for i in range(len(class_distr))]\nplt.plot(xt, class_distr, \"-o\", color=\"red\")\nplt.xticks(xt, labels=labels, rotation=90)\nplt.ylabel(\"Percentage of occurence in dataset\")\nplt.title(\"Distribution of classification classes\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:54.247240Z","iopub.execute_input":"2023-05-23T09:04:54.248565Z","iopub.status.idle":"2023-05-23T09:04:54.551630Z","shell.execute_reply.started":"2023-05-23T09:04:54.248508Z","shell.execute_reply":"2023-05-23T09:04:54.550633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now do the same for the segmentation","metadata":{}},{"cell_type":"code","source":"def counts_seg(df):\n    seg_distr = np.zeros(21)\n    total_nb_pixels = 0\n    for seg in df[\"seg\"]:\n        # Get the counts of the segmentation pixels:\n        unique, counts = np.unique(seg, return_counts=True)\n        # Add counts to correct class\n        for i, label in enumerate(unique):\n            seg_distr[label] += counts[i]\n        # Add size of image (pixels)\n        total_nb_pixels += len(seg.flatten())\n    # Divide by counts by total nb pixels for probabilities\n    return seg_distr/total_nb_pixels","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:54.552919Z","iopub.execute_input":"2023-05-23T09:04:54.554241Z","iopub.status.idle":"2023-05-23T09:04:54.561578Z","shell.execute_reply.started":"2023-05-23T09:04:54.554200Z","shell.execute_reply":"2023-05-23T09:04:54.560485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply to our dataframe:","metadata":{}},{"cell_type":"code","source":"# Get segmentation pixels\nseg_distr = counts_seg(train_df)\n# Plot\nplt.figure(figsize=(11,5))\nxt = [i for i in range(len(seg_distr))]\nplt.plot(xt, seg_distr, \"-o\", color=\"red\")\nall_labels = [\"background\"] + labels.tolist()\nplt.xticks(xt, labels=all_labels, rotation=90)\nplt.ylabel(\"Percentage of occurence in dataset\")\nplt.title(\"Distribution of segmentation classes\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:54.568248Z","iopub.execute_input":"2023-05-23T09:04:54.568671Z","iopub.status.idle":"2023-05-23T09:04:56.187442Z","shell.execute_reply.started":"2023-05-23T09:04:54.568632Z","shell.execute_reply":"2023-05-23T09:04:56.186280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(seg_distr)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:56.189018Z","iopub.execute_input":"2023-05-23T09:04:56.191842Z","iopub.status.idle":"2023-05-23T09:04:56.197977Z","shell.execute_reply.started":"2023-05-23T09:04:56.191800Z","shell.execute_reply":"2023-05-23T09:04:56.196724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Utilities\nBelow, we define a few auxiliary functions. First, we define functions to simplify generating the Kaggle submission:","metadata":{"papermill":{"duration":0.197841,"end_time":"2022-04-12T14:49:02.437252","exception":false,"start_time":"2022-04-12T14:49:02.239411","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _rle_encode(img):\n    \"\"\"\n    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n\n    Parameters\n    ----------\n    img: np.ndarray - binary img array\n    \n    Returns\n    -------\n    rle: String - running length encoded version of img\n    \"\"\"\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle\n\ndef generate_submission(df):\n    \"\"\"\n    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame - filled dataframe that needs to be converted\n    \n    Returns\n    -------\n    submission_df: pd.DataFrame - df in submission format.\n    \"\"\"\n    df_dict = {\"Id\": [], \"Predicted\": []}\n    for idx, _ in df.iterrows():\n        df_dict[\"Id\"].append(f\"{idx}_classification\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n    \n    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n    submission_df.to_csv(\"submission.csv\")\n    return submission_df","metadata":{"papermill":{"duration":0.213344,"end_time":"2022-04-12T14:49:02.848597","exception":false,"start_time":"2022-04-12T14:49:02.635253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T09:04:56.199620Z","iopub.execute_input":"2023-05-23T09:04:56.201227Z","iopub.status.idle":"2023-05-23T09:04:56.215646Z","shell.execute_reply.started":"2023-05-23T09:04:56.201192Z","shell.execute_reply":"2023-05-23T09:04:56.214569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also define an auxiliary function to reshape images using OpenCV:","metadata":{}},{"cell_type":"code","source":"def reshape_images(images, size):\n    \"\"\"\n    Simple auxiliary function to reshape a given sequence of images to the square shape (size, size).\n    Parameters\n    ----------\n    images: list, np.array - Array of images to be resized.\n    size: int - Size along one dimension of the final images. Shape will be (size, size).\n    \n    Returns\n    -------\n    new_images_array: np.array -  Resized images inside a numpy array\n    \n    \"\"\"\n    new_images = []\n    # Resize each image in the given sequence\n    for img in images:\n        resized = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n        new_images.append(resized)\n    # Convert from list to array\n    new_images_array = np.array(new_images)\n    return new_images_array","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:56.217385Z","iopub.execute_input":"2023-05-23T09:04:56.217808Z","iopub.status.idle":"2023-05-23T09:04:56.229238Z","shell.execute_reply.started":"2023-05-23T09:04:56.217771Z","shell.execute_reply":"2023-05-23T09:04:56.228223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_training(history, loss_key = \"loss\", acc_key = \"accuracy\"):\n    \"\"\"\n    Plots the training of a Keras model, based on the history provided by the fit methods.\n    \"\"\"\n    train_loss  = history[loss_key]\n    val_loss    = history[\"val_\" + loss_key]\n    train_acc   = history[acc_key]\n    val_acc     = history[\"val_\" + acc_key]\n    epochs_list = np.linspace(1, len(train_loss), len(train_loss))\n\n    # Make a plot\n    plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n\n    plt.subplot(121)\n    plt.plot(epochs_list, train_loss, '-o', color=\"red\", label=\"Train\")\n    plt.plot(epochs_list, val_loss, '-o', color=\"blue\", label=\"Validation\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid()\n    plt.yscale('log')\n    plt.legend()\n\n    plt.subplot(122)\n    plt.plot(epochs_list, train_acc, '-o', color=\"red\", label=\"Train\")\n    plt.plot(epochs_list, val_acc, '-o', color=\"blue\", label=\"Validation\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.grid()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:56.230866Z","iopub.execute_input":"2023-05-23T09:04:56.231386Z","iopub.status.idle":"2023-05-23T09:04:56.244800Z","shell.execute_reply.started":"2023-05-23T09:04:56.231298Z","shell.execute_reply":"2023-05-23T09:04:56.243584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Utility function which plots an image from the training set (by specifying its index) as well as the output of the model to check its behaviour at prediction time.","metadata":{}},{"cell_type":"code","source":"def test_classification(model, img_index, threshold=0.5, verbose=True):\n    \"\"\"\n    Tests a classification model by making predictions on a specific image.\n\n    Args:\n        model (ClassificationModel): The trained classification model.\n        img_index (int): The index of the image to be tested.\n        threshold (float, optional): The threshold for class probability. Default is 0.5.\n\n    \"\"\"\n    # Get image & correct label(s)\n    img = X_train[img_index]  # Fetch the image from the training set\n    input_img = np.expand_dims(img, axis=0)  # Add an extra dimension for batch processing\n    true_label = labels[y_train[img_index] == 1].tolist()  # Get the true label(s) from the training labels\n\n    # Make prediction with threshold\n    pred = model(input_img)[0]  # Get the predicted class probabilities for the input image\n    pred_indices = np.argwhere(pred > threshold)  # Find the indices where the probabilities exceed the threshold\n\n    # Convert to strings\n    pred_label = np.array(labels.tolist())[pred_indices].flatten()  # Convert the predicted indices to their corresponding labels\n\n    # Print results\n    if verbose:\n        print(f\"Correct   label(s): {true_label}\")\n        print(f\"Predicted label(s): {pred_label}\")\n\n    # Plot image and prediction distribution\n    plt.subplots(1, 2, figsize=(15, 5))\n    plt.subplot(121)\n    plt.imshow(img)\n    title = \"\"\n    plt.title(true_label)\n    plt.xticks([])\n    plt.yticks([])\n\n    # Plot distribution\n    plt.subplot(122)\n    xt = [i for i in range(len(pred))]\n    plt.plot(xt, pred, \"-o\", color=\"red\")\n    plt.xticks(xt, labels=labels, rotation=90)\n    plt.ylabel(\"Probability\")\n    plt.grid()\n    plt.axhline(threshold, ls=\"--\", color=\"black\")\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:56.246762Z","iopub.execute_input":"2023-05-23T09:04:56.247950Z","iopub.status.idle":"2023-05-23T09:04:56.260974Z","shell.execute_reply.started":"2023-05-23T09:04:56.247918Z","shell.execute_reply":"2023-05-23T09:04:56.259492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Image classification","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing:","metadata":{}},{"cell_type":"markdown","source":"First, we reshape the images into the desired shape that all the architectures use. We then have 749 train images of size $(224, 224)$ as this is the input dimension that all our architectures use. Each training image has a corresponding label which is a one-hot encoded vector of size 20.","metadata":{}},{"cell_type":"code","source":"X_train = reshape_images(train_df[\"img\"], 224)\ny_train = np.asarray(train_df[train_df.columns[:20]])\nX_test = reshape_images(test_df[\"img\"], 224)\nprint(f\"Shape of X_train is {X_train.shape}\")\nprint(f\"Shape of y_train is {y_train.shape}\")\nprint(f\"Shape of X_test  is {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:02:51.332803Z","iopub.execute_input":"2023-05-23T10:02:51.333616Z","iopub.status.idle":"2023-05-23T10:02:54.672860Z","shell.execute_reply.started":"2023-05-23T10:02:51.333576Z","shell.execute_reply":"2023-05-23T10:02:54.671643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0 Introduction and overview","metadata":{}},{"cell_type":"markdown","source":"\nAs already mentioned, this first section will deal with the classification task of the Pascal VOC dataset. The goal is to implement a deep learning architecture, namely a **convolutional neural network** (CNN), and train it to recognise all the 20 classes of the dataset, as shown for instance in the example image above. The CNN should process the image on the left and correctly predict the presence of the train and person class for that image. Before we delve into the code, we provide an overview and explanation of the different set-ups we have considered for this work, to keep the discussion compact and organized. We considered different architectures, and trained each architecture on three different mechanisms. \n\n### 2.0.1 Architectures\n\nThe first architecture we consider is **MobileNetV2**, developed at Google [1]. The architecture is simple and makes use of bottleneck residual blocks besides the conventional convolutional and pooling layers. These consist of a sequence of three layers. The first is a $1 \\times 1$ 2D convolutional layer with a ReLU6 activation function ($\\text{ReLU6}(x) = \\min\\left( \\max\\left( 0, x \\right), 6 \\right)$). The second is a $3 \\times 3$ depthwise convolutional layer, again with ReLU6 activation function. A *depthwise separable convolution* factorizes the standard convolutional operator into two layers. The first layer is a depthwise convolution which uses a single convolutional filter per input channel. The second layer performs a pointwise convolution, with kernel size $1 \\times 1$, to build new features. The advantage of these operators is that their computational cost is 8 to 9 times smaller than standard convolutions, at only a small reduction in the accuracy. The final part of the bottleneck residual block uses a linear $1\\times 1$ convolutional layer. Overall, MobileNetV2 has less parameters than MobileNetV1 (discussed below).\n\nThe precise architecture is shown below. First, a regular convolution is applied. Then, several linear bottleneck layers, discussed in detail in the paper, are applied. Finally, after the image has been downsampled to a $7\\times 7$ image with 1280 kernels, an averaging pool is used to get a vector of $1280$ features which can then be processed into a classifier. Below, we flatten the output layer (to go from shape $1\\times 1 \\times 1280$ to shape $1280$) and process these features into a classifier. \n<figure style=\"text-align: center;\">\n    <img src=https://miro.medium.com/v2/resize:fit:1016/1*5iA55983nBMlQn9f6ICxKg.png>\n    <figcaption>The MobileNetV2 architecture.</figcaption>\n</figure>\n\nThe second architecture we investigate is **VGG16** [2]. The novel insight that this paper put forward was that significant improvements in the accuracy can be achieved by increasing their depth by adding more convolutional layers, which is made feasible by using very small $3 \\times 3$ convolutional filters in all layers, which is the smallest receptive field that can capture the notions of left/right and down/up. The network uses standard ReLU activation functions. The drawback of the architecture is that it is over 533MB, which makes deploying VGG a tiresome task [3]. Smaller network architectures can therefore be desirable. VGG16 outperformed state-of-the-art models at its time of introduction in the ILSVRC-2012 and ILSVRC-2013 competitions. The architecture is shown below:\n<figure style=\"text-align: center;\">\n    <img src=\"https://miro.medium.com/v2/1*NNifzsJ7tD2kAfBXt3AzEg.png\" width=75%>\n    <figcaption>The VGG16 architecture.</figcaption>\n</figure>\n\nThe third architecture we consider is the **InceptionV3** [4]. It addressed the computational complexity of deep models such as VGG16 and came up with a way to easily scale up such models to larger sizes. The key insight that this paper used is that larger convolutional filters, such as $5\\times 5$ or $7\\times 7$, while much more expensive to evaluate, can capture dependencies between signals further away. In order to still use such larger kernels but without an increased computation time, the idea was to replace these convolutions with a multi-layer network with less parameters but the same input and output sizes. For instance, a $5 \\times 5$ convolutional layer can be replaced by a two layers of $3 \\times 3$ convolutions. This reduces the number of parameters by sharing the weights between the adjacent tiles. This leads to a $28\\%$ relative gain in computation time. Besides, the model was trained using label-smoothing regularization. That is, instead of having a fixed \"ground truth\" label, the label is replaced by a distribution which represents a mixture between the original ground truth distribution and a fixed distribution. This prevents the largest logit of the network to become much larger than all other logits, which may result in better generalization.  \n\n<figure style=\"text-align: center;\">\n    <img src=\"https://pytorch.org/assets/images/inception_v3.png\">\n    <figcaption>The InceptionV3 architecture.</figcaption>\n</figure>\n\n\nThe final architecture that we consider is **MobileNet**, the predecessor of the first architecture that we discussed [5]. This original architecture also made use of depthwise separable convolution, explained above. Further details are found in the paper.\n<figure style=\"text-align: center;\">\n    <img src=\"https://www.researchgate.net/publication/331675538/figure/fig2/AS:735605247967235@1552393366382/layers-of-MobileNet-architecture-4.ppm\" width=30%>\n    <figcaption>MobileNetV1 architecture.</figcaption>\n</figure>\n\n### 2.0.2 Training mechanisms\n\nFurthermore, each architecture was trained with three different **training mechanisms**.\n\nFirst, we consider the case of **transfer learning**. These classes implement pre-trained models for image classification by loading pre-trained weights from the ImageNet dataset. It allows us to leverage their performance and apply it to our specific classification task on images from the Pascal VOC dataset. After loading the architectures, we modify them by replacing the fully connected layers at the end of the network with a new flattened layer of the final output, and we add a dense (*i.e.*, fully connected) layer with a sigmoid activation function. This final layer has 20 output nodes, such that it is compatible with the Pascal VOC dataset which has 20 classes.\n\nSecond, we consider **transfer learning with fine-tuning**. Here, we allow to fine-tune the architecture on top of the transfer learning. These classes also use transfer learning as before, and hence much of the code will be similar to the previous class. However, now we include an additional step called **fine-tuning**. After the modified model is trained with the initial hyperparameters, the layers of the base models are unfrozen, allowing them to be trained along with our custom layers. This fine-tuning step aims to further improve the base model's performance by allowing the lower layers to learn more specific features from our new dataset. This allows us to get more flexibility in the model and can yield a more capable network.\n\nThird, we consider training the architectures **from scratch**. This means that we do not import any weights from pre-trained models, but instead train the entire model on our own dataset.\n\n### 2.0.2 Training set-up and hyperparameters\n\nThe training of all classification models generally use more or less the same **set-up and hyperparameters**. We have chosen for the **Adam optimizer** and a **cross-entropy loss**. In Adam, derived from “adaptive moments” one computes an unbiased estimate for the first and second moments of the gradient. Adam is generally regarded as fairly robust to the choice of hyperparameters. Details can be found in Section 8.5.3 of the book [*Deep Learning*](https://www.deeplearningbook.org/) by Goodfellow *et al.*. The cross-entropy loss **TODO discuss** \n\nThe model is trained for around 20 to 30 epochs on the Pascal VOC dataset. We use a small learning rate of around $10^{-5}$ as this empirically gave good results. We make us of early stopping to prevent overfitting, with patience of around $3$ epochs and threshold improvement of $0.01$. For this, we use a train-validation split, and use $20\\%$ of the dataset as validation data. We also process the training data in batches of size $32$.\n\n\n**References:**\n\n[1] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L. C. (2018). *Mobilenetv2: Inverted residuals and linear bottlenecks*. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).\n\n[2] Simonyan, K., & Zisserman, A. (2014). *Very deep convolutional networks for large-scale image recognition*. arXiv preprint arXiv:1409.1556.\n\n[3] Neurohive. (2018, November 20). VGG16 - Convolutional Network for Classification and Detection. Retrieved May 16, 2023, from https://neurohive.io/en/popular-networks/vgg16/\n\n[4] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). *Rethinking the inception architecture for computer vision*. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n\n[5] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M. and Adam, H. (2017). *Mobilenets: Efficient convolutional neural networks for mobile vision applications*. arXiv preprint arXiv:1704.04861.","metadata":{"papermill":{"duration":0.196641,"end_time":"2022-04-12T14:49:03.240824","exception":false,"start_time":"2022-04-12T14:49:03.044183","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 2.1 Results","metadata":{}},{"cell_type":"markdown","source":"The results of the different architectures are shown below. We see that the best results are obtained with MobileNetV2 and that, in general, transfer learning and finetuning give the best results. This is to be expected, since transfer learning allows us to essentially import architectures that have been trained for a much longer time on much more images. By importing the first layers of these pretrained models, we can use networks that already are able to extract relevant features from the images. We also expect fine-tuning to give additional improvements, since this final stage allows us to retrain the entire architecture, such that we are made sure that the pretrained layers and our custom-defined dense layer(s) are properly working together. Moreover, we notice from the figure that training these models from scratch can already give quite a high performance, which is also pretty consistent across the different architectures. Finally, the VGG16 architecture has remarkably a lower performance than all the other models.\n\nBelow, we will work with **InceptionV3**, as we found this model slightly easier to work with.","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/cv-ga2-class-results/sorted_classification_names.pickle', 'rb') as handle:\n    classification_names = pickle.load(handle)\n    \nwith open('/kaggle/input/cv-ga2-class-results/sorted_classification_val_accs.pickle', 'rb') as handle:\n    classification_val_accs = pickle.load(handle)\n    \nxticks = [i for i in range(len(classification_names))]\n\nplt.figure(figsize=(11,3))\nplt.plot(xticks, classification_val_accs, '-o', color=\"blue\")\nplt.xticks(xticks, labels=classification_names, rotation=45)\nplt.grid()\nplt.ylabel(\"Validation accuracy\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:04:59.844417Z","iopub.execute_input":"2023-05-23T09:04:59.844888Z","iopub.status.idle":"2023-05-23T09:05:00.114885Z","shell.execute_reply.started":"2023-05-23T09:04:59.844846Z","shell.execute_reply":"2023-05-23T09:05:00.113910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Final model: InceptionV3 (transfer learning + fine-tuning)","metadata":{}},{"cell_type":"markdown","source":"The final model is shown below. After the layers of InceptionV3, we add a custom layer that finally has 20 output nodes and which uses a sigmoid activation function, such that the output falls between 0 and 1 such that it can be interpreted as a probability. Actual predictions of the model are then obtained by taking $0.5$ as a threshold: if the output of a certain node is above this threshold, then the model predicts that this class is present in the image. In principle, one can essentially view this threshold value as yet another hyperparameter, which can then be tuned on a separate validation set. However, this goes beyon the scope of this project, and we are also quite satisfied with the current performance of the model.","metadata":{}},{"cell_type":"code","source":"class ClassificationModel:\n    def __init__(self, X, y):\n        \"\"\"\n        Initializes a ClassificationModel object.\n\n        Args:\n        \n            X (numpy.ndarray): Input image data with shape (num_samples, height, width, channels).\n            y (numpy.ndarray): Target labels with shape (num_samples, num_classes).\n\n        \"\"\"\n        # Load pre-trained InceptionV3 model\n        base_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n        base_model.trainable = False\n\n        # Add custom layers on top of the base model\n        new_l = tf.keras.layers.Flatten()(base_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        model = tf.keras.Model(base_model.input, new_l)\n\n        # Preprocess the input data\n        X_train = tf.keras.applications.inception_v3.preprocess_input(X)\n\n        # Compile the model\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-4),\n                      loss='binary_crossentropy', metrics=['binary_accuracy'])\n        \n        # Set up callbacks for training\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=3, min_delta=0.01),\n            tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_binary_accuracy\", save_best_only=True)\n        ]\n        \n        # Train the model with the pre-training settings\n        pre_history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.pre_history = pre_history\n\n        # Enable fine-tuning of the base model\n        base_model.trainable = True\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-8),\n                      loss='binary_crossentropy', metrics=['binary_accuracy'])\n        \n        # Train the model with fine-tuning\n        history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.model = model\n        self.history = history\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the class probabilities for input images.\n\n        Args:\n            X (numpy.ndarray): Input image data with shape (num_samples, height, width, channels).\n\n        Returns:\n            numpy.ndarray: Predicted class probabilities with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.inception_v3.preprocess_input(X)\n        return self.model.predict(preprocessed_X, verbose=0)\n\n    def __call__(self, X):\n        \"\"\"\n        Makes predictions using the model.\n\n        Args:\n            X (numpy.ndarray): Input image data with shape (num_samples, height, width, channels).\n\n        Returns:\n            numpy.ndarray: Predicted class probabilities with shape (num_samples, num_classes).\n\n        \"\"\"\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:00.116340Z","iopub.execute_input":"2023-05-23T09:05:00.116946Z","iopub.status.idle":"2023-05-23T09:05:00.132562Z","shell.execute_reply.started":"2023-05-23T09:05:00.116902Z","shell.execute_reply":"2023-05-23T09:05:00.131501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.1 Training the model","metadata":{}},{"cell_type":"markdown","source":"First, we train the model (or load in the training from the memory) and discuss the learning curves we obtained.","metadata":{}},{"cell_type":"code","source":"if TRAIN_MODELS:\n    classification_model = ClassificationModel(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:00.134403Z","iopub.execute_input":"2023-05-23T09:05:00.134793Z","iopub.status.idle":"2023-05-23T09:05:00.146513Z","shell.execute_reply.started":"2023-05-23T09:05:00.134755Z","shell.execute_reply":"2023-05-23T09:05:00.145504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nif not TRAIN_MODELS:\n    with open('/kaggle/input/cv-ga2-class-results/classification_model.pickle', 'rb') as handle:\n        classification_model = pickle.load(handle);","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:00.148622Z","iopub.execute_input":"2023-05-23T09:05:00.149105Z","iopub.status.idle":"2023-05-23T09:05:12.379978Z","shell.execute_reply.started":"2023-05-23T09:05:00.149068Z","shell.execute_reply":"2023-05-23T09:05:12.378890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To save the trained model:\n# with open('/kaggle/working/classification_model.pickle', 'wb') as handle:\n#     pickle.dump(classification_model, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:12.381521Z","iopub.execute_input":"2023-05-23T09:05:12.382013Z","iopub.status.idle":"2023-05-23T09:05:12.387767Z","shell.execute_reply.started":"2023-05-23T09:05:12.381969Z","shell.execute_reply":"2023-05-23T09:05:12.386707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we show the final training epochs for our chosen InceptionV3 model, which was trained with transfer learning and fine-tuning. The top part shows the transfer learning part, the bottom shows the fine-tuning stage. We notice that the model already has a quite high performance from the start and only requires a few iterations to improve to the accuracy reported above. Moreover, most of the improvement is made during transfer learning, where we increase the accuracy by another $~\\sim 4\\%$. Remarkably, the fine-tuning part has a wilder behaviour: even though the loss of the validation set was increasing during fine-tuning, the accuracy was increasing as well. We also observed that it is quite easy to overfit on the training data during fine-tuning, such that we chose for a very low learning rate. While this is not really observed for the run we show below (the training accuracy was even decreasing at first), it seems likely that we started overfitting in the final epoch.  ","metadata":{}},{"cell_type":"code","source":"show_training(classification_model.pre_history.history, acc_key=\"binary_accuracy\")\nshow_training(classification_model.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:12.389545Z","iopub.execute_input":"2023-05-23T09:05:12.390258Z","iopub.status.idle":"2023-05-23T09:05:13.774030Z","shell.execute_reply.started":"2023-05-23T09:05:12.390220Z","shell.execute_reply":"2023-05-23T09:05:13.772871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 Checking performance of the model","metadata":{}},{"cell_type":"markdown","source":"Even though we can use metrics and monitor the loss during training, it is often informative to check the predictions of the model and assess it qualitatively. Below, we show an example where the model is able to correctly predict all the classes present in an image:","metadata":{}},{"cell_type":"code","source":"test_classification(classification_model, 15)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:13.775628Z","iopub.execute_input":"2023-05-23T09:05:13.776288Z","iopub.status.idle":"2023-05-23T09:05:19.558280Z","shell.execute_reply.started":"2023-05-23T09:05:13.776248Z","shell.execute_reply":"2023-05-23T09:05:19.557191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's an example where the model misses one of the classes present in the image. In this and other, similar \"bad\" predictions, the model has a high output value for the missing class, but since it is slightly below the threshold of 0.5, it does not predict the class in the final result. As already mentioned, it can be possible to improve the results by tuning this threshold, to reduce this kind of misclassifications from happening.","metadata":{}},{"cell_type":"code","source":"test_classification(classification_model, 7)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:05:19.560029Z","iopub.execute_input":"2023-05-23T09:05:19.560756Z","iopub.status.idle":"2023-05-23T09:05:20.209309Z","shell.execute_reply.started":"2023-05-23T09:05:19.560715Z","shell.execute_reply":"2023-05-23T09:05:20.208286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define an auxiliary function that turns the predictions of the output layer into predictions suitable for the Kaggle competition","metadata":{}},{"cell_type":"markdown","source":"**TODO** discussion on Dice score & tuning the threshold & adding the argmax function","metadata":{}},{"cell_type":"code","source":"def get_predicted_classes(predictions, threshold=0.35, use_argmax=True):\n    \"\"\"\n    Predictions has shape (n_samples, n_classes). Predictions: these are the NON thresholded values.\n    \"\"\"\n    # Apply threshold    \n    yhat = np.where(predictions > threshold, 1, 0)\n    \n    if use_argmax:\n        thresholded = yhat.copy()\n        for i, pred in enumerate(yhat):\n            if not(1 in pred):\n                index = np.argmax(predictions[i])\n                yhat[i, index] = 1\n            \n    return yhat","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:00:58.463859Z","iopub.execute_input":"2023-05-23T10:00:58.464237Z","iopub.status.idle":"2023-05-23T10:00:58.472668Z","shell.execute_reply.started":"2023-05-23T10:00:58.464204Z","shell.execute_reply":"2023-05-23T10:00:58.471521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"test_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:03:02.697337Z","iopub.execute_input":"2023-05-23T10:03:02.698558Z","iopub.status.idle":"2023-05-23T10:03:09.601726Z","shell.execute_reply.started":"2023-05-23T10:03:02.698508Z","shell.execute_reply":"2023-05-23T10:03:09.600519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, get the predicted labels on the test set","metadata":{}},{"cell_type":"code","source":"pred = classification_model(X_test)\nprint(len(pred))\nprint(len(test_df))","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:03:09.604201Z","iopub.execute_input":"2023-05-23T10:03:09.604611Z","iopub.status.idle":"2023-05-23T10:03:14.612677Z","shell.execute_reply.started":"2023-05-23T10:03:09.604570Z","shell.execute_reply":"2023-05-23T10:03:14.610675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_test_classification(classification_model, threshold=0.35, use_argmax=True):\n    \"\"\"\n    Predicts the classification of the Kaggle test set and puts them into test_df\n    \"\"\"\n    \n    pred = classification_model(X_test)\n    pred = get_predicted_classes(pred, threshold=threshold, use_argmax=use_argmax)\n    for i in range(len(test_df)):\n        test_df.loc[i, test_df.columns[:20]] = pred[i]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:03:17.814597Z","iopub.execute_input":"2023-05-23T10:03:17.814985Z","iopub.status.idle":"2023-05-23T10:03:17.825024Z","shell.execute_reply.started":"2023-05-23T10:03:17.814950Z","shell.execute_reply":"2023-05-23T10:03:17.823983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_test_classification(classification_model)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:03:18.103002Z","iopub.execute_input":"2023-05-23T10:03:18.103493Z","iopub.status.idle":"2023-05-23T10:03:27.262032Z","shell.execute_reply.started":"2023-05-23T10:03:18.103454Z","shell.execute_reply":"2023-05-23T10:03:27.260953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T10:03:27.263791Z","iopub.execute_input":"2023-05-23T10:03:27.264151Z","iopub.status.idle":"2023-05-23T10:03:34.601481Z","shell.execute_reply.started":"2023-05-23T10:03:27.264119Z","shell.execute_reply":"2023-05-23T10:03:34.600389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Semantic segmentation\nThe goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). ","metadata":{"papermill":{"duration":0.19763,"end_time":"2022-04-12T14:49:07.536010","exception":false,"start_time":"2022-04-12T14:49:07.338380","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 3.0 Data preprocessing and utilities","metadata":{}},{"cell_type":"markdown","source":"### 3.0.1 Preprocessing","metadata":{}},{"cell_type":"markdown","source":"For convenience, we again resize the images into shape $224 \\times 224 \\times 3$, and store these as new entries in the train dataframe.","metadata":{}},{"cell_type":"code","source":"# Resize training data\ntrain_df['img_n'] = train_df[\"img\"].map(lambda img:  cv2.resize(img, (224,224)))\ntrain_df[\"seg\"] = train_df[\"seg\"].map(lambda img:  cv2.resize(img, (224,224), interpolation=cv2.INTER_NEAREST))\n# Convert to arrays for compatibility with Keras\ntrain_img_keras = np.array(train_df[\"img_n\"].values.tolist())\ntrain_seg_keras = np.array(train_df[\"seg\"].values.tolist())","metadata":{"papermill":{"duration":11.823715,"end_time":"2022-04-12T14:49:19.557577","exception":false,"start_time":"2022-04-12T14:49:07.733862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T09:06:30.626769Z","iopub.execute_input":"2023-05-23T09:06:30.627765Z","iopub.status.idle":"2023-05-23T09:06:30.965288Z","shell.execute_reply.started":"2023-05-23T09:06:30.627709Z","shell.execute_reply":"2023-05-23T09:06:30.964182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We perform a train-validation split:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test  = train_test_split(train_img_keras, train_seg_keras, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:33.592905Z","iopub.execute_input":"2023-05-23T09:06:33.593897Z","iopub.status.idle":"2023-05-23T09:06:33.691086Z","shell.execute_reply.started":"2023-05-23T09:06:33.593848Z","shell.execute_reply":"2023-05-23T09:06:33.689956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make use of the following auxiliary functions during preprocessing:","metadata":{}},{"cell_type":"code","source":"def augment(input_image, input_mask):\n    \"\"\"\n    Augments an input image and its corresponding mask by randomly flipping them horizontally.\n\n    Args:\n        input_image (tf.Tensor): The input image tensor.\n        input_mask (tf.Tensor): The input mask tensor.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor]: A tuple containing the augmented image and mask tensors.\n    \"\"\"\n    if tf.random.uniform(()) > 0.5:\n        # Randomly flip the image and mask horizontally\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n    return input_image, input_mask\n\n\ndef normalize(input_image, input_mask):\n    \"\"\"\n    Normalizes an input image by dividing it by 255.0 and casts it to float32.\n\n    Args:\n        input_image (tf.Tensor): The input image tensor.\n        input_mask (tf.Tensor): The input mask tensor.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor]: A tuple containing the normalized image and the original mask tensor.\n    \"\"\"\n    # Normalize the input image by dividing it by 255.0 and cast it to float32\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:36.056817Z","iopub.execute_input":"2023-05-23T09:06:36.057216Z","iopub.status.idle":"2023-05-23T09:06:36.065019Z","shell.execute_reply.started":"2023-05-23T09:06:36.057180Z","shell.execute_reply":"2023-05-23T09:06:36.063722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess the data. We make use of one-hot encoding.","metadata":{}},{"cell_type":"code","source":"num_classes = 21","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:37.589781Z","iopub.execute_input":"2023-05-23T09:06:37.590809Z","iopub.status.idle":"2023-05-23T09:06:37.596689Z","shell.execute_reply.started":"2023-05-23T09:06:37.590758Z","shell.execute_reply":"2023-05-23T09:06:37.595466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train dataset\ntrain_images_bc, train_labels_bc = (X_train, y_train.reshape(-1, 224,224,1))\ntrain_labels_cat                 = to_categorical(train_labels_bc, num_classes=num_classes)\ntrain_images, train_labels       = normalize(train_images_bc, train_labels_cat)\n# Validation\nval_labels_cat                   = to_categorical(y_val, num_classes=num_classes)\nval_images, val_labels_cat       = normalize(X_val, val_labels_cat)\n# Test\ntest_labels_cat                  = to_categorical(y_test, num_classes=num_classes)\ntest_images, test_labels_cat     = normalize(X_test, test_labels_cat)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:37.802838Z","iopub.execute_input":"2023-05-23T09:06:37.803776Z","iopub.status.idle":"2023-05-23T09:06:40.196272Z","shell.execute_reply.started":"2023-05-23T09:06:37.803732Z","shell.execute_reply":"2023-05-23T09:06:40.195163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Before one-hot encoding: {train_labels_bc.shape}\")\nprint(f\"After  one-hot encoding: {train_labels_cat.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:40.198911Z","iopub.execute_input":"2023-05-23T09:06:40.199716Z","iopub.status.idle":"2023-05-23T09:06:40.206239Z","shell.execute_reply.started":"2023-05-23T09:06:40.199671Z","shell.execute_reply":"2023-05-23T09:06:40.205005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.0.2 Utilities","metadata":{}},{"cell_type":"markdown","source":"Some auxiliary functions are defined below","metadata":{}},{"cell_type":"code","source":"test_labels_cat.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:14:50.561445Z","iopub.execute_input":"2023-05-23T09:14:50.561975Z","iopub.status.idle":"2023-05-23T09:14:50.574615Z","shell.execute_reply.started":"2023-05-23T09:14:50.561939Z","shell.execute_reply":"2023-05-23T09:14:50.573278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(m, i):\n    \"\"\"\n    Predicts image classes and displays visualizations.\n\n    This function takes an image and an index as input, performs prediction\n    on the image using a pre-trained model, and displays visualizations of\n    the image and the predicted classes.\n\n    Args:\n        img (numpy.ndarray): Input image array.\n        i (int): Index of the image in the dataset.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Get the image and its segmentation\n    img = test_images[i]\n    seg = test_labels_cat[i]\n    seg = np.argmax(seg, axis=2)\n    \n    # Reshape the image to match the model's input shape\n    x = tf.reshape(img, (-1, 224, 224, 3))\n\n    # Perform prediction on the image using the pre-trained model\n    d = m.predict(x)\n\n    # Set a threshold for the predictions\n    threshold = 0.55\n    result = d > threshold\n\n    # Extract the predicted class labels from the predictions\n    imclass = np.argmax(result, axis=3)[0, :, :]\n\n    # Print the unique class labels in the predicted classes\n    print(np.unique(imclass))\n\n    # Create a figure to display the visualizations\n    fig, axs = plt.subplots(1, 3, figsize=(15, 7))\n\n    # Plot the original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(np.asarray(img))\n\n    # Plot the predicted classes\n    plt.subplot(1, 3, 2)\n    plt.imshow(imclass)\n\n    # Plot the original image again\n    plt.subplot(1, 3, 3)\n    plt.imshow(np.asarray(seg), vmin=0, vmax=20)\n\n    # Mask the predicted classes where the label is 0\n    masked_imclass = np.ma.masked_where(imclass == 0, imclass)\n    \n    for ax in axs:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # Uncomment the following lines if you want to overlay the masked classes\n    # on the original image or the predicted classes\n    # plt.imshow(imclass, alpha=0.5)\n    # plt.imshow(masked_imclass, alpha=0.5)\n\n    # Uncomment the following lines if you have the 'train_labels_cat' array\n    # and want to visualize the true class labels\n    # test_labels = train_labels_cat.reshape(224, 224)\n    # real_class = np.argmax(test_labels[25], axis=3)[0, :, :]\n    # print(np.unique(np.argmax(test_labels_cat[i], axis=2)))\n    # plt.imshow(np.argmax(test_labels_cat[i], axis=2))","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:17:22.699615Z","iopub.execute_input":"2023-05-23T09:17:22.700073Z","iopub.status.idle":"2023-05-23T09:17:22.719420Z","shell.execute_reply.started":"2023-05-23T09:17:22.700031Z","shell.execute_reply":"2023-05-23T09:17:22.718403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a few functions which we use to build our models:","metadata":{}},{"cell_type":"code","source":"def double_conv_block(x: tf.Tensor, n_filters: int):\n    \"\"\"\n    Creates a double convolutional block consisting of two Conv2D layers with ReLU activation.\n\n    Args:\n        x (tf.Tensor): The input tensor.\n        n_filters (int): The number of filters (output channels) for the Conv2D layers.\n\n    Returns:\n        tf.Tensor: The output tensor after passing through the double convolutional block.\n    \"\"\"\n    # Conv2D layer with n_filters filters, 3x3 kernel, 'same' padding, ReLU activation, and 'he_normal' initializer\n    x = Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n    # Conv2D layer with n_filters filters, 3x3 kernel, 'same' padding, ReLU activation, and 'he_normal' initializer\n    x = Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n    return x\n\ndef downsample_block(x: tf.Tensor, n_filters: int, dropout: float = 0):\n    \"\"\"\n    Creates a downsample block consisting of a double convolutional block, max pooling, and optional dropout.\n\n    Args:\n        x (tf.Tensor): The input tensor.\n        n_filters (int): The number of filters (output channels) for the convolutional layers.\n        dropout (float, optional): The dropout rate. Defaults to 0.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor]: A tuple containing the feature tensor after the double convolutional block (f),\n        and the downsampled and optionally dropout-applied tensor (p).\n    \"\"\"\n    # Apply double convolutional block to input tensor\n    f = double_conv_block(x, n_filters)\n    # Max pooling with 2x2 pool size\n    p = MaxPooling2D((2, 2))(f)\n    # Optional dropout layer with specified dropout rate (default: dropout of zero percentage)\n    p = Dropout(dropout)(p)\n    return f, p\n\ndef upsample_block(x: tf.Tensor, conv_features: tf.Tensor, n_filters: int, dropout: float = 0):\n    \"\"\"\n    Creates an upsample block consisting of upsampling, concatenation, dropout, and double convolution.\n\n    Args:\n        x (tf.Tensor): The input tensor for upsampling.\n        conv_features (tf.Tensor): The tensor from the corresponding downsample block for concatenation.\n        n_filters (int): The number of filters (output channels) for the convolutional layers.\n        dropout (float, optional): The dropout rate. Defaults to 0.\n\n    Returns:\n        tf.Tensor: The output tensor after the upsample block.\n    \"\"\"\n    # Upsample using Conv2DTranspose with n_filters filters, 3x3 kernel, stride 2, and 'same' padding\n    x = Conv2DTranspose(n_filters, 3, strides=2, padding=\"same\")(x)\n    # Concatenate the upsampled tensor with the corresponding downsample block's feature tensor\n    x = concatenate([x, conv_features])\n    # Optional dropout layer with specified dropout rate (default: rate 0)\n    x = Dropout(dropout)(x)\n    # Apply double convolutional block to the concatenated tensor\n    x = double_conv_block(x, n_filters)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:15:42.370457Z","iopub.execute_input":"2023-05-23T09:15:42.370894Z","iopub.status.idle":"2023-05-23T09:15:42.384377Z","shell.execute_reply.started":"2023-05-23T09:15:42.370855Z","shell.execute_reply":"2023-05-23T09:15:42.383210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the above auxiliary functions that define our upsampling and downsampling blocks, we are now going to recreate the **U-Net** architecture [1]. **TODO provide discussion**\n\n**References:**\n\n[1] Ronneberger, O., Fischer, P., & Brox, T. (2015). *U-net: Convolutional networks for biomedical image segmentation*. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.","metadata":{}},{"cell_type":"markdown","source":"## 3.1 U-Net","metadata":{}},{"cell_type":"code","source":"def unet(pretrained_weights = None, input_size = (224,224,3), weights= [1.,1.,1.,1.],\\\n         activation='relu', dropout=0, loss='categorical_crossentropy', optimizer='adam',\\\n             dilation_rate=(1,1), reg=0.01):\n    \"\"\"\n    U-Net model for image segmentation. \n\n    Args:\n    - pretrained_weights: (str) path to the saved pretrained weights of the model.\n    - input_size: (tuple) input size of the image.\n    - weights: (list) weights for loss function.\n    - activation: (str) activation function for convolution layers.\n    - dropout: (float) dropout rate.\n    - loss: (str) loss function for model optimization.\n    - optimizer: (str) optimizer for model optimization.\n    - dilation_rate: (int) dilation rate for convolution layers.\n    - reg: (float) regularization parameter.\n    Returns:\n    - unet_model: (tf.keras.Model) compiled U-Net model.\n    \"\"\"\n    # input layer\n    inputs  = Input(shape=input_size)\n    # encoder: contracting path - downsample\n    # 1 - downsample\n    f1, p1 = downsample_block(inputs, 64)\n    # 2 - downsample\n    f2, p2 = downsample_block(p1, 128)\n    # 3 - downsample\n    f3, p3 = downsample_block(p2, 256)\n    # 4 - downsample\n    f4, p4 = downsample_block(p3, 512)\n    \n    # 5 - bottleneck\n    bottleneck = double_conv_block(p4, 1024)\n    \n    # decoder: expanding path - upsample\n    # 6 - upsample\n    u6 = upsample_block(bottleneck, f4, 512)\n    # 7 - upsample\n    u7 = upsample_block(u6, f3, 256)\n    # 8 - upsample\n    u8 = upsample_block(u7, f2, 128)\n    # 9 - upsample\n    u9 = upsample_block(u8, f1, 64)\n    # outputs\n    outputs = Conv2D(num_classes, 1, activation='softmax')(u9)\n    # unet model with Keras Functional API\n    unet_model = tf.keras.Model([inputs], [outputs], name=\"U-Net\")\n    # unet_model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n\n    if(pretrained_weights):\n        # load saved pretrained weights\n        print('Using {0} pretrained weights'.format(pretrained_weights))\n        unet_model.load_weights(pretrained_weights)\n\n    return unet_model\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:15:42.855279Z","iopub.execute_input":"2023-05-23T09:15:42.855674Z","iopub.status.idle":"2023-05-23T09:15:42.868477Z","shell.execute_reply.started":"2023-05-23T09:15:42.855640Z","shell.execute_reply":"2023-05-23T09:15:42.867126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a new instance of the model and print the architecture we are using","metadata":{}},{"cell_type":"code","source":"new_model = unet()\nopt = Adam(learning_rate=1e-3)\nloss = 'categorical_crossentropy'\nmetrics = ['categorical_accuracy']\nepochs = 100\nsteps_per_epoch = 30\nnew_model.compile(optimizer=opt, loss=loss, metrics=metrics)\nnew_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:15:43.568944Z","iopub.execute_input":"2023-05-23T09:15:43.569332Z","iopub.status.idle":"2023-05-23T09:15:44.081577Z","shell.execute_reply.started":"2023-05-23T09:15:43.569297Z","shell.execute_reply":"2023-05-23T09:15:44.080774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.1 Custom U-Net architecture, from scratch","metadata":{}},{"cell_type":"markdown","source":"First, we train the architecture defined above from scratch. **TODO check discussion** During training, this architecture leads to a large divergence between traininig and validation accuracy. It results in pixels from one class being classified as multiple different classes.","metadata":{}},{"cell_type":"code","source":"LOAD_WEIGHTS = True\nif not LOAD_WEIGHTS:\n    # Train the model\n    history_scratch = new_model.fit(train_images, train_labels_cat, \n                        validation_data=(val_images, val_labels_cat), \n                        epochs=100, batch_size=32)\n    # Save the obtained weights, to export to Kaggle datasets\n    new_model.save('/kaggle/working/model_scratch')\n    new_model.save('/downloads/vision_scratch_model')\nelse:\n    # Download models from the dataset\n    reconstructed_model_1 = keras.models.load_model('/kaggle/input/models/model_scratch/model_scratch')\n    scratch_loss, scratch_acc = reconstructed_model_1.evaluate(test_images, test_labels_cat)\n    print(f\"The accuracy of the scratch model is {scratch_acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:03.612014Z","iopub.execute_input":"2023-05-22T21:25:03.612378Z","iopub.status.idle":"2023-05-22T21:25:27.086133Z","shell.execute_reply.started":"2023-05-22T21:25:03.612339Z","shell.execute_reply":"2023-05-22T21:25:27.084917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_training(history_scratch.history)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.088386Z","iopub.execute_input":"2023-05-22T21:25:27.088828Z","iopub.status.idle":"2023-05-22T21:25:27.094277Z","shell.execute_reply.started":"2023-05-22T21:25:27.088775Z","shell.execute_reply":"2023-05-22T21:25:27.093038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training, we want to obtain predictions by setting a threshold on the softmax output:","metadata":{}},{"cell_type":"code","source":"#test(np.asarray(reconstructed_model_1, test_images[1]),1)\n#test(np.asarray(reconstructed_model_1, test_images[12]),12)\n#test(np.asarray(reconstructed_model_1, test_images[33]),33)\n#test(np.asarray(reconstructed_model_1, test_images[14]),14)\n#test(np.asarray(reconstructed_model_1, test_images[22]),22)\n#test(np.asarray(reconstructed_model_1, test_images[31]),31)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:34:25.234800Z","iopub.execute_input":"2023-05-22T21:34:25.235877Z","iopub.status.idle":"2023-05-22T21:34:25.240616Z","shell.execute_reply.started":"2023-05-22T21:34:25.235833Z","shell.execute_reply":"2023-05-22T21:34:25.239400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 Keras U-Net architecture","metadata":{}},{"cell_type":"code","source":"!pip install -U -q segmentation-models\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nimport segmentation_models\nfrom segmentation_models import Unet\nfrom segmentation_models.utils import set_trainable","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:28:59.824275Z","iopub.execute_input":"2023-05-22T21:28:59.825254Z","iopub.status.idle":"2023-05-22T21:29:14.031925Z","shell.execute_reply.started":"2023-05-22T21:28:59.825200Z","shell.execute_reply":"2023-05-22T21:29:14.030720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we import the U-Net architecture that Keras implemented, and train it on our Pascal VOC dataset. ","metadata":{}},{"cell_type":"code","source":"num_classes = 21\nLOAD_WEIGHTS = True\nif not LOAD_WEIGHTS:\n    # Define and train the model\n    transfer_model = Unet(classes=num_classes)\n    transfer_model.trainable = True\n    transfer_model.compile('Adam', 'categorical_crossentropy', ['categorical_accuracy'])\n    # Save the model\n    transfer_model.save('/kaggle/working/model_scratch_keras')\n    transfer_model.save('/downloads/vision_scratch_model_keras')\n    history_unet_transfer = transfer_model.fit(train_images, train_labels_cat, \n                                          validation_data=(val_images, val_labels_cat), \n                                          epochs=95, batch_size=32)\nelse:\n    reconstructed_model_2 = keras.models.load_model('/kaggle/input/models/model_scratch_keras/model_scratch_keras')\n    loss, acc = reconstructed_model_2.evaluate(test_images, test_labels_cat)\n    print(f\"The Keras U-Net model trained from scratch has accuracy {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:29:14.035570Z","iopub.execute_input":"2023-05-22T21:29:14.036625Z","iopub.status.idle":"2023-05-22T21:29:23.921688Z","shell.execute_reply.started":"2023-05-22T21:29:14.036575Z","shell.execute_reply":"2023-05-22T21:29:23.920500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transfer_loss, transfer_acc = reconstructed_model_2.evaluate(test_images, test_labels_cat)\nprint(f\"The transfer model accuracy is {transfer_acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:29:37.460798Z","iopub.execute_input":"2023-05-22T21:29:37.461269Z","iopub.status.idle":"2023-05-22T21:29:39.883470Z","shell.execute_reply.started":"2023-05-22T21:29:37.461225Z","shell.execute_reply":"2023-05-22T21:29:39.882378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.3 Transfer learning, Keras U-Net","metadata":{}},{"cell_type":"markdown","source":" Here, we import the U-Net architecture as well as the weights. During training, this training accuracy very slowly improves. Some classes are not identified during testing (only background). However compared to the model trained from scratch, pixels from one object are not being classified as so many different classes anymore.","metadata":{}},{"cell_type":"code","source":"if TRAIN_MODELS:\n    # Train the model\n    inp = Input(shape=(None, None, 21))\n    model4 = Unet(backbone_name='resnet34', classes=21, encoder_weights='imagenet')\n    model4.compile('Adam', 'categorical_crossentropy', ['categorical_accuracy'])\n    history_transfer = model4.fit(train_images, train_labels_cat, \n                                  validation_data=(val_images, val_labels_cat), \n                                  epochs=105, batch_size=32)\n    # Save the weights\n    model4.save('/kaggle/working/model_transfer_cat')\n    model4.save('/downloads/vision_transfer_model_cat')\n    \nelif LOAD_WEIGHTS:\n    reconstructed_model_3 = keras.models.load_model('/kaggle/input/models/model_transfer_cat/model_transfer_cat')\n    loss, acc = reconstructed_model_3.evaluate(test_images, test_labels_cat)\n    print(f\"This model has accuracy {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:30:53.359136Z","iopub.execute_input":"2023-05-22T21:30:53.360485Z","iopub.status.idle":"2023-05-22T21:31:06.266947Z","shell.execute_reply.started":"2023-05-22T21:30:53.360441Z","shell.execute_reply":"2023-05-22T21:31:06.265863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.4 Transfer learning, VGG16\n\nIn https://github.com/farakiko/ImageSegmentationPASCAL/blob/master/Code.ipynb we see VGG16 used for segmentation. This model results in the best score in the submission.","metadata":{}},{"cell_type":"code","source":"num_classes=21\n\nbase_model = VGG16(include_top=False, weights='imagenet',input_shape=(224, 224, 3))\nbase_model.trainable=False\n\nvgg_model = Sequential()\n\nvgg_model.add(base_model)\nvgg_model.add(Conv2D(4096, (7, 7), activation='tanh',\n                padding='same'))\nvgg_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nvgg_model.add(Dropout(0.5))\nvgg_model.add((Conv2D(4096, (1, 1), activation='tanh',\n                padding='same')))\nvgg_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nvgg_model.add(Dropout(0.5))\nvgg_model.add(Conv2D(num_classes,  (1, 1), kernel_initializer='he_normal'))\nvgg_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nvgg_model.add(Conv2DTranspose(num_classes, kernel_size=(32, 32),  strides=(32, 32), use_bias=False))\nvgg_model.add(Activation('softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:52.161779Z","iopub.execute_input":"2023-05-23T09:06:52.162731Z","iopub.status.idle":"2023-05-23T09:06:55.826593Z","shell.execute_reply.started":"2023-05-23T09:06:52.162677Z","shell.execute_reply":"2023-05-23T09:06:55.825254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN_MODELS:\n    opt = Adam(lr=0.001)\n    loss = 'categorical_crossentropy'\n    metrics = ['categorical_accuracy']\n    epochs = 100\n    steps_per_epoch = 80\n    vgg_model.compile(optimizer=opt, loss=loss, metrics=metrics)\n    history = vgg_model.fit(train_images, train_labels_cat, \n                                      validation_data=(val_images, val_labels_cat), \n                                      epochs=110, batch_size=16)\n    vgg_model.save('/kaggle/working/model_vgg')","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:06:55.828457Z","iopub.execute_input":"2023-05-23T09:06:55.828960Z","iopub.status.idle":"2023-05-23T09:06:55.842410Z","shell.execute_reply.started":"2023-05-23T09:06:55.828918Z","shell.execute_reply":"2023-05-23T09:06:55.840966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD_WEIGHTS=True\nif LOAD_WEIGHTS:\n    reconstructed_model_5 = keras.models.load_model('/kaggle/input/models/model_vgg_2/model_vgg')\n    loss, acc = reconstructed_model_5.evaluate(test_images, test_labels_cat)\n    print(f\"This model has accuracy {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:07:52.534210Z","iopub.execute_input":"2023-05-23T09:07:52.534958Z","iopub.status.idle":"2023-05-23T09:07:58.082116Z","shell.execute_reply.started":"2023-05-23T09:07:52.534915Z","shell.execute_reply":"2023-05-23T09:07:58.080951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(reconstructed_model_5, 1)\ntest(reconstructed_model_5, 12)\ntest(reconstructed_model_5, 33)\ntest(reconstructed_model_5, 14)\ntest(reconstructed_model_5, 22)\ntest(reconstructed_model_5, 31)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:17:28.762099Z","iopub.execute_input":"2023-05-23T09:17:28.762622Z","iopub.status.idle":"2023-05-23T09:17:31.501914Z","shell.execute_reply.started":"2023-05-23T09:17:28.762572Z","shell.execute_reply":"2023-05-23T09:17:31.500901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Prepare for submission","metadata":{}},{"cell_type":"markdown","source":"We prepare the testing data for submission, and apply the same preprocessing steps to it:","metadata":{}},{"cell_type":"code","source":"test_df['img_n'] = test_df[\"img\"].map(lambda img:  cv2.resize(img, (224,224)))\ntest_img_keras = np.array(test_df[\"img_n\"].values.tolist())\ntest_norm = tf.cast(test_img_keras, tf.float32) / 255.0","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:17:48.662936Z","iopub.execute_input":"2023-05-23T09:17:48.663596Z","iopub.status.idle":"2023-05-23T09:17:48.983173Z","shell.execute_reply.started":"2023-05-23T09:17:48.663552Z","shell.execute_reply":"2023-05-23T09:17:48.982065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_norm[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:45:31.287473Z","iopub.execute_input":"2023-05-23T09:45:31.288561Z","iopub.status.idle":"2023-05-23T09:45:31.579909Z","shell.execute_reply.started":"2023-05-23T09:45:31.288504Z","shell.execute_reply":"2023-05-23T09:45:31.578619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we load in our favoured ","metadata":{}},{"cell_type":"code","source":"model_ = keras.models.load_model('/kaggle/input/models/model_vgg_2/model_vgg')","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:17:59.227336Z","iopub.execute_input":"2023-05-23T09:17:59.228590Z","iopub.status.idle":"2023-05-23T09:18:02.718534Z","shell.execute_reply.started":"2023-05-23T09:17:59.228538Z","shell.execute_reply":"2023-05-23T09:18:02.717417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try out a single prediction (awkward due to shape: first axis must be n_samples)","metadata":{}},{"cell_type":"code","source":"# Get a test image\nindex = 18\n# Get segmentation prediction\nseg = model_(tf.reshape(test_norm[index], [-1,224,224,3]))[0]\nseg = np.argmax(seg, axis=2)\n# Plot them\nplt.subplot(1,2,1)\nplt.imshow(test_norm[index])\nplt.subplot(1,2,2)\nplt.imshow(seg, vmin=0, vmax=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:24:00.696759Z","iopub.execute_input":"2023-05-23T09:24:00.697157Z","iopub.status.idle":"2023-05-23T09:24:01.066383Z","shell.execute_reply.started":"2023-05-23T09:24:00.697120Z","shell.execute_reply":"2023-05-23T09:24:01.065285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Iterate over whole test dataset**","metadata":{}},{"cell_type":"code","source":"def predict_test_segmentation(segmentation_model):\n    \"\"\"\n    Predicts the segmentations of the Kaggle test set and puts them into test_df\n    \"\"\"\n    for i, img in enumerate(test_norm):\n        # We need the original shape for the prediction\n        og_shape = np.shape(test_df.at[i, \"img\"])\n        # Do the prediction on preprocessed data\n        seg = segmentation_model(tf.reshape(img, [-1,224,224,3]))[0]\n        seg = np.argmax(seg, axis=2)\n        # Prepare to put it in the dataframe\n        # Have to reverse the shape to get it in (width, height) format!\n        seg_og_shape = cv2.resize(np.asarray(seg, dtype='uint8'), og_shape[:-1][::-1])\n        test_df.at[i, \"seg\"] = seg_og_shape","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:46:47.242528Z","iopub.execute_input":"2023-05-23T09:46:47.243074Z","iopub.status.idle":"2023-05-23T09:47:15.973527Z","shell.execute_reply.started":"2023-05-23T09:46:47.243031Z","shell.execute_reply":"2023-05-23T09:47:15.971934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in range(len(test_norm)):\n#     org_shape = test_norm[idx].shape\n#     x = test_norm[idx]\n#     x=tf.reshape(x, [-1,224,224,3])\n#     d = model_.predict(x)\n#     result=d>0.55# setting the threshold\n#     imclass = np.argmax(result, axis=3)[0,:,:]\n#     imclass_re = cv2.resize(np.asarray(imclass, dtype='uint8'),(test_df.at[idx, \"seg\"].shape[0],test_df.at[idx, \"seg\"].shape[1]))\n#     test_df.at[idx, \"seg\"] = cv2.resize(np.asarray(imclass, dtype='uint8'),(test_df.at[idx, \"seg\"].shape[0],test_df.at[idx, \"seg\"].shape[1]))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:31:52.672810Z","iopub.execute_input":"2023-05-22T21:31:52.673126Z","iopub.status.idle":"2023-05-22T21:33:11.215022Z","shell.execute_reply.started":"2023-05-22T21:31:52.673093Z","shell.execute_reply":"2023-05-22T21:33:11.213934Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a test image\nindex = 18\n# Plot them\nplt.subplot(1,2,1)\nplt.imshow(test_df.at[index, \"img\"])\nplt.subplot(1,2,2)\nplt.imshow(test_df.at[index, \"seg\"], vmin=0, vmax=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T09:48:00.982242Z","iopub.execute_input":"2023-05-23T09:48:00.982860Z","iopub.status.idle":"2023-05-23T09:48:01.313309Z","shell.execute_reply.started":"2023-05-23T09:48:00.982820Z","shell.execute_reply":"2023-05-23T09:48:01.312301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit to competition\nYou don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details.","metadata":{"papermill":{"duration":0.196901,"end_time":"2022-04-12T14:49:19.951018","exception":false,"start_time":"2022-04-12T14:49:19.754117","status":"completed"},"tags":[]}},{"cell_type":"code","source":"generate_submission(test_df)","metadata":{"papermill":{"duration":81.176133,"end_time":"2022-04-12T14:50:41.324425","exception":false,"start_time":"2022-04-12T14:49:20.148292","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T10:03:52.049047Z","iopub.execute_input":"2023-05-23T10:03:52.050162Z","iopub.status.idle":"2023-05-23T10:04:01.550493Z","shell.execute_reply.started":"2023-05-23T10:03:52.050105Z","shell.execute_reply":"2023-05-23T10:04:01.549330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Adversarial attack","metadata":{"papermill":{"duration":0.197466,"end_time":"2022-04-12T14:50:41.721228","exception":false,"start_time":"2022-04-12T14:50:41.523762","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"In this part, we are going to implement an adversarial attack on one of our models.","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Formulation of the attack","metadata":{}},{"cell_type":"markdown","source":"Our task is to perturb an image such that our classification model predicts, with high confidence, that there is a horse and a person in the image while there is actually only an airplane present.\n\nWe tried two different approaches to orchestrate the attack. The first approach we considered was the **Fast Gradient Signed Method** (FGSM). **TODO discussion**\n\nHowever, the FGSM was too rudimentary to reach high confidences in the network for the target label (we are performing a targeted attack not just changing the prediction label). Therefore, we decided to switch to the **Projected Gradient Descent** (PGD) instead. \n\nReasons to choose PGD over FGSM:\n\n- FGSM is designed to be fast but not optimal (the perturbation we get may be large for the effect on the confidence it has).\n- It is easy to do untargeted attacks with FGSM but it is harder for targeted attacks (we would have to search the appropiate epsilon parameter).\n\n**References:** We followed [this tutorial](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) to implement FGSM but later changed to a custom PGD implementation following [this example](https://adversarial-ml-tutorial.org/) from Zico Kolter and Aleksander Madry. The Tensorflow code is inspired by [this link](https://github.com/cleverhans-lab/cleverhans).","metadata":{}},{"cell_type":"markdown","source":"In line of the context of adversarial attacks, which often happen after training and when the model is deployed in the real world (\"in the wild\"), we consider a random image found on the internet containing an airplane. We preprocess it and deliver it to our classification model. As expected, the model is able to accurately predict the presence of the airplane.","metadata":{}},{"cell_type":"code","source":"!wget https://i.insider.com/62dcfe205bf4820019b5ece7?width=700 -O airplane.webp","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.164600Z","iopub.status.idle":"2023-05-22T21:25:27.165398Z","shell.execute_reply.started":"2023-05-22T21:25:27.165125Z","shell.execute_reply":"2023-05-22T21:25:27.165152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the image file in its raw format\nimage_raw = tf.io.read_file('airplane.webp')\n\n# Decode the raw image into a tensor\norig_image = tf.image.decode_image(image_raw)\n\ndef preprocess(image):\n    \"\"\"\n    Preprocesses the image tensor for classification.\n\n    Args:\n        image (tf.Tensor): Input image tensor.\n\n    Returns:\n        tf.Tensor: Preprocessed image tensor.\n    \"\"\"\n    # Resize the image to our chosen size\n    image = tf.image.resize(image, (224, 224))\n\n    # Preprocess the image using the InceptionV3 preprocessing function\n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n\n    # Add a batch dimension to the image tensor (single image per batch)\n    image = image[None, ...]\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.166864Z","iopub.status.idle":"2023-05-22T21:25:27.167689Z","shell.execute_reply.started":"2023-05-22T21:25:27.167400Z","shell.execute_reply":"2023-05-22T21:25:27.167430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess and predict:\npreprocessed_image = preprocess(orig_image)\npred = classification_model.model.predict(preprocessed_image, verbose=False)[0]\n# Get predicted label(s) and convert to string\npred_indices = np.argwhere(pred > 0.5)\npred_label = np.array(labels.tolist())[pred_indices].flatten()\n\n# Print results\nprint(f\"Correct   label(s): aeroplane\")\nprint(f\"Predicted label(s): {pred_label}\")\n\n# Plot image and prediction distribution\nplt.subplots(1, 2, figsize=(10, 3))\nplt.subplot(121)\nplt.imshow(preprocessed_image[0])\nplt.title(\"aeroplane\")\nplt.xticks([])\nplt.yticks([])\n\n# Plot distribution\nplt.subplot(122)\nxt = [i for i in range(len(pred))]\nplt.plot(xt, pred, \"-o\", color=\"red\")\nplt.xticks(xt, labels=labels, rotation=90)\nplt.ylabel(\"Probability\")\nplt.grid()\nplt.axhline(0.5, ls=\"--\", color=\"black\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.169284Z","iopub.status.idle":"2023-05-22T21:25:27.170128Z","shell.execute_reply.started":"2023-05-22T21:25:27.169848Z","shell.execute_reply":"2023-05-22T21:25:27.169877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Loss function and gradient for the attack","metadata":{}},{"cell_type":"markdown","source":"We will now start building our attack on the network. We implement the PGD, explained in detail above:","metadata":{}},{"cell_type":"code","source":"loss_fn = tf.keras.losses.BinaryCrossentropy()\ndef compute_gradient(input_image, target_label):\n    \"\"\"\n    Computes the adversarial gradient that minimizes the loss from the image towards the target label.\n\n    Args:\n        input_image (tf.Tensor): Input image tensor.\n        target_label (tf.Tensor): Target label tensor with shape (1, label_id).\n\n    Returns:\n        tf.Tensor: The adversarial gradient that minimizes the loss from the image towards the target label.\n    \"\"\"\n    with tf.GradientTape() as tape:\n        # Record the gradient operations happening inside this tape context\n        tape.watch(input_image)\n        \n        # Forward pass\n        prediction = classification_model.model(input_image)\n\n        # We're trying to **minimize** the distance between the target (adversarial) label\n        # and the predicted label, which is why we use a negative sign in the loss.\n        # Note: Regularization is not added to the loss function here as we will be\n        # regularizing the perturbation (the gradient) later on.\n        loss = -loss_fn(target_label, prediction)\n\n    # Backward pass\n    # Compute the derivative of the loss with respect to the input image\n    gradient = tape.gradient(loss, input_image)\n\n    return gradient","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.171599Z","iopub.status.idle":"2023-05-22T21:25:27.172438Z","shell.execute_reply.started":"2023-05-22T21:25:27.172151Z","shell.execute_reply":"2023-05-22T21:25:27.172179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_gradient(grad, alpha_norm):\n    \"\"\"\n    Normalizes the gradient using L2 normalization along each dimension of the image.\n\n    Args:\n        grad (tf.Tensor): Input gradient tensor.\n        alpha_norm (float): Scaling factor for the normalized perturbation.\n\n    Returns:\n        tf.Tensor: Normalized perturbation.\n    \"\"\"\n    axis = list(range(1, len(grad.get_shape())))  # [1,2,3]\n\n    # Separately normalize each gradient dimension of the image using the 'axis'\n    optimal_perturbation = tf.math.l2_normalize(grad, axis=axis)\n\n    return alpha_norm * optimal_perturbation\n\ndef projected_gradient_descent(image, alpha_norm, epochs, target_label):\n    \"\"\"\n    Applies the projected gradient descent method to generate an adversarial image.\n\n    Args:\n        image (np.ndarray): Input image array.\n        alpha_norm (float): Scaling factor for the normalized perturbation.\n        epochs (int): Number of optimization steps.\n        target_label (tf.Tensor): Target label tensor with shape (1, label_id).\n\n    Returns:\n        np.ndarray: Adversarial image generated by the projected gradient descent method.\n        np.ndarray: Accumulated perturbation during the optimization process.\n    \"\"\"\n    perturbation_sum = np.zeros(image.shape)\n\n    img_delta = image\n\n    for _ in range(epochs):\n        grad = compute_gradient(img_delta, target_label)\n        optimal_perturbation = normalize_gradient(grad, alpha_norm)\n        img_delta = img_delta + optimal_perturbation\n\n        perturbation_sum = perturbation_sum + optimal_perturbation\n\n    return img_delta, perturbation_sum","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.173978Z","iopub.status.idle":"2023-05-22T21:25:27.174873Z","shell.execute_reply.started":"2023-05-22T21:25:27.174551Z","shell.execute_reply":"2023-05-22T21:25:27.174578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_images(adversarial_image, description, perturbations, original_label, original_confidence):\n    \"\"\"Auxiliary function for plotting the adverserial attack for visualization.\"\"\"\n    origFigSize = plt.rcParams['figure.figsize']\n    plt.rcParams['figure.figsize'] = [15, 3]\n    plt.figure()\n\n    # 1 - Original image\n    plt.subplot(1,4,1)\n    plt.imshow(*preprocessed_image)\n    plt.title(f'Original \\n {original_label} : {original_confidence}')\n    plt.xticks([])\n    plt.yticks([])\n    \n    # 2 - The perturbations\n    plt.subplot(1,4,2)\n    plt.imshow(*(perturbations*10))\n    plt.title(f'{description}')\n    plt.xticks([])\n    plt.yticks([])\n\n    # 3 - The adversarial image and predictions\n    plt.subplot(1,4,3)\n    plt.imshow(*(adversarial_image))\n    plt.xticks([])\n    plt.yticks([])\n\n    # Get the prediction of the model for the title\n    adv_image_probs = classification_model.model.predict(adversarial_image, verbose=0)[0]\n    adv_pred_indices = np.argwhere(adv_image_probs > threshold)\n    adv_image_probs = np.round(adv_image_probs, 2)\n    adv_pred_label = np.array(labels.tolist())[adv_pred_indices].flatten() \n    plt.title(f'Adversarial \\n {adv_pred_label} : {adv_image_probs[adv_pred_indices].flatten()}')\n    \n    # 4 - Show distributions\n    plt.subplot(1,4,4)\n    xt = [i for i in range(len(adv_image_probs))]\n    plt.plot(xt, adv_image_probs, \"-o\", color=\"red\")\n    plt.xticks(xt, labels=labels, rotation=90)\n    plt.ylabel(\"Probability\")\n    plt.grid()\n    plt.axhline(0.5, ls=\"--\", color=\"black\")\n    \n    plt.show() \n    plt.rcParams['figure.figsize'] = origFigSize","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.176370Z","iopub.status.idle":"2023-05-22T21:25:27.177157Z","shell.execute_reply.started":"2023-05-22T21:25:27.176889Z","shell.execute_reply":"2023-05-22T21:25:27.176915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 20\nthreshold = 0.5\n\n# Get prediction labels\npred_indices = np.argwhere(image_probs > threshold)\npred_label = np.array(labels.tolist())[pred_indices].flatten()\nimage_probs = np.round(image_probs, 2)\n\ncateg_encoder_layer = tf.keras.layers.CategoryEncoding(\n          num_tokens=NUM_CLASSES, output_mode=\"multi_hot\")\n\n# Target labels are horse and person:\ntarget_label = categ_encoder_layer([12, 14])\n\n# Create a batch with a single image label\ntarget_label = tf.reshape(target_label, (1, NUM_CLASSES))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.178679Z","iopub.status.idle":"2023-05-22T21:25:27.179523Z","shell.execute_reply.started":"2023-05-22T21:25:27.179236Z","shell.execute_reply":"2023-05-22T21:25:27.179265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Applying the adversarial attack","metadata":{}},{"cell_type":"markdown","source":"Now that we built the appropriate loss function and the PGD method, let's see how this perturbations affects the original image of the airplane and whether the changes are actually perceptible. The perturbations are magnified below for visualization purposes: we note that this does not mean that it is actually visible by humans when not magnified.","metadata":{}},{"cell_type":"code","source":"# Decrease me to hide the perturbation trading off target confidence\nalphas = [0.01, 0.1, 0.5]\ndescriptions = [f'Perturbation (magnified x10) \\n L2 norm α = {a:0.2f}' for a in alphas]\n\n# Increase me to get a more accurate estimation of the perturbation\nEPOCHS = 40\n\nfor i, eps in enumerate(alphas): \n    adv_x, perturbations = projected_gradient_descent(preprocessed_image, eps, EPOCHS, target_label)\n    display_images(adv_x, descriptions[i], perturbations, str(pred_label), str(image_probs[pred_indices].flatten()))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.181043Z","iopub.status.idle":"2023-05-22T21:25:27.181922Z","shell.execute_reply.started":"2023-05-22T21:25:27.181601Z","shell.execute_reply":"2023-05-22T21:25:27.181630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Discussion","metadata":{}},{"cell_type":"markdown","source":"We now present a discussion regarding the adversarial attack we implemented.\n\nOne could ask **whether such a white-box attack is realistic**. While most models are currently released as black-box models, a possible leak of model weights (in the case of propietary models) or the increasing usage of open source models with pretrained weights would definitely pose a threat for the security of those systems. Once the attacker identified the model and weights used, the attacker would be able to replicate the enviroment and craft adversarial inputs such that systems, which are not designed to be protected against such attacks, would be vulnerable, even when sitting behind an opaque API. This is also really important now that larger models are being released and not everyone is capable (by money or time constraints) of retraining the weights. While adversarial attacks were discussed here in the context of computer vision, we note that similar dangers exist in other AI fields employing deep learning. Large language models can similarly be misused in case their weights are leaked, and [the leak of Meta's LLaMa model a few months ago](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse) demonstrates that such a scenario can be turned into a reality.\n\nComing back to our attack, the **adversary was successful** in perturbing the images such that they changed the prediction of the model to the deceptive label with almost $100\\%$ confidence, while still being clearly recognisable for a human observer by the original class. \n\nWe remark that this means that the initial CNN that we trained is in some sense **unreliable**. At least it demonstrates that we should not trust the CNN blindly, and while chances are small that real-life images will disrupt the CNN, one should be prepared in case such an event happens. For instance, if a similar CNN was trained in a medical environment, inaccurate predictions can have drastic consequences. However, we do note that such an environment is likely less threatened by actual adversarial *attacks*, since it is likely that such models are not publicly accessible and there are fewer cases where attackers could benefit from such an attack. Moreover, patients themselves benefit from the model and its predictions, so they are likely not going to try to attack the model. \n\nTo **increase the robustness** of the CNN to protect it against attacks, we do not think that there is a practical defense against all possible attacks when the attacker has the model weights, but a deterrent would be to keep the weights as private as possible. This is a vulnerability shared by all machine learning models that learn by optimisation and can not be ascribed to metrics such as accuracy or any other performance measure. However, we can defend the model from transfer attacks where the attacker only has similar weights or the architecture of the model. There is also the adversarial accuracy metric that tests whether a datapoint and its neighbouring datapoints have a similar prediction as it would be harder to find an adversarial input that maximises the loss. Furthermore, there exist benchmarking datasets online, which we could use to get an idea of how vulnerable our system is, in case we wish to deploy the model in real-life applications.","metadata":{}},{"cell_type":"markdown","source":"# 5. Discussion\nFinally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision.","metadata":{"papermill":{"duration":0.195695,"end_time":"2022-04-12T14:50:42.117581","exception":false,"start_time":"2022-04-12T14:50:41.921886","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# A. Classification architectures","metadata":{}},{"cell_type":"markdown","source":"Here, we provide the complete code that we used to compare different classification models. This appendix is divided into four parts, related to the different architectures (the architectures were discussed at the beginning of Section 2):\n1. MobileNetV2 (Section A.1)\n2. VGG16 (Section A.2)\n3. InceptionV3 (Section A.3)\n4. MobileNet (Section A.4)\n5. Comparison between different architectures\n\nEach section is further subdivided into three subsections, which are related to the different training mechanisms that we considered:\n\n1. Transfer learning (Section A.X.1)\n2. Transfer learning with fine-tuning (Section A.X.2)\n3. Training the weights from scratch (Section A.X.3)\n\nThe specific architecture designs are shown at the beginning of each subsection. The details of the training, hyperparameter selection and a comparison between the training methods are discussed in Section A.1 for the MobileNetV2 architecture. The other sections make use of very similar set-ups and hyperparameter settings, so we only provide the necessary code there and readers are encouraged to read through Section A.1 for explanation. ","metadata":{}},{"cell_type":"markdown","source":"## A.1 MobileNetV2","metadata":{}},{"cell_type":"markdown","source":"### A.1.1 Transfer learning","metadata":{}},{"cell_type":"code","source":"# Specify where the trained model should be saved:\nSAVE_LOCATION = \"/kaggle/working/mobv2_m1_best.h5\"\n\nclass MobileNetV2_Model:\n    \"\"\"\n    A class representing a MobileNetV2 model for image classification.\n\n    Attributes\n    ----------\n    model : tf.keras.Model\n        The MobileNetV2 model.\n\n    Methods\n    -------\n    __init__()\n        Initializes the MobileNetV2 model.\n    fit(X, y)\n        Trains the MobileNetV2 model with training images X and labels y.\n    predict(X)\n        Performs predictions using the MobileNetV2 model.\n    __call__(X)\n        Calls the predict() method.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the MobileNetV2 model.\n\n        This function constructs a MobileNetV2 model with pre-trained weights from ImageNet.\n        The fully connected layers of the original model are replaced with a new flattened layer\n        and a dense layer with 20 units and sigmoid activation.\n\n        \"\"\"\n        # Load in the MobileNetV2 architecture from Keras, use weights from Imagenet\n        mobv2_model = tf.keras.applications.MobileNetV2(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            weights=\"imagenet\"\n        )\n\n        # Freeze all layers of the MobileNetV2 model\n        for l in mobv2_model.layers:\n            l.trainable = False\n\n        # Add new layer on top of the MobileNetV2 model to process our dataset\n        new_l = tf.keras.layers.Flatten()(mobv2_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=OUT_ACTIVATION)(new_l)\n\n        # Create a new model with the modified layers\n        self.model = tf.keras.Model(inputs=mobv2_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the MobileNetV2 model.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n        y : np.array\n            The target labels as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        X_train = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n\n        # Compile the model with settings for optimization and loss calculation\n        self.model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-4),\n            loss=\"binary_crossentropy\",\n            metrics=[\"binary_accuracy\"]\n        )\n\n        # Set up callbacks for early stopping and saving the best model\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=SAVE_LOCATION,\n                monitor=\"val_loss\",\n                save_best_only=True\n            )\n        ]\n\n        # Train the model\n        history = self.model.fit(\n            X_train, y,\n            epochs=30,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n        \n        # Store the information obtained during training as field\n        self.history = history\n\n    def predict(self, X):\n        \"\"\"\n        Performs predictions using the MobileNetV2 model.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n        return self.model.predict(X, verbose=0)\n\n    def __call__(self, X):\n        \"\"\"\n        Calls the predict() method.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n        return self.model.predict(X, verbose=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.183486Z","iopub.status.idle":"2023-05-22T21:25:27.184346Z","shell.execute_reply.started":"2023-05-22T21:25:27.184075Z","shell.execute_reply":"2023-05-22T21:25:27.184102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, define a new instance of the model, and train it on the Pascal VOC dataset:","metadata":{}},{"cell_type":"code","source":"# Define an instance:\nmobv2_m1 = MobileNetV2_Model()\n## Show complete architecture -- lengthy output!\n# mobv2_m1.model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-05-22T21:25:27.186077Z","iopub.status.idle":"2023-05-22T21:25:27.186924Z","shell.execute_reply.started":"2023-05-22T21:25:27.186609Z","shell.execute_reply":"2023-05-22T21:25:27.186636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on Pascal VOC dataset\nmobv2_m1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.188431Z","iopub.status.idle":"2023-05-22T21:25:27.189351Z","shell.execute_reply.started":"2023-05-22T21:25:27.189013Z","shell.execute_reply":"2023-05-22T21:25:27.189043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize the training by plotting:","metadata":{}},{"cell_type":"code","source":"show_training(mobv2_m1.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.190832Z","iopub.status.idle":"2023-05-22T21:25:27.191683Z","shell.execute_reply.started":"2023-05-22T21:25:27.191387Z","shell.execute_reply":"2023-05-22T21:25:27.191414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.1.2 Transfer learning and finetuning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/mobv2_m2_best.h5\"\n\nclass MobileNetV2_fine_Model:\n    \"\"\"\n    A class representing a fine-tuned MobileNetV2 model for image classification.\n\n    Attributes\n    ----------\n    model : tf.keras.Model\n        The fine-tuned MobileNetV2 model.\n\n    Methods\n    -------\n    __init__(X, y)\n        Initializes and trains the fine-tuned MobileNetV2 model.\n    predict(X)\n        Performs predictions using the fine-tuned MobileNetV2 model.\n    __call__(X)\n        Calls the predict() method.\n\n    \"\"\"\n\n    def __init__(self, X, y):\n        \"\"\"\n        Initializes and trains the fine-tuned MobileNetV2 model.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n        y : np.array\n            The target labels as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        base_model = tf.keras.applications.MobileNetV2(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            weights=\"imagenet\"\n        )\n\n        # Freeze all layers of the base MobileNetV2 model\n        base_model.trainable = False\n\n        # Add new layers on top of the base model\n        new_l = tf.keras.layers.Flatten()(base_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n\n        # Create the new model with the modified layers\n        model = tf.keras.Model(base_model.input, new_l)\n\n        X_train = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n\n        # Compile and train the model with the first set of hyperparameters\n        model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-4),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.001),\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=SAVE_LOCATION,\n                monitor=\"val_loss\",\n                save_best_only=True\n            )\n        ]\n\n        pre_history = model.fit(\n            X_train, y,\n            epochs=20,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n\n        # Allow ALL layers of the base model to be trainable\n        base_model.trainable = True\n\n        # Compile and train the model with the second set of hyperparameters\n        model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-6),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        history = model.fit(\n            X_train, y,\n            epochs=20,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n\n        self.model = model\n        self.pre_history = pre_history\n        self.history = history\n\n    def predict(self, X):\n        \"\"\"\n        Performs predictions using the fine-tuned MobileNetV2 model.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n        return self.model.predict(preprocessed_X, verbose=0)\n\n    def __call__(self, X):\n        \"\"\"\n        Calls the predict() method.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n        return self.model.predict(preprocessed_X, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.193271Z","iopub.status.idle":"2023-05-22T21:25:27.194137Z","shell.execute_reply.started":"2023-05-22T21:25:27.193835Z","shell.execute_reply":"2023-05-22T21:25:27.193863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobv2_m2 = MobileNetV2_fine_Model(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.195579Z","iopub.status.idle":"2023-05-22T21:25:27.196434Z","shell.execute_reply.started":"2023-05-22T21:25:27.196143Z","shell.execute_reply":"2023-05-22T21:25:27.196172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(mobv2_m2.pre_history.history, acc_key=\"binary_accuracy\")\nshow_training(mobv2_m2.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.197960Z","iopub.status.idle":"2023-05-22T21:25:27.198812Z","shell.execute_reply.started":"2023-05-22T21:25:27.198510Z","shell.execute_reply":"2023-05-22T21:25:27.198540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.1.3 Training from scratch","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/mobv2_m3_best.h5\"\n\nclass MobileNetV2_scratch_Model:\n    \"\"\"\n    A class representing a MobileNetV2 model trained from scratch for image classification.\n\n    Attributes\n    ----------\n    model : tf.keras.Model\n        The MobileNetV2 model trained from scratch.\n\n    Methods\n    -------\n    __init__()\n        Initializes the MobileNetV2 model trained from scratch.\n    fit(X, y)\n        Trains the MobileNetV2 model on the given dataset.\n    predict(X)\n        Performs predictions using the MobileNetV2 model.\n    __call__(X)\n        Calls the predict() method.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the MobileNetV2 model trained from scratch.\n\n        \"\"\"\n        mobv2_model = tf.keras.applications.MobileNetV2(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            weights=None\n        )\n\n        # Add new layers on top of the MobileNetV2 model\n        new_l = tf.keras.layers.Flatten()(mobv2_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n\n        # Create the model with the modified layers\n        self.model = tf.keras.Model(inputs=mobv2_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the MobileNetV2 model on the given dataset.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n        y : np.array\n            The target labels as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        X_train = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n\n        self.model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-5),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.01),\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=SAVE_LOCATION,\n                monitor=\"val_loss\",\n                save_best_only=True\n            )\n        ]\n\n        history = self.model.fit(\n            X_train, y,\n            epochs=50,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n        \n        self.history = history\n\n    def predict(self, X):\n        \"\"\"\n        Performs predictions using the MobileNetV2 model.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        preprocessed_X = tf.keras.applications.mobilenet_v2.preprocess_input(X)\n        return self.model.predict(preprocessed_X, verbose=0)\n\n    def __call__(self, X):\n        \"\"\"\n        Calls the predict() method.\n\n        Parameters\n        ----------\n        X : np.array\n            The input images as a numpy array with shape (num_samples, height, width, channels).\n\n        Returns\n        -------\n        np.array\n            The predicted class probabilities as a numpy array with shape (num_samples, num_classes).\n\n        \"\"\"\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.200422Z","iopub.status.idle":"2023-05-22T21:25:27.201321Z","shell.execute_reply.started":"2023-05-22T21:25:27.201018Z","shell.execute_reply":"2023-05-22T21:25:27.201048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobv2_m3 = MobileNetV2_scratch_Model()\nmobv2_m3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.202876Z","iopub.status.idle":"2023-05-22T21:25:27.203723Z","shell.execute_reply.started":"2023-05-22T21:25:27.203432Z","shell.execute_reply":"2023-05-22T21:25:27.203462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(mobv2_m3.history.history, acc_key = \"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.205216Z","iopub.status.idle":"2023-05-22T21:25:27.206097Z","shell.execute_reply.started":"2023-05-22T21:25:27.205789Z","shell.execute_reply":"2023-05-22T21:25:27.205817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A.2 VGG16","metadata":{}},{"cell_type":"markdown","source":"### A.2.1 Transfer learning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/vgg_m1_best.h5\"\n\nclass VGG16_Model:\n    def __init__(self):\n        vgg_model = tf.keras.applications.VGG16(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            weights=\"imagenet\"\n        )\n\n        # Freeze the layers of the VGG16 model\n        for l in vgg_model.layers:\n            l.trainable = False\n\n        # Add new layers on top of the VGG16 model\n        new_l = tf.keras.layers.Flatten()(vgg_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n\n        # Create the model with the modified layers\n        self.model = tf.keras.Model(inputs=vgg_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.vgg16.preprocess_input(X)\n\n        self.model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-5),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=SAVE_LOCATION,\n                monitor=\"val_loss\",\n                save_best_only=True\n            )\n        ]\n\n        history = self.model.fit(\n            X_train, y,\n            epochs=20,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n        \n        self.history = history\n\n    def predict(self, X):\n        preprocessed_X = tf.keras.applications.vgg16.preprocess_input(X)\n        return self.model.predict(preprocessed_X, verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.207654Z","iopub.status.idle":"2023-05-22T21:25:27.208511Z","shell.execute_reply.started":"2023-05-22T21:25:27.208217Z","shell.execute_reply":"2023-05-22T21:25:27.208253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_m1 = VGG16_Model()\nvgg_m1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.210106Z","iopub.status.idle":"2023-05-22T21:25:27.210943Z","shell.execute_reply.started":"2023-05-22T21:25:27.210636Z","shell.execute_reply":"2023-05-22T21:25:27.210664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(vgg_m1.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.212488Z","iopub.status.idle":"2023-05-22T21:25:27.213336Z","shell.execute_reply.started":"2023-05-22T21:25:27.213038Z","shell.execute_reply":"2023-05-22T21:25:27.213066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.2.2 Transfer learning with fine-tuning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/vgg_m2_best.h5\"\n\nclass VGG16_fine_Model:\n    def __init__(self, X, y):\n        base_model = tf.keras.applications.VGG16(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            weights=\"imagenet\"\n        )\n\n        # Freeze the layers of the VGG16 base model\n        base_model.trainable = False\n\n        # Add new layers on top of the VGG16 base model\n        new_l = tf.keras.layers.Flatten()(base_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n\n        # Create the model with the modified layers\n        model = tf.keras.Model(inputs=base_model.input, outputs=new_l)\n\n        X_train = tf.keras.applications.vgg16.preprocess_input(X)\n\n        model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-4),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=SAVE_LOCATION,\n                monitor=\"val_loss\",\n                save_best_only=True\n            )\n        ]\n\n        pre_history = model.fit(\n            X_train, y,\n            epochs=20,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n\n        # Enable training of all layers in the VGG16 model\n        base_model.trainable = True\n\n        model.compile(\n            run_eagerly=True,\n            optimizer=tf.keras.optimizers.Adam(1e-6),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy']\n        )\n\n        history = model.fit(\n            X_train, y,\n            epochs=20,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=callbacks\n        )\n\n        self.model = model\n        self.pre_history = pre_history\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(preprocessed_X, verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.214904Z","iopub.status.idle":"2023-05-22T21:25:27.215735Z","shell.execute_reply.started":"2023-05-22T21:25:27.215437Z","shell.execute_reply":"2023-05-22T21:25:27.215466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_m2 = VGG16_fine_Model(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.217215Z","iopub.status.idle":"2023-05-22T21:25:27.218067Z","shell.execute_reply.started":"2023-05-22T21:25:27.217761Z","shell.execute_reply":"2023-05-22T21:25:27.217791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(vgg_m2.pre_history.history, acc_key=\"binary_accuracy\")\nshow_training(vgg_m2.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.219576Z","iopub.status.idle":"2023-05-22T21:25:27.220420Z","shell.execute_reply.started":"2023-05-22T21:25:27.220128Z","shell.execute_reply":"2023-05-22T21:25:27.220157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.2.3 Training from scratch","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/vgg_m3_best.h5\"\n\nclass VGG16_scratch_Model:\n    def __init__(self):\n        vgg_model = tf.keras.applications.MobileNetV2(include_top=False, input_shape=(224,224,3), weights=None)\n        new_l = tf.keras.layers.Flatten()(vgg_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        self.model = tf.keras.Model(inputs=vgg_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.vgg16.preprocess_input(X)\n        self.model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_loss\", save_best_only=True)]\n        history = self.model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.vgg16.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.221897Z","iopub.status.idle":"2023-05-22T21:25:27.222673Z","shell.execute_reply.started":"2023-05-22T21:25:27.222401Z","shell.execute_reply":"2023-05-22T21:25:27.222428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_m3 = VGG16_scratch_Model()\nvgg_m3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.224111Z","iopub.status.idle":"2023-05-22T21:25:27.224930Z","shell.execute_reply.started":"2023-05-22T21:25:27.224629Z","shell.execute_reply":"2023-05-22T21:25:27.224657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(vgg_m3.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.226319Z","iopub.status.idle":"2023-05-22T21:25:27.227191Z","shell.execute_reply.started":"2023-05-22T21:25:27.226911Z","shell.execute_reply":"2023-05-22T21:25:27.226940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A.3 InceptionV3","metadata":{}},{"cell_type":"markdown","source":"### A.3.1 Transfer learning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/incv3_m1_best.h5\"\n\nclass InceptionV3_Model:\n    def __init__(self):\n        incv3_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n        for l in incv3_model.layers:\n            l.trainable = False\n        new_l = tf.keras.layers.Flatten()(incv3_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        self.model = tf.keras.Model(inputs=incv3_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.inception_v3.preprocess_input(X)\n        self.model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=3, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_binary_accuracy\", save_best_only=True)]\n        history = self.model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.inception_v3.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.228746Z","iopub.status.idle":"2023-05-22T21:25:27.229629Z","shell.execute_reply.started":"2023-05-22T21:25:27.229318Z","shell.execute_reply":"2023-05-22T21:25:27.229346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incv3_m1 = InceptionV3_Model()\nincv3_m1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.231116Z","iopub.status.idle":"2023-05-22T21:25:27.231916Z","shell.execute_reply.started":"2023-05-22T21:25:27.231621Z","shell.execute_reply":"2023-05-22T21:25:27.231648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(incv3_m1.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.233307Z","iopub.status.idle":"2023-05-22T21:25:27.234103Z","shell.execute_reply.started":"2023-05-22T21:25:27.233832Z","shell.execute_reply":"2023-05-22T21:25:27.233859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.3.2 Transfer learning and fine-tuning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/incv3_m2_best.h5\"\n\nclass InceptionV3_fine_Model:\n    def __init__(self, X, y):\n        base_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n        base_model.trainable = False\n\n        new_l = tf.keras.layers.Flatten()(base_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        model = tf.keras.Model(base_model.input, new_l)\n\n        X_train = tf.keras.applications.inception_v3.preprocess_input(X)\n\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=3, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_binary_accuracy\", save_best_only=True)]\n        pre_history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.pre_history = pre_history\n\n        base_model.trainable = True\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-6), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.model = model\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.inception_v3.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.235513Z","iopub.status.idle":"2023-05-22T21:25:27.236311Z","shell.execute_reply.started":"2023-05-22T21:25:27.236041Z","shell.execute_reply":"2023-05-22T21:25:27.236068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incv3_m2 = InceptionV3_fine_Model(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.237724Z","iopub.status.idle":"2023-05-22T21:25:27.238504Z","shell.execute_reply.started":"2023-05-22T21:25:27.238239Z","shell.execute_reply":"2023-05-22T21:25:27.238266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(incv3_m2.history.history, acc_key=\"binary_accuracy\")\nshow_training(incv3_m2.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.239981Z","iopub.status.idle":"2023-05-22T21:25:27.240767Z","shell.execute_reply.started":"2023-05-22T21:25:27.240481Z","shell.execute_reply":"2023-05-22T21:25:27.240508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.3.3 Training from scratch","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/incv3_m3_best.h5\"\n\nclass InceptionV3_scratch_Model:\n\n    def __init__(self):\n        incv3_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(224,224,3), weights=None)\n        new_l = tf.keras.layers.Flatten()(incv3_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        self.model = tf.keras.Model(inputs=incv3_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.inception_v3.preprocess_input(X)\n        self.model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=3, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_binary_accuracy\", save_best_only=True)]\n        history = self.model.fit(X_train, y, epochs=30,batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.inception_v3.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.242193Z","iopub.status.idle":"2023-05-22T21:25:27.242994Z","shell.execute_reply.started":"2023-05-22T21:25:27.242715Z","shell.execute_reply":"2023-05-22T21:25:27.242743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incv3_m3 = InceptionV3_scratch_Model()\nincv3_m3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.244439Z","iopub.status.idle":"2023-05-22T21:25:27.245225Z","shell.execute_reply.started":"2023-05-22T21:25:27.244960Z","shell.execute_reply":"2023-05-22T21:25:27.244986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(incv3_m3.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.246683Z","iopub.status.idle":"2023-05-22T21:25:27.247530Z","shell.execute_reply.started":"2023-05-22T21:25:27.247243Z","shell.execute_reply":"2023-05-22T21:25:27.247272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A.4 MobileNet","metadata":{}},{"cell_type":"markdown","source":"### A.4.1 Transfer learning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/mobv1_m1_best.h5\"\n\nclass MobileNet_Model:\n    def __init__(self):\n        mob_model = tf.keras.applications.MobileNet(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n        for l in mob_model.layers:\n            l.trainable = False\n        new_l = tf.keras.layers.Flatten()(mob_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        self.model = tf.keras.Model(inputs=mob_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.mobilenet.preprocess_input(X)\n        self.model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_loss\", save_best_only=True)]\n        history = self.model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.mobilenet.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.249075Z","iopub.status.idle":"2023-05-22T21:25:27.249916Z","shell.execute_reply.started":"2023-05-22T21:25:27.249617Z","shell.execute_reply":"2023-05-22T21:25:27.249644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mob_m1 = MobileNet_Model()\nmob_m1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.251316Z","iopub.status.idle":"2023-05-22T21:25:27.252117Z","shell.execute_reply.started":"2023-05-22T21:25:27.251847Z","shell.execute_reply":"2023-05-22T21:25:27.251875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(mob_m1.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.253560Z","iopub.status.idle":"2023-05-22T21:25:27.254388Z","shell.execute_reply.started":"2023-05-22T21:25:27.254125Z","shell.execute_reply":"2023-05-22T21:25:27.254152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.4.2 Transfer learning and fine-tuning","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/mobv1_m2_best.h5\"\n\nclass MobileNet_fine_Model:\n\n    def __init__(self, X, y):\n        base_model = tf.keras.applications.MobileNet(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n        base_model.trainable = False\n\n        new_l = tf.keras.layers.Flatten()(base_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        model = tf.keras.Model(base_model.input, new_l)\n\n        X_train = tf.keras.applications.mobilenet.preprocess_input(X)\n\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_loss\", save_best_only=True)]\n        pre_history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.pre_history = pre_history\n\n        base_model.trainable = True\n        model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-6), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        history = model.fit(X_train, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n        self.model = model\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.mobilenet.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.255806Z","iopub.status.idle":"2023-05-22T21:25:27.256586Z","shell.execute_reply.started":"2023-05-22T21:25:27.256318Z","shell.execute_reply":"2023-05-22T21:25:27.256344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mob_m2 = MobileNet_fine_Model(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.258008Z","iopub.status.idle":"2023-05-22T21:25:27.258851Z","shell.execute_reply.started":"2023-05-22T21:25:27.258544Z","shell.execute_reply":"2023-05-22T21:25:27.258572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(mob_m2.pre_history.history, acc_key=\"binary_accuracy\")\nshow_training(mob_m2.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.260500Z","iopub.status.idle":"2023-05-22T21:25:27.261313Z","shell.execute_reply.started":"2023-05-22T21:25:27.261042Z","shell.execute_reply":"2023-05-22T21:25:27.261068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A.4.3 Training from scratch","metadata":{}},{"cell_type":"code","source":"SAVE_LOCATION = \"/kaggle/working/mobv1_m3_best.h5\"\n\nclass MobileNet_scratch_Model:\n\n    def __init__(self):\n        mob_model = tf.keras.applications.MobileNet(include_top=False, input_shape=(224,224,3), weights=None)\n        new_l = tf.keras.layers.Flatten()(mob_model.output)\n        new_l = tf.keras.layers.Dense(20, activation=\"sigmoid\")(new_l)\n        self.model = tf.keras.Model(inputs=mob_model.input, outputs=new_l)\n\n    def fit(self, X, y):\n        X_train = tf.keras.applications.mobilenet.preprocess_input(X)\n        self.model.compile(run_eagerly=True, optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\n        callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=2, min_delta=0.01),\n                     tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_LOCATION, monitor=\"val_binary_accuracy\", save_best_only=True)]\n        history = self.model.fit(X_train, y, epochs=30,batch_size=32, validation_split=0.25, callbacks=callbacks)\n        self.history = history\n\n    def predict(self, X):\n        return self.model.predict(tf.keras.applications.mobilenet.preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.262866Z","iopub.status.idle":"2023-05-22T21:25:27.263649Z","shell.execute_reply.started":"2023-05-22T21:25:27.263369Z","shell.execute_reply":"2023-05-22T21:25:27.263396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mob_m3 = MobileNet_scratch_Model()\nmob_m3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.265175Z","iopub.status.idle":"2023-05-22T21:25:27.266135Z","shell.execute_reply.started":"2023-05-22T21:25:27.265798Z","shell.execute_reply":"2023-05-22T21:25:27.265838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training(mob_m3.history.history, acc_key=\"binary_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.267765Z","iopub.status.idle":"2023-05-22T21:25:27.268639Z","shell.execute_reply.started":"2023-05-22T21:25:27.268339Z","shell.execute_reply":"2023-05-22T21:25:27.268370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A.5 Comparison between architectures","metadata":{}},{"cell_type":"markdown","source":"We now compare the different models (and different training methods) considered above to choose our final architecture. We sort the validation accuracies to make a fair comparison. ","metadata":{}},{"cell_type":"code","source":"classification_models = [mobv2_m1, mobv2_m2, mobv2_m3, vgg_m1, vgg_m2, vgg_m3, incv3_m1, incv3_m2, incv3_m3, mob_m1, mob_m2, mob_m3]\nmodel_names = [\"MobileNetV2\", \"VGG16\", \"InceptionV3\", \"MobileNet\"]\nclassification_names = []\nfor name in model_names:\n    classification_names.append(name + \" (TL)\")\n    classification_names.append(name + \" (TL + FT)\")\n    classification_names.append(name + \" (scratch)\")\n\nclassification_names = np.array(classification_names)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.270211Z","iopub.status.idle":"2023-05-22T21:25:27.271138Z","shell.execute_reply.started":"2023-05-22T21:25:27.270832Z","shell.execute_reply":"2023-05-22T21:25:27.270866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Collect the validation accuracies:","metadata":{}},{"cell_type":"code","source":"classification_val_accs = []\nfor i, model in enumerate(classification_models):\n    name = classification_names[i]\n    history = model.history.history\n    val_acc = np.max(history[\"val_binary_accuracy\"])\n    classification_val_accs.append(val_acc)\n    \nclassification_val_accs = np.array(classification_val_accs)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.272633Z","iopub.status.idle":"2023-05-22T21:25:27.273500Z","shell.execute_reply.started":"2023-05-22T21:25:27.273200Z","shell.execute_reply":"2023-05-22T21:25:27.273228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sort validation accuracies, from highest to lowest.","metadata":{}},{"cell_type":"code","source":"sort_ind = np.argsort(classification_val_accs)[::-1]\n\nsorted_classification_val_accs = classification_val_accs[sort_ind]\nsorted_classification_names    = classification_names[sort_ind]","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.275169Z","iopub.status.idle":"2023-05-22T21:25:27.276076Z","shell.execute_reply.started":"2023-05-22T21:25:27.275784Z","shell.execute_reply":"2023-05-22T21:25:27.275812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xticks = [i for i in range(len(classification_names))]\n\nplt.figure(figsize=(11,3))\nplt.plot(xticks, sorted_classification_val_accs, '-o', color=\"blue\")\nplt.xticks(xticks, labels=sorted_classification_names, rotation=45)\nplt.grid()\nplt.ylabel(\"Validation accuracy\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.277543Z","iopub.status.idle":"2023-05-22T21:25:27.278416Z","shell.execute_reply.started":"2023-05-22T21:25:27.278101Z","shell.execute_reply":"2023-05-22T21:25:27.278130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.279995Z","iopub.status.idle":"2023-05-22T21:25:27.280870Z","shell.execute_reply.started":"2023-05-22T21:25:27.280558Z","shell.execute_reply":"2023-05-22T21:25:27.280586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/sorted_classification_val_accs.pickle', 'wb') as handle:\n    pickle.dump(sorted_classification_val_accs, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('/kaggle/working/sorted_classification_names.pickle', 'wb') as handle:\n    pickle.dump(sorted_classification_names, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:25:27.282460Z","iopub.status.idle":"2023-05-22T21:25:27.283360Z","shell.execute_reply.started":"2023-05-22T21:25:27.283052Z","shell.execute_reply":"2023-05-22T21:25:27.283083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}