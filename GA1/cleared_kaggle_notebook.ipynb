{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble: install and import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important notice** due to OutOfMemory errors, we use different input data. Below, we define the paths for this Kaggle notebook. We also import preprocessed test images (see discussion in notebook) as well as 2 template images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:52:02.298762Z",
     "iopub.status.busy": "2023-04-12T13:52:02.298349Z",
     "iopub.status.idle": "2023-04-12T13:52:02.303994Z",
     "shell.execute_reply": "2023-04-12T13:52:02.302882Z",
     "shell.execute_reply.started": "2023-04-12T13:52:02.298720Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/traindata\"\n",
    "test_path = \"/kaggle/input/test-preprocessed\"\n",
    "template_path = \"/kaggle/input/templates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:50:16.615177Z",
     "iopub.status.busy": "2023-04-12T13:50:16.614493Z",
     "iopub.status.idle": "2023-04-12T13:50:44.545492Z",
     "shell.execute_reply": "2023-04-12T13:50:44.544169Z",
     "shell.execute_reply.started": "2023-04-12T13:50:16.615141Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install deepface mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-12T13:52:06.622112Z",
     "iopub.status.busy": "2023-04-12T13:52:06.621404Z",
     "iopub.status.idle": "2023-04-12T13:52:15.101652Z",
     "shell.execute_reply": "2023-04-12T13:52:15.100578Z",
     "shell.execute_reply.started": "2023-04-12T13:52:06.622073Z"
    },
    "papermill": {
     "duration": 0.230891,
     "end_time": "2021-03-08T07:57:06.335029",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.104138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import numpy as np # linear algebra\n",
    "import cv2 # OpenCV package\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "import seaborn as sns # diffeerent plotting library\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle  # import pickle, to read and save variables\n",
    "import time # measure elapsed time in execution\n",
    "from typing import Callable # type hinting\n",
    "from tqdm import tqdm # progress bar\n",
    "# Scikit learn\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_predict\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, multilabel_confusion_matrix, matthews_corrcoef\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.exposure import rescale_intensity\n",
    "from urllib import request # module for opening HTTP requests\n",
    "# MTCNN, or Multi-Task Cascaded Convolutional Neural Networks\n",
    "from mtcnn.mtcnn import MTCNN  \n",
    "# DeepFace\n",
    "from deepface import DeepFace\n",
    "from deepface.detectors import FaceDetector\n",
    "# PyTorch\n",
    "import torch.optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import VGG from Keras for final pipeline\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "# KUL H02A5a Computer Vision: Group Assignment 1\n",
    "\n",
    "Student numbers: <span style=\"color:red\">r0708518, r0927391, r0925509, r0924356, r0912639</span>.\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022868,
     "end_time": "2021-03-08T07:57:06.382109",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.359241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature representations: then and now\n",
    "\n",
    "Welcome to the workshop on feature representations and their use in face recognition! This tutorial will take you from the basics of loading images to building handcrafted feature representations of faces and eventually introduces deep learning as a way to generate features.\n",
    "This notebook is structured as follows:\n",
    "\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. Publishing best results\n",
    "6. Discussion\n",
    "\n",
    "The most important contributions (improvements on the template notebook) are signified with an exclamation mark in the section title.\n",
    "\n",
    "<!--\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "We will start the tutorial by reading the training and testing image data provided by the Kaggle competition. Next, we will preprocess the data in order to extract faces out of the images and distill features from them.\n",
    "\n",
    "## 0.1. Loading data\n",
    "You will notice that the training set is many times smaller than the test set. While this might strike you as odd, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! *Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:52:55.790074Z",
     "iopub.status.busy": "2023-04-12T13:52:55.788623Z",
     "iopub.status.idle": "2023-04-12T13:52:57.000559Z",
     "shell.execute_reply": "2023-04-12T13:52:56.999616Z",
     "shell.execute_reply.started": "2023-04-12T13:52:55.790028Z"
    },
    "papermill": {
     "duration": 37.543619,
     "end_time": "2021-03-08T07:57:43.9495",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.405881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Point to correct data\n",
    "base_file_path = train_path\n",
    "\n",
    "# Read train data with pandas csv\n",
    "train = pd.read_csv(\n",
    "    os.path.join(base_file_path, 'train_set.csv'), index_col = 0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "# Read test data with pandas csv\n",
    "test = pd.read_csv(\n",
    "    os.path.join(base_file_path, 'test_set.csv'), index_col = 0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# Read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [cv2.cvtColor(np.load(os.path.join(base_file_path, 'train/train_{}.npy').format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in train.iterrows()]\n",
    "\n",
    "## We processed the test image and will load it again afterwards, we ignore the raw files for memory reasons!\n",
    "# test['img'] = [cv2.cvtColor(np.load(os.path.join(base_file_path, 'test/test_{}.npy').format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "#                 for index, row in test.iterrows()]\n",
    "\n",
    "# Report sizes of train and test data\n",
    "train_size, test_size = len(train), len(test)\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Enabling plots is likely to crash browsers! We define a global variable which allows us to enable or disable showing the plots, as plotting all images can slow down the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:03:23.189760Z",
     "iopub.status.busy": "2023-04-12T14:03:23.189390Z",
     "iopub.status.idle": "2023-04-12T14:03:23.194331Z",
     "shell.execute_reply": "2023-04-12T14:03:23.193342Z",
     "shell.execute_reply.started": "2023-04-12T14:03:23.189727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show important plots (for discussion etc)\n",
    "SHOW_PLOTS = True\n",
    "# Show preprocessor plots (a lot of images, not so crucial)\n",
    "SHOW_PREPROCESSOR_PLOTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distributions. We can see that the training set contains an identifier, name of the person, the image as an array of pixel values, and the class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:53:37.859119Z",
     "iopub.status.busy": "2023-04-12T13:53:37.858412Z",
     "iopub.status.idle": "2023-04-12T13:53:40.227211Z",
     "shell.execute_reply": "2023-04-12T13:53:40.225869Z",
     "shell.execute_reply.started": "2023-04-12T13:53:37.859080Z"
    },
    "papermill": {
     "duration": 3.315629,
     "end_time": "2021-03-08T07:57:47.336913",
     "exception": false,
     "start_time": "2021-03-08T07:57:44.021284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set, on the other hand, does *not* have a class label: our goal will be to predict this using the information of the training data! (*note:* by default, we do not read the raw test images: in that case, the `img` column shows -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:54:14.598506Z",
     "iopub.status.busy": "2023-04-12T13:54:14.597304Z",
     "iopub.status.idle": "2023-04-12T13:54:14.607614Z",
     "shell.execute_reply": "2023-04-12T13:54:14.606348Z",
     "shell.execute_reply.started": "2023-04-12T13:54:14.598441Z"
    },
    "papermill": {
     "duration": 3.283501,
     "end_time": "2021-03-08T07:57:50.644778",
     "exception": false,
     "start_time": "2021-03-08T07:57:47.361277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the distributions of the different classes inside the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:54:15.732659Z",
     "iopub.status.busy": "2023-04-12T13:54:15.731931Z",
     "iopub.status.idle": "2023-04-12T13:54:15.747198Z",
     "shell.execute_reply": "2023-04-12T13:54:15.746091Z",
     "shell.execute_reply.started": "2023-04-12T13:54:15.732618Z"
    },
    "papermill": {
     "duration": 0.046628,
     "end_time": "2021-03-08T07:57:50.716317",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.669689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the above table that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025108,
     "end_time": "2021-03-08T07:57:50.766719",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.741611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.3. Preprocess data\n",
    "\n",
    "\n",
    "<!-- <div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is loaded into the notebook, we will preprocess the images and detect faces present in the image.\n",
    "First, we declare a variable that will set the final size of the faces for our pipeline. We chose for a size of (224, 244), which is the size used in the VGG-16 architecture that gives the best results in the final pipeline. Apart from that, we also define an auxiliary function to facilitate the plotting of a sequence of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T13:54:18.146896Z",
     "iopub.status.busy": "2023-04-12T13:54:18.144667Z",
     "iopub.status.idle": "2023-04-12T13:54:18.151787Z",
     "shell.execute_reply": "2023-04-12T13:54:18.150711Z",
     "shell.execute_reply.started": "2023-04-12T13:54:18.146844Z"
    }
   },
   "outputs": [],
   "source": [
    "FACE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:18.524273Z",
     "iopub.status.busy": "2023-04-12T14:05:18.523551Z",
     "iopub.status.idle": "2023-04-12T14:05:18.532263Z",
     "shell.execute_reply": "2023-04-12T14:05:18.531255Z",
     "shell.execute_reply.started": "2023-04-12T14:05:18.524235Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_image_sequence(data, imgs_per_row=10, cmap=\"Greys_r\"):\n",
    "    \"\"\"Auxiliary function that plots a sequence of images in convenient format.\n",
    "    Args:\n",
    "        data (np.array): Sequence of images\n",
    "        imgs_per_row (int, optional): Number of images on each row. Defaults to 7.\n",
    "        cmap (str, optional): Colormap used in plotting. Defaults to \"Greys_r\" for greyscale images.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    n_rows = n//imgs_per_row\n",
    "    if n%imgs_per_row != 0:\n",
    "        n_rows += 1\n",
    "    n_cols = imgs_per_row\n",
    "    fig, axs = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        ax = axs[i//imgs_per_row, i%imgs_per_row]\n",
    "        ax.imshow(data[i], cmap=cmap)\n",
    "    # Disable ticks\n",
    "    for row in axs:\n",
    "        for ax in row:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an auxiliary function that, given an image and a face detected inside that image (specified by a bounding box around the face), allows us to modify the bounding box and e.g. enlarge it to include features such as hairstyle which are usually cropped from images by most face detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:21.933683Z",
     "iopub.status.busy": "2023-04-12T14:05:21.933292Z",
     "iopub.status.idle": "2023-04-12T14:05:21.943787Z",
     "shell.execute_reply": "2023-04-12T14:05:21.942584Z",
     "shell.execute_reply.started": "2023-04-12T14:05:21.933647Z"
    }
   },
   "outputs": [],
   "source": [
    "def cut_out_face(img: np.array, x: int, y: int, width: int, height: int, width_factor: float = 0.1, height_factor: float = -1, \n",
    "                 center_x: int = None, center_y: int = None, square: bool = True) -> np.array:\n",
    "    \"\"\"\n",
    "    Cuts out the face detected in an image with a modified bounding box.\n",
    "    Args:\n",
    "        img (np.array): Original image.\n",
    "        x (int): The x coordinate of the bounding box of the detected face.\n",
    "        y (int): The y coordinate of the bounding box of the detected face.\n",
    "        width (int): The width of the bounding box of the detected face.\n",
    "        height (int): The height of the bounding box of the detected face.\n",
    "        width_factor (float, optional): Multiplier increasing the width of the bounding box. Defaults to 0.1.\n",
    "        height_factor (float, optional): Multiplier increasing the height of the bounding box. Defaults to -1, such that it is set by the value of width_factor.\n",
    "        center_x (int, optional): The x coordinate of a keypoint, such as nose or mouth. Defaults to None.\n",
    "        center_y (int, optional): The y coordinate of a keypoint, such as nose or mouth. Defaults to None.\n",
    "        square (boolean, optional): Specify whether bounding box has to be square-shaped. Defaults to True.\n",
    "    Returns:\n",
    "        np.array: Face cropped out of the original image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If height factor not set, default it to twice width factor\n",
    "    if height_factor < 0:\n",
    "        height_factor = 2 * width_factor\n",
    "    \n",
    "    # If we want the images to be square, adjust height or width (go for smallest square)\n",
    "    if square:\n",
    "        if width <= height:\n",
    "            height = width\n",
    "        else:\n",
    "            width = height\n",
    "            \n",
    "    # In case a central point is provided, shift the coordinates to center that object\n",
    "    if center_x is not None and center_y is not None:\n",
    "        x = center_x - width/2\n",
    "        y = center_y - height/2\n",
    "    \n",
    "    # Get the width and height of the image\n",
    "    img_height, img_width = img.shape[0], img.shape[1]\n",
    "    \n",
    "    # Get new initial positions\n",
    "    new_x = max(0, int(x - width_factor*width))\n",
    "    new_y = max(0, int(y - height_factor*height))\n",
    "    \n",
    "    new_width  = min(img_width  - new_x, int((1 + 2*width_factor)*width))\n",
    "    new_height = min(img_height - new_y, int((1 + 2*height_factor)*height))\n",
    "    \n",
    "    return img[new_y:new_y+new_height, new_x:new_x+new_width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the face detectors fail to detect a face in an image occasionally (especially less refined techniques such as the HAAR detector), we will write an auxiliary function that gets the indices (inside a dataframe) of images that contain negative pixel values, which signals a failed detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:23.139968Z",
     "iopub.status.busy": "2023-04-12T14:05:23.138984Z",
     "iopub.status.idle": "2023-04-12T14:05:23.146543Z",
     "shell.execute_reply": "2023-04-12T14:05:23.145626Z",
     "shell.execute_reply.started": "2023-04-12T14:05:23.139906Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_negatives_images(imgs: np.array, verbose: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Auxiliary function that saves the indices of images in a Numpy array that contain negative pixel values.\n",
    "    Args:\n",
    "        imgs (np.array): Sequence of images to be checked for negative pixel values.\n",
    "        verbose (bool, optional): Signal whether to print the indices containing negative pixel values. Defaults to False.\n",
    "    Returns:\n",
    "        list: A list containing the indices containing negative pixel values.\n",
    "    \"\"\"\n",
    "    # Save indices of images that have negative pixel values\n",
    "    indices_to_delete = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        if np.any(img.flatten() < 0):\n",
    "            if verbose:\n",
    "                print(f\"Negative values in image {i} detected\")\n",
    "            indices_to_delete.append(i)\n",
    "    return indices_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3.1 Data augmentation/enhancement (!)\n",
    "\n",
    "As already mentioned, the training data has just 80 examples, which is a quite low compared to the test set. In addition to that, some pictures have multiple people in them, are a montage of different pictures of the same person or are a meme mixing real people and cartoon figures. Therefore, in this section we will do a couple of things to improve and overall quality of the training data.\n",
    "\n",
    "We will do a quick, manual crop of some images. Sometimes, that crop will result on an easier time for the face detection algorithms, since pictures that include 2 or more people will get simplified to just show the most important one (Sarah, Jesse, Michael or Mila). At the same time, some montages or memes can include different people we are interested in, that is why we will also perform some crops so that we can divide the pictures in 2 different ones with new labels that will increase the size of the data. In the table below, we summarize all augmentations/enhancement that we then perform in a cell below the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  :root {\n",
    "    --col-width: 500px;\n",
    "  }\n",
    "  \n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "  }\n",
    "  \n",
    "  th,\n",
    "  td {\n",
    "    padding: 8px;\n",
    "    border: 1px solid black;\n",
    "  }\n",
    "  \n",
    "  th:nth-child(1) {\n",
    "    width: var(--col-width);\n",
    "  }\n",
    "  \n",
    "  th:nth-child(2),\n",
    "  th:nth-child(3) {\n",
    "    width: calc((100% - var(--col-width)) / 2);\n",
    "    max-width: var(--col-width);\n",
    "  }\n",
    "  .my-table th:nth-child(1) {\n",
    "  width: var(--col-width);\n",
    "}\n",
    "\n",
    ".my-table th:nth-child(2),\n",
    ".my-table th:nth-child(3) {\n",
    "  width: calc((100% - var(--col-width)) / 2);\n",
    "  max-width: var(--col-width);\n",
    "}\n",
    "</style>\n",
    "<table class=\"my-table\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Index</th>\n",
    "      <th>Before</th>\n",
    "      <th>After</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>18</td>\n",
    "      <td>Picture shows both Jesse and Michael, but is classified as Jesse (label = 1).</td>\n",
    "      <td>Image at Index 18 just shows Jesse, and new image at Index 80 shows Michael with label = 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>25</td>\n",
    "      <td>The picture shows both Michael and a cartoon.</td>\n",
    "      <td>Image cropped to just show Michael.</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td>28</td>\n",
    "      <td>The picture shows Mila with 2 other people.</td>\n",
    "      <td>Image cropped to just show Mila.</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td>29</td>\n",
    "      <td>The picture shows Mila with another person.</td>\n",
    "      <td>Image cropped to just show Mila.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>30</td>\n",
    "      <td>The picture shows two pictures of Mila.</td>\n",
    "      <td>Image separated to get two samples from it (Indexes 30 and 81).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>32</td>\n",
    "      <td>The picture shows two pictures of Sarah.</td>\n",
    "      <td>Image separated to get two samples from it (Indexes 32 &amp; 82).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>34</td>\n",
    "      <td>The picture shows Michael, Jesse and a third person in a Pok√©mon meme.</td>\n",
    "      <td>Image cropped to get Jesse at Index 34 and Michael at new Index 83.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>39</td>\n",
    "      <td>The picture shows Mila with another person.</td>\n",
    "      <td>Image cropped to just get Mila.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>40</td>\n",
    "      <td>The picture shows Mila with another person.</td>\n",
    "      <td>Image cropped to just get Mila.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>41</td>\n",
    "      <td>The picture shows Michael with another person.</td>\n",
    "      <td>Image cropped to just get him.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>49</td>\n",
    "      <td>The picture shows Jesse with another person.</td>\n",
    "      <td>Image cropped to just get him.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>50</td>\n",
    "      <td>The picture shows Mila with another person.</td>\n",
    "      <td>Image cropped to just get her.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>52</td>\n",
    "      <td>The picture shows Jesse with another person.</td>\n",
    "      <td>Image cropped to just get him.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>53</td>\n",
    "      <td>The picture shows Jesse with another person.</td>\n",
    "      <td>Image cropped to just get him.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>57</td>\n",
    "      <td>The picture shows 2 photos of Mila.</td>\n",
    "      <td>Image cropped to get one at Index 57 and the other at new Index 84.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>59</td>\n",
    "      <td>The picture shows a photo of Sarah with another person.</td>\n",
    "      <td>Image cropped to just get her</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>61</td>\n",
    "      <td>The picture shows a photo of Mila with another person.</td>\n",
    "      <td>Image cropped to just get her</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>70</td>\n",
    "      <td>The picture shows a photo of Jesse with 2 other people.</td>\n",
    "      <td>Image cropped to just get him.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>77</td>\n",
    "      <td>The picture shows 2 photos of Mila.</td>\n",
    "      <td>Image cropped to get one at Index 77 and the other at new Index 85.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>65</td>\n",
    "      <td>The picture shows an \"image not found\" placeholder, due to possibly the data being corrupted.</td>\n",
    "      <td>Image is deleted from improved training data.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:24.753509Z",
     "iopub.status.busy": "2023-04-12T14:05:24.753148Z",
     "iopub.status.idle": "2023-04-12T14:05:24.796461Z",
     "shell.execute_reply": "2023-04-12T14:05:24.795358Z",
     "shell.execute_reply.started": "2023-04-12T14:05:24.753476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the original training data, going to improve upon that below\n",
    "train_improved = train.copy()\n",
    "print(\"Length of TRAIN df: \" + str(len(train_improved.index)))\n",
    "# Index 18\n",
    "aux = pd.DataFrame({\"name\": [\"Michael_Cera\"], \"class\": [0], \"img\": [train_improved.loc[18].img[:, 270:]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][18] = train_improved.loc[18].img[:, :260]\n",
    "# Index 25\n",
    "train_improved[\"img\"][25] = train_improved.loc[25].img[:, 200:]\n",
    "# Index 28\n",
    "train_improved[\"img\"][28] = train_improved.loc[28].img[:, 275:625]\n",
    "# Index 29\n",
    "train_improved[\"img\"][29] = train_improved.loc[29].img[:, :400]\n",
    "# Index 30\n",
    "aux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[30].img[:, :100]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][30] = train_improved.loc[30].img[:, 100:]\n",
    "# Index 32\n",
    "aux = pd.DataFrame({\"name\": [\"Sarah_Hyland\"], \"class\": [0], \"img\": [train_improved.loc[32].img[:, 420:]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][32] = train_improved.loc[32].img[:, :420]\n",
    "# Index 34\n",
    "aux = pd.DataFrame({\"name\": [\"Michael_Cera\"], \"class\": [0], \"img\": [train_improved.loc[34].img[160:, :160]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][34] = train_improved.loc[34].img[160:, 210:380]\n",
    "# Index 39\n",
    "train_improved[\"img\"][39] = train_improved.loc[39].img[:, 125:220]\n",
    "# Index 40\n",
    "train_improved[\"img\"][40] = train_improved.loc[40].img[:, 107:]\n",
    "# Index 41\n",
    "train_improved[\"img\"][41] = train_improved.loc[41].img[:, :200]\n",
    "# Index 49\n",
    "train_improved[\"img\"][49] = train_improved.loc[49].img[:, :150]\n",
    "# Index 50\n",
    "train_improved[\"img\"][50] = train_improved.loc[50].img[:200, :]\n",
    "# Index 52\n",
    "train_improved[\"img\"][52] = train_improved.loc[52].img[:, 325:]\n",
    "# Index 53\n",
    "train_improved[\"img\"][53] = train_improved.loc[53].img[:, 300:]\n",
    "# Index 57\n",
    "aux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[57].img[:, :100]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][57] = train_improved.loc[57].img[:, 100:]\n",
    "# Index 59\n",
    "train_improved[\"img\"][59] = train_improved.loc[59].img[:, 220:]\n",
    "# index 61\n",
    "train_improved[\"img\"][61] = train_improved.loc[61].img[:, 200:]\n",
    "# Index 70\n",
    "train_improved[\"img\"][70] = train_improved.loc[70].img[:, 175:]\n",
    "# Index 77\n",
    "aux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[77].img[:, :100]]})\n",
    "train_improved = train_improved.append(aux, ignore_index=True)\n",
    "train_improved[\"img\"][77] = train_improved.loc[77].img[:, 100:]\n",
    "# Index 65\n",
    "train_improved = train_improved.drop([65])\n",
    "print(\"Length of improved TRAIN df: \" + str(len(train_improved.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.2 HAAR preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we cleaned up and augmented our training data, we are ready to extract faces out of the images. In this first example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces. After detection, the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first preprocessor we try out is the HaarPreprocessor. About this cascade classifiers it can be said that they are part of a machine learning approach where a lot of images with faces and others with any are needed to train the model. The extraction of the features of the images using white/black rectangles is quite inefficient, so Adaboost is used to select the best features, and for each feature, the best threshold is found. In the end, at the time of its implementation, a pretrained model is used from a XML file and the detection is mainly done with the method detectMultiScale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:26.743690Z",
     "iopub.status.busy": "2023-04-12T14:05:26.742987Z",
     "iopub.status.idle": "2023-04-12T14:05:26.756299Z",
     "shell.execute_reply": "2023-04-12T14:05:26.755074Z",
     "shell.execute_reply.started": "2023-04-12T14:05:26.743651Z"
    },
    "papermill": {
     "duration": 0.042776,
     "end_time": "2021-03-08T07:57:50.834913",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.792137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        # original HAAR img size\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "        #return [cut_out_face(img, x, y, w, h) for (x, y, w, h) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HAAR preprocessor is now just used for the train data to show visualization examples in the following subsection and how the process of discarding bad images would look like, since better results can be achieved with other face detectors.\n",
    "\n",
    "Either way, firstly the HAAR preprocessor object has to be created for its later use on the train data. (We save the preprocessed data and use a different face size for the sake of the tutorial on SIFT later on -- the final pipeline for the classifiers will not rely on the HAAR preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:29.024983Z",
     "iopub.status.busy": "2023-04-12T14:05:29.024022Z",
     "iopub.status.idle": "2023-04-12T14:05:33.439291Z",
     "shell.execute_reply": "2023-04-12T14:05:33.438294Z",
     "shell.execute_reply.started": "2023-04-12T14:05:29.024930Z"
    }
   },
   "outputs": [],
   "source": [
    "haar_face_size = (100,100)\n",
    "haar_preprocessor = HAARPreprocessor(path = '../../tmp', face_size=haar_face_size)\n",
    "train_X_HAAR, train_y_HAAR = haar_preprocessor(train), train[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the results of the HAAR preprocessor on the basic train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:42.086479Z",
     "iopub.status.busy": "2023-04-12T14:05:42.085528Z",
     "iopub.status.idle": "2023-04-12T14:05:42.091917Z",
     "shell.execute_reply": "2023-04-12T14:05:42.090624Z",
     "shell.execute_reply.started": "2023-04-12T14:05:42.086440Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_HAAR, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the HAAR detector occasionally fails to detect a face in an image. Hence, we will remove images that had any negative pixel value, since it would mean that there was no detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:52.846523Z",
     "iopub.status.busy": "2023-04-12T14:05:52.845750Z",
     "iopub.status.idle": "2023-04-12T14:05:52.870612Z",
     "shell.execute_reply": "2023-04-12T14:05:52.869635Z",
     "shell.execute_reply.started": "2023-04-12T14:05:52.846489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the indices of the images with no detection\n",
    "indices_to_delete = detect_negatives_images(train_X_HAAR, True)\n",
    "\n",
    "# Update both the train_X and train_Y arrays deleting those unuseful examples\n",
    "train_X_HAAR = np.delete(train_X_HAAR, indices_to_delete, axis=0)\n",
    "train_y_HAAR = np.delete(train_y_HAAR, indices_to_delete, axis=0)\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_HAAR, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are still some false detections, due to pieces of hair or a shirt. Hence, we manually remove these false positives and show how the final images would look like. Note that this means that the HAAR preprocessor is not robust or reliable, such that we will consider other face detectors below to process the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:05:56.370386Z",
     "iopub.status.busy": "2023-04-12T14:05:56.369460Z",
     "iopub.status.idle": "2023-04-12T14:05:56.380532Z",
     "shell.execute_reply": "2023-04-12T14:05:56.379409Z",
     "shell.execute_reply.started": "2023-04-12T14:05:56.370330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try to remove images that aren't faces or aren't Mila, Jesse, Michael, or Sarah\n",
    "indices_not_faces = [5, 17, 22, 23, 27, 33, 38, 47, 58, 59, 62, 67]\n",
    "train_X_HAAR = np.delete(train_X_HAAR, indices_not_faces, axis=0)\n",
    "train_y_HAAR = np.delete(train_y_HAAR, indices_not_faces, axis=0)\n",
    "# Plot the images\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_HAAR, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could also look at the different people by their labels. For instance, we will show the images of Mila:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:06:00.650158Z",
     "iopub.status.busy": "2023-04-12T14:06:00.649598Z",
     "iopub.status.idle": "2023-04-12T14:06:00.656120Z",
     "shell.execute_reply": "2023-04-12T14:06:00.655102Z",
     "shell.execute_reply.started": "2023-04-12T14:06:00.650112Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_HAAR[train_y_HAAR == 2], imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.3 MTCNN preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HAAR preprocessing gives a limited performance when it comes to face detection. We hence cannot trust it to process the large test data and give reliable results. Therefore, other possibilities have been explored, and the first of them is a MTCNN preprocessor. MTCNN stands for **MultiTask Cascaded Convolutional Networks**, which is a deep learning approach that adopts a cascaded structure of 3 stages which exploits the correlation between face detection and alignment in unconstrained environments to move past the performance of other methods like the previous one.\n",
    "\n",
    "In this section, we look at [this paper](https://arxiv.org/abs/1604.02878) and [this article](https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/) for the implementation of the framework. The mtcnn package needs to be installed (which was done at the start of the notebook), and from that, the pipeline is built is a similar fashion to the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:06:03.799051Z",
     "iopub.status.busy": "2023-04-12T14:06:03.798485Z",
     "iopub.status.idle": "2023-04-12T14:06:03.816117Z",
     "shell.execute_reply": "2023-04-12T14:06:03.814670Z",
     "shell.execute_reply.started": "2023-04-12T14:06:03.798999Z"
    }
   },
   "outputs": [],
   "source": [
    "class MTCNNPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around MTCNN.\"\"\"\n",
    "    \n",
    "    def __init__(self, face_size, width_factor = 0.2):\n",
    "\n",
    "        self.face_size = face_size\n",
    "        self.detector = MTCNN()\n",
    "        self.width_factor = width_factor\n",
    "\n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        # Make a copy, since MTCNN plots bounding boxes on top of our images\n",
    "        return self.detector.detect_faces(img.copy())\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        # Detect the faces\n",
    "        faces = self.detect_faces(img)\n",
    "        # Boxes contain x, y, w, h of bounding box of each detected face\n",
    "        boxes    = [face['box'] for face in faces]  \n",
    "        # Nose key gives x and y\n",
    "        noses = [face['keypoints']['nose'] for face in faces]\n",
    "        \n",
    "        # Extract faces\n",
    "        extracted_faces = []\n",
    "        for i in range(len(faces)):\n",
    "            x, y, w, h = boxes[i]\n",
    "            center_x, center_y = noses[i]\n",
    "            cut_out = cut_out_face(img, x, y, w, h, width_factor = self.width_factor, center_x = center_x, center_y = center_y)\n",
    "            extracted_faces.append(cut_out)\n",
    "        \n",
    "        return extracted_faces\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # only return the first face, and resize\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the previous case, this MTCNN preprocessor is now just used for the train data to show visualization examples in the following subsection and how the process of discarding bad images would look like, since better results can still be achieved with the last preprocessing technique shown in this notebook. Also, by doing so, the differences between this first two approaches can be seen in the final pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step, as always, is defining the instance of the MTCNN preprocessor object, to use right after on the train data. We use `%%capture` to suppress output of the MTCNN class within this notebook. Note that this preprocessor is slower than the HAAR preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:06:05.643157Z",
     "iopub.status.busy": "2023-04-12T14:06:05.642792Z",
     "iopub.status.idle": "2023-04-12T14:07:19.378328Z",
     "shell.execute_reply": "2023-04-12T14:07:19.376958Z",
     "shell.execute_reply.started": "2023-04-12T14:06:05.643123Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mtcnn_preprocessor = MTCNNPreprocessor(FACE_SIZE)\n",
    "train_X_MTCNN, train_y_MTCNN = mtcnn_preprocessor(train), train['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of MTCNN contains more than just a bounding box around a face, and an example output is shown below in comment. It is a dictionary which, besides the bounding box of the face under the key `box`, also contains a keyword `keypoints`. This is another dictionary containing keys of which the values indicate coordinates of important features of a face, such as locations of the eyes and nose. Our custom function `cut_out_face` allows users to give e.g. the coordinates of the nose, to center this keypoint location across all faces. Such centering is beneficial for instance for PCA, as we will discuss later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:24.057533Z",
     "iopub.status.busy": "2023-04-12T14:08:24.056736Z",
     "iopub.status.idle": "2023-04-12T14:08:24.062120Z",
     "shell.execute_reply": "2023-04-12T14:08:24.060872Z",
     "shell.execute_reply.started": "2023-04-12T14:08:24.057494Z"
    }
   },
   "outputs": [],
   "source": [
    "## Example output of detect_faces:\n",
    "# {'box': [62, 102, 170, 220],\n",
    "#   'confidence': 0.9995554089546204,\n",
    "#   'keypoints': {'left_eye': (102, 194),\n",
    "#    'right_eye': (187, 191),\n",
    "#    'nose': (145, 243),\n",
    "#    'mouth_left': (112, 273),\n",
    "#    'mouth_right': (183, 269)}},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the results of the MTCNN preprocessor on the basic train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:27.296166Z",
     "iopub.status.busy": "2023-04-12T14:08:27.295557Z",
     "iopub.status.idle": "2023-04-12T14:08:27.301639Z",
     "shell.execute_reply": "2023-04-12T14:08:27.300627Z",
     "shell.execute_reply.started": "2023-04-12T14:08:27.296117Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_MTCNN, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, in the same way it was done in the HAAR case, the non-detections and some bad results are discarded to get a final cleaned set of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:32.084317Z",
     "iopub.status.busy": "2023-04-12T14:08:32.083162Z",
     "iopub.status.idle": "2023-04-12T14:08:32.163728Z",
     "shell.execute_reply": "2023-04-12T14:08:32.162591Z",
     "shell.execute_reply.started": "2023-04-12T14:08:32.084262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove some manually detected bad results\n",
    "indices_not_faces = [8, 28, 30, 40]\n",
    "train_X_MTCNN = np.delete(train_X_MTCNN, indices_not_faces, axis=0)\n",
    "train_y_MTCNN = np.delete(train_y_MTCNN, indices_not_faces, axis=0)\n",
    "\n",
    "# Discard non-detections\n",
    "indices_to_delete = detect_negatives_images(train_X_MTCNN, True)\n",
    "\n",
    "# Update both the train_X and train_Y arrays deleting those indices\n",
    "train_X_MTCNN = np.delete(train_X_MTCNN, indices_to_delete, axis=0)\n",
    "train_y_MTCNN = np.delete(train_y_MTCNN, indices_to_delete, axis=0)\n",
    "# Plot image sequence\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_MTCNN,  imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will not continue with the MTCNN preprocessor, we delete the variables to clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:35.015721Z",
     "iopub.status.busy": "2023-04-12T14:08:35.014630Z",
     "iopub.status.idle": "2023-04-12T14:08:35.021294Z",
     "shell.execute_reply": "2023-04-12T14:08:35.019964Z",
     "shell.execute_reply.started": "2023-04-12T14:08:35.015681Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_X_MTCNN\n",
    "del train_y_MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.4 DeepFace preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final face detector we explore is **DeepFace**. To explain how the DeepFace preprocessor works and provide a motivation for its use, let's look at an example. Both HAAR and MTCNN failed on training image with index 49, shown below. The detectors have a more confidence detection due to Kristen Stewart. We can exploit the knowledge from the class label, and use a template of Jesse to force our detector to detect Jesse's face instead by relying on a similarity score. Note that we can also prevent this issue by cropping the image such that only the person we wish to detect is in the image, as we did in the beginning of our preprocessing. However, it is clear that the method provided by DeepFace is more robust, and is easily scalable towards larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:38.105893Z",
     "iopub.status.busy": "2023-04-12T14:08:38.105400Z",
     "iopub.status.idle": "2023-04-12T14:08:38.115938Z",
     "shell.execute_reply": "2023-04-12T14:08:38.114733Z",
     "shell.execute_reply.started": "2023-04-12T14:08:38.105846Z"
    }
   },
   "outputs": [],
   "source": [
    "test_image = train.loc[49].img\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plt.imshow(test_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved a template where Jesse's face is clearly visible and the single face in the picture. We load it in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - get correct path to templates and also consider how to do this in Kaggle, the templates are just images from the training data so we can also just get those since it doesn't have to be a .png but can also be a simple numpy array, so perhaps it's better to find a nice image to be used as template, then load in the full (non-preprocessed) image and use those as templates, since then we don\"t have to save them in Kaggle or anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:55.856777Z",
     "iopub.status.busy": "2023-04-12T14:08:55.856395Z",
     "iopub.status.idle": "2023-04-12T14:08:55.867916Z",
     "shell.execute_reply": "2023-04-12T14:08:55.866909Z",
     "shell.execute_reply.started": "2023-04-12T14:08:55.856741Z"
    }
   },
   "outputs": [],
   "source": [
    "template_jesse = cv2.imread(os.path.join(template_path, \"template_jesse.png\"))\n",
    "# We have to convert to RGB\n",
    "template_jesse = template_jesse[...,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use DeepFace's \"verify\" method, which compares two images and verifies that the same person is present in both images. In the example output, we see that DeepFace then gives us two facial areas: one for each image. Since we are not interested in the template, we take the face from the first image. Below, we show an example output in comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:08:58.591114Z",
     "iopub.status.busy": "2023-04-12T14:08:58.589968Z",
     "iopub.status.idle": "2023-04-12T14:09:28.016088Z",
     "shell.execute_reply": "2023-04-12T14:09:28.015027Z",
     "shell.execute_reply.started": "2023-04-12T14:08:58.591072Z"
    }
   },
   "outputs": [],
   "source": [
    "result = DeepFace.verify(img1_path = test_image, img2_path = template_jesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:10:42.767378Z",
     "iopub.status.busy": "2023-04-12T14:10:42.766660Z",
     "iopub.status.idle": "2023-04-12T14:10:42.772392Z",
     "shell.execute_reply": "2023-04-12T14:10:42.771260Z",
     "shell.execute_reply.started": "2023-04-12T14:10:42.767339Z"
    }
   },
   "outputs": [],
   "source": [
    "### Example output:\n",
    "# {'verified': True,\n",
    "#  'distance': 0.11051846168495327,\n",
    "#  'threshold': 0.4,\n",
    "#  'model': 'VGG-Face',\n",
    "#  'detector_backend': 'opencv',\n",
    "#  'similarity_metric': 'cosine',\n",
    "#  'facial_areas': {'img1': {'x': 65, 'y': 49, 'w': 54, 'h': 54},\n",
    "#   'img2': {'x': 116, 'y': 83, 'w': 124, 'h': 124}},\n",
    "#  'time': 2.23\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detected face, when cropped out and plotted  below, is indeed Jesse's face -- we hence were able to detect the desired face in this image due to DeepFace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:10:49.280520Z",
     "iopub.status.busy": "2023-04-12T14:10:49.279644Z",
     "iopub.status.idle": "2023-04-12T14:10:49.286391Z",
     "shell.execute_reply": "2023-04-12T14:10:49.285345Z",
     "shell.execute_reply.started": "2023-04-12T14:10:49.280466Z"
    }
   },
   "outputs": [],
   "source": [
    "face = result[\"facial_areas\"][\"img1\"]\n",
    "x, y, w, h = face[\"x\"], face[\"y\"], face[\"w\"], face[\"h\"]\n",
    "# Plot the face\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plt.imshow(test_image[y:y+h, x:x+w])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we cannot use the template (it is not Jesse or Mila), we have to resort back to a simple face detector to detect faces. There are several detectors implemented in DeepFace, which can be accessed by providing the right name as a string. For instance, the `opencv` detector is simply the HAAR cascade detector we discussed earlier. Also, the MTCNN detector is implemented here. A more state-of-the-art detector based on deep learning is also implemented and selected in the end, known as [RetinaFace](https://arxiv.org/pdf/1905.00641.pdf).\n",
    "\n",
    "RetinaFace achieves better results compared to both HAAR and MTCNN since it combines face detection, landmark localization and face bounding box regression. Its network is composed of a feature extraction part and convolutional layers that predict the boxes, confidence scores and landmark points. These last ones are also what it is used to align the faces to desired poses. It has been chosen because it achieves higher accuracy and robustness in changing conditions and poses while being only slightly slower in comparison to HAAR and MTCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:10:56.154996Z",
     "iopub.status.busy": "2023-04-12T14:10:56.153847Z",
     "iopub.status.idle": "2023-04-12T14:11:10.081583Z",
     "shell.execute_reply": "2023-04-12T14:11:10.080526Z",
     "shell.execute_reply.started": "2023-04-12T14:10:56.154938Z"
    }
   },
   "outputs": [],
   "source": [
    "detector_name = \"retinaface\"\n",
    "detector = FaceDetector.build_model(detector_name) #options: opencv, ssd, dlib, mtcnn or retinaface\n",
    "obj = FaceDetector.detect_faces(detector, detector_name, test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list containing the original image, the bounding box of the face, and some score of the detection. Unfortunately, RetinaFace does not give locations of keypoints as MTCNN did. Getting the face out of the image is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:11:15.957772Z",
     "iopub.status.busy": "2023-04-12T14:11:15.956844Z",
     "iopub.status.idle": "2023-04-12T14:11:15.965014Z",
     "shell.execute_reply": "2023-04-12T14:11:15.963960Z",
     "shell.execute_reply.started": "2023-04-12T14:11:15.957717Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, w, h = obj[0][1]\n",
    "# Plot the face\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plt.imshow(test_image[y:y+h, x:x+w])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The face detector`DEEPPreprocessor` is a preprocessor based on [DeepFace](https://github.com/serengil/deepface). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:12:06.135001Z",
     "iopub.status.busy": "2023-04-12T14:12:06.134538Z",
     "iopub.status.idle": "2023-04-12T14:12:06.151709Z",
     "shell.execute_reply": "2023-04-12T14:12:06.150645Z",
     "shell.execute_reply.started": "2023-04-12T14:12:06.134965Z"
    }
   },
   "outputs": [],
   "source": [
    "class DEEPPreprocessor():\n",
    "    \n",
    "    def __init__(self, face_size, template_jesse_loc =\"template_jesse.png\", template_mila_loc = \"template_mila.png\",\n",
    "                 detector_name = \"retinaface\", width_factor = 0.1):\n",
    "        \"\"\"Preprocessing pipeline built around DeepFace. \n",
    "\n",
    "        Args:\n",
    "            face_size (tuple[int, int]): Size of the faces for resizing after successful face detection.\n",
    "            template_jesse_loc (str, optional): Filename of template image of Jesse. Defaults to \"template_jesse.png\".\n",
    "            template_mila_loc (str, optional): Filename of template image of Mila. Defaults to \"template_mila.png\".\n",
    "            detector_name (str, optional): Argument required by DeepFace's FaceDetector object. Possible choices are\n",
    "            opencv, ssd, dlib, mtcnn or retinaface. Defaults to \"retinaface\".\n",
    "            width_factor (float, optional): When cutting out the face in an image, specify the width of the bounding box. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save face size and width factor for bounding boxes\n",
    "        self.width_factor = width_factor\n",
    "        self.face_size = face_size\n",
    "        \n",
    "        # Load in Jesse's template\n",
    "        self.template_jesse = cv2.imread(os.path.join(template_path, template_jesse_loc))\n",
    "        self.template_jesse = self.template_jesse[...,::-1]\n",
    "        \n",
    "        # Load in Mila's template\n",
    "        self.template_mila = cv2.imread(os.path.join(template_path, template_mila_loc))\n",
    "        self.template_mila = self.template_mila[...,::-1]\n",
    "        \n",
    "        # Initialize face detector\n",
    "        self.detector_name = detector_name\n",
    "        self.detector = FaceDetector.build_model(self.detector_name) \n",
    "        \n",
    "            \n",
    "    def extract_face(self, img, label):\n",
    "        \"\"\"Detect and cut out face out of the image. NOTE - unlike HAAR and MTCNN, DeepFace methods detects a single face.\n",
    "        This method decides how to detect faces based on the label that is provided, and hence can only be used on training data\n",
    "        to improve the face detection and extraction. If the label is 1 or 2, we rely on the templates of Jesse and Mila to \n",
    "        improve the performance of the detector (cf. Kristen Stewart example).\n",
    "\n",
    "        Args:\n",
    "            img (np.array): Image.\n",
    "            label (int): Class label of training data.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Face cropped out of the image.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the face bounding box to an empty box (in case no detection, still empty box)\n",
    "        x, y, w, h = 0, 0, 0, 0\n",
    "        # In case we do NOT have Jesse or Mila, just use face detector\n",
    "        if label == 0:\n",
    "            # Detect a face\n",
    "            face = FaceDetector.detect_faces(detector, self.detector_name, img)\n",
    "            # Get the bounding box\n",
    "            if face is not None and len(face) > 0:\n",
    "                x, y, w, h = face[0][1]\n",
    "        else:\n",
    "            # In case we know we have to detect Jesse or Mila, make sure we detect them! Use \"verify\"\n",
    "            if label == 1:\n",
    "                template = self.template_jesse\n",
    "            elif label == 2:\n",
    "                template = self.template_mila\n",
    "            # Use template to verify faces\n",
    "            result = DeepFace.verify(img1_path = img, img2_path = template, enforce_detection=False)\n",
    "            face = result[\"facial_areas\"][\"img1\"]\n",
    "            x, y, w, h = face[\"x\"], face[\"y\"], face[\"w\"], face[\"h\"]\n",
    "            \n",
    "        return cut_out_face(img, x, y, w, h, width_factor=self.width_factor)\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        \"\"\"Preprocesses the data and extracts the images out of it.\n",
    "\n",
    "        Args:\n",
    "            data_row (pd.DataFrame): Pandas dataframe of training data (images and class labels).\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        # Detect face\n",
    "        face = self.extract_face(data_row['img'], data_row['class'])\n",
    "        \n",
    "        # No face detected, return nan image                \n",
    "        if len(face.flatten()) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # Resize if face detected\n",
    "        return cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the DeepFace preprocessor on the training data. Note that this preprocessor is computationally the heaviest of all three discussed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:12:12.666348Z",
     "iopub.status.busy": "2023-04-12T14:12:12.665425Z",
     "iopub.status.idle": "2023-04-12T14:12:57.720251Z",
     "shell.execute_reply": "2023-04-12T14:12:57.719160Z",
     "shell.execute_reply.started": "2023-04-12T14:12:12.666292Z"
    }
   },
   "outputs": [],
   "source": [
    "deep_preprocessor = DEEPPreprocessor(FACE_SIZE)\n",
    "train_X_DEEP, train_y_DEEP = deep_preprocessor(train), train['class'].values\n",
    "# Plot\n",
    "img_seq = train_X_DEEP\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(img_seq, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we delete a few bad faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:12:57.722634Z",
     "iopub.status.busy": "2023-04-12T14:12:57.722259Z",
     "iopub.status.idle": "2023-04-12T14:12:57.757311Z",
     "shell.execute_reply": "2023-04-12T14:12:57.756261Z",
     "shell.execute_reply.started": "2023-04-12T14:12:57.722597Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_not_faces = [40, 65]\n",
    "train_X_DEEP = np.delete(train_X_DEEP, indices_not_faces, axis=0)\n",
    "train_y_DEEP = np.delete(train_y_DEEP, indices_not_faces, axis=0)\n",
    "# Plot\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X_DEEP, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to use the augmented data, we again delete the above data for the sake of memory/performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:12:57.759478Z",
     "iopub.status.busy": "2023-04-12T14:12:57.759080Z",
     "iopub.status.idle": "2023-04-12T14:12:57.765098Z",
     "shell.execute_reply": "2023-04-12T14:12:57.763639Z",
     "shell.execute_reply.started": "2023-04-12T14:12:57.759438Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_X_DEEP\n",
    "del train_y_DEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.5 Selection of data to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the demonstrations above, where we showcased three face detectors and discussed their (dis)advantages, we will now choose to work with DeepFace, as RetinaFace is likely the most effective model out of the three detectors we have discussed. Hence, we hope to maximize the number of (true) detections on the test set with RetinaFace. Since the classifiers will extract features from this preprocessed data, it makes sense to use the same detection strategy for the training set. Hence, below we process the entire augmented training dataset with RetinaFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:12:57.768174Z",
     "iopub.status.busy": "2023-04-12T14:12:57.767627Z",
     "iopub.status.idle": "2023-04-12T14:13:34.218614Z",
     "shell.execute_reply": "2023-04-12T14:13:34.217535Z",
     "shell.execute_reply.started": "2023-04-12T14:12:57.768131Z"
    }
   },
   "outputs": [],
   "source": [
    "deep_preprocessor = DEEPPreprocessor(FACE_SIZE)\n",
    "train_X_DEEP, train_y_DEEP = deep_preprocessor(train_improved), train_improved['class'].values\n",
    "# Plot\n",
    "if SHOW_PREPROCESSOR_PLOTS:\n",
    "    plot_image_sequence(train_X, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder of the tutorial, we will work with this preprocessed training data, so we save it in an additional variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:13:34.220456Z",
     "iopub.status.busy": "2023-04-12T14:13:34.220092Z",
     "iopub.status.idle": "2023-04-12T14:13:34.227929Z",
     "shell.execute_reply": "2023-04-12T14:13:34.226214Z",
     "shell.execute_reply.started": "2023-04-12T14:13:34.220416Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_X_DEEP\n",
    "train_y = train_y_DEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100995,
     "end_time": "2021-03-08T07:59:03.904684",
     "exception": false,
     "start_time": "2021-03-08T07:59:03.803689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.4. Store Preprocessed data (optional)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T15:42:46.366182Z",
     "iopub.status.busy": "2023-04-12T15:42:46.365761Z",
     "iopub.status.idle": "2023-04-12T15:42:46.448228Z",
     "shell.execute_reply": "2023-04-12T15:42:46.447079Z",
     "shell.execute_reply.started": "2023-04-12T15:42:46.366146Z"
    },
    "papermill": {
     "duration": 0.109823,
     "end_time": "2021-03-08T07:59:04.11528",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.005457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## save preprocessed data\n",
    "prep_path = '/kaggle/working/prepped_data/'\n",
    "if not os.path.exists(prep_path):\n",
    "    os.mkdir(prep_path)\n",
    "    \n",
    "np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n",
    "np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n",
    "# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: We have used the DeepFace preprocessor on the test set as well. However, this is computationally very intensive (processing the whole dataset took us around 3 hours of computing), which is why we do not re-do the preproccesing here. Rather, we will import the data and point towards the relevant directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:49:24.926754Z",
     "iopub.status.busy": "2023-04-12T14:49:24.926353Z",
     "iopub.status.idle": "2023-04-12T14:49:24.932927Z",
     "shell.execute_reply": "2023-04-12T14:49:24.931745Z",
     "shell.execute_reply.started": "2023-04-12T14:49:24.926717Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prep_X_loc = \"/kaggle/input/test-preprocessed/test\"\n",
    "print(f\"Loading preprocessed test data from {test_prep_X_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100212,
     "end_time": "2021-03-08T07:59:04.516059",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.415847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Feature Representations\n",
    "\n",
    "Now that we preprocessed the data and extracted the faces out of the provided training data with an effective face detector, the next step is to build lower-dimensional representations, known as **feature representations** of the faces of each of the three classes. These features will be provided to our classifiers later on to be able to infer the class (recognize the face) in a new image.\n",
    "\n",
    "Our feature extractors will be subclasses of a simple base class that implements the identity function:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:22.991443Z",
     "iopub.status.busy": "2023-04-12T14:16:22.991082Z",
     "iopub.status.idle": "2023-04-12T14:16:22.997374Z",
     "shell.execute_reply": "2023-04-12T14:16:22.996161Z",
     "shell.execute_reply.started": "2023-04-12T14:16:22.991410Z"
    },
    "papermill": {
     "duration": 0.108781,
     "end_time": "2021-03-08T07:59:04.725071",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.61629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.134288,
     "end_time": "2021-03-08T07:59:04.959911",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.825623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1. Baseline 1: Scale Invariant Feature Transform (SIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method to extract features out of images is the **scale invariant feature transform** (SIFT). SIFT is a method of feature extraction that is invariant to various transformations, such as scale, orientation, angle, etc. This makes it a promising method for face recognition as it can extract facial features specific to a person of interest from various images. A successful SIFT implementation is robust (same features are extracted from the same object in different conditions) and discriminative (different image objects can be easily separated from each other in feature space).\n",
    "\n",
    "In this section we will implement SIFT using the opencv framework and demonstrate the advantages and disadvantages of these handcrafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:26.076012Z",
     "iopub.status.busy": "2023-04-12T14:16:26.075130Z",
     "iopub.status.idle": "2023-04-12T14:16:26.093643Z",
     "shell.execute_reply": "2023-04-12T14:16:26.092516Z",
     "shell.execute_reply.started": "2023-04-12T14:16:26.075959Z"
    },
    "papermill": {
     "duration": 0.110122,
     "end_time": "2021-03-08T07:59:05.171171",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.061049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SIFTFeatureExtractor(IdentityFeatureExtractor):\n",
    "\n",
    "    # initialize feature extractor\n",
    "    def __init__(self, nFeatures, sigma, nOctaveLayers = 3, contrastThreshold = .04, edgeThreshold=10):\n",
    "        self.nFeatures = nFeatures\n",
    "        self.sigma = sigma\n",
    "        self.allDescriptors = None\n",
    "        self.allKeypoints = None\n",
    "        self.sift = cv2.SIFT_create(nfeatures=nFeatures, sigma=sigma, nOctaveLayers=nOctaveLayers, contrastThreshold=contrastThreshold, edgeThreshold=edgeThreshold)\n",
    "\n",
    "    # get keypoints and feature descriptors for each image\n",
    "    def detect_and_compute(self, images):\n",
    "        all_descriptors = np.zeros((len(images),self.nFeatures, 128))\n",
    "        all_keypoints = []\n",
    "        # iterate over all images\n",
    "        for i in range(len(images)):\n",
    "            im = np.array(images[i], dtype='uint8')\n",
    "            # convert to grayscale\n",
    "            gray= cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "            # apply bilateral filter to reduce noise\n",
    "            filtered = cv2.bilateralFilter(gray, 3,150,150)\n",
    "            # detectAndCompute returns keypoints and feature descriptors\n",
    "            kp1, des1 = self.sift.detectAndCompute(filtered, None)\n",
    "            (rows,columns) = des1.shape\n",
    "            # maintain only nFeatures features for each image\n",
    "            if rows>self.nFeatures:\n",
    "                des1 = des1[0:self.nFeatures]\n",
    "            all_descriptors[i] =np.array(des1)\n",
    "            all_keypoints.append(np.array(kp1)[0:self.nFeatures])\n",
    "        self.allKeypoints = all_keypoints\n",
    "        self.allDescriptors = all_descriptors\n",
    "        return all_descriptors, all_keypoints\n",
    "\n",
    "    # return all matching features between two images\n",
    "    def match(self, des1, des2):\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50) # or pass empty dictionary\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = matcher.knnMatch(np.float32(des1), np.float32(des2), k=2)\n",
    "        return matches\n",
    "\n",
    "    # draw the keypoints of an image\n",
    "    def drawKeypoints(self, img, keypoints):\n",
    "        image = np.zeros((100,100,3))\n",
    "        image =cv2.drawKeypoints(img,keypoints,image)\n",
    "        plt.imshow(image)\n",
    "\n",
    "    # draw the matches between two images\n",
    "    def drawMatches(self, im1, kp1, des1, im2, kp2, des2):\n",
    "        matches = self.match(des1, des2)\n",
    "        ratio_thresh = .85\n",
    "        good_matches = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < ratio_thresh * n.distance:\n",
    "                good_matches.append([m])\n",
    "        im = cv2.drawMatchesKnn(im1, kp1, im2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        fig=plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(im)\n",
    "        plt.show()\n",
    "\n",
    "    # filter out the poor matches between two images\n",
    "    def getGoodFeatures(self, class_template, desc, kp):\n",
    "        good_descriptors = []\n",
    "        good_keypoints = []\n",
    "        for i,des in enumerate(desc):\n",
    "            matches = SIFTExtractor.match(class_template, des)\n",
    "            # measure euclidian distance between two matches\n",
    "            ratio_thresh = .85\n",
    "            for m,n in matches:\n",
    "                if m.distance < ratio_thresh * n.distance:\n",
    "                    good_descriptors.append(des[m.queryIdx])\n",
    "                    good_keypoints.append(kp[i][m.queryIdx])\n",
    "        return good_descriptors, good_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100377,
     "end_time": "2021-03-08T07:59:05.372401",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.272024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.1. Extracting Features\n",
    "\n",
    "Here, we will demonstrate how to use SIFT and interpret the results. First we initialize our SIFT extractor with the parameters `nFeatures=10` and `sigma=.9`. Selecting a value of 10 for `nFeatures` allows our SIFT extractor to limit keypoints to discriminative features such as the contours of the eyes, nose, mouth. The `sigma` parameter represents the sigma of the Gaussian that is applied to the image. Since our images have a weak quality and an additional bilateral filter is applied to each image before extracting features, we reduce this number.\n",
    "\n",
    "These values (as well as the optional parameters `contrastTreshold`, `edgeThreshold`, `nOctaveLayers`) can be customized for various recognition tasks and depending on the quality of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:27.198514Z",
     "iopub.status.busy": "2023-04-12T14:16:27.197761Z",
     "iopub.status.idle": "2023-04-12T14:16:27.203898Z",
     "shell.execute_reply": "2023-04-12T14:16:27.202638Z",
     "shell.execute_reply.started": "2023-04-12T14:16:27.198475Z"
    },
    "papermill": {
     "duration": 0.100308,
     "end_time": "2021-03-08T07:59:05.57403",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.473722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIFTExtractor = SIFTFeatureExtractor(nFeatures=10, sigma=.6, nOctaveLayers = 3, contrastThreshold = .04, edgeThreshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the descriptors and keypoints are extracted from four images. The matches between these four images will be visualized in the following cells. (For the discussion on SIFT, we rely on the HAAR preprocessed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:29.424066Z",
     "iopub.status.busy": "2023-04-12T14:16:29.423703Z",
     "iopub.status.idle": "2023-04-12T14:16:29.574033Z",
     "shell.execute_reply": "2023-04-12T14:16:29.572988Z",
     "shell.execute_reply.started": "2023-04-12T14:16:29.424033Z"
    }
   },
   "outputs": [],
   "source": [
    "# get keypoint and feature descriptors from each class of data\n",
    "descriptors_mila, keypoints_mila = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==2])\n",
    "descriptors_jesse, keypoints_jesse = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==1])\n",
    "descriptors_michael_and_sarah, keypoints_michael_and_sarah = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==0])\n",
    "\n",
    "gray_jesse_1      = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==1][0]),cv2.COLOR_BGR2GRAY)\n",
    "gray_jesse_2      = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==1][1]),cv2.COLOR_BGR2GRAY)\n",
    "gray_michael_1 = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==0][0]),cv2.COLOR_BGR2GRAY)\n",
    "gray_mila_1        = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==2][0]),cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the keypoints on an image of Jesse Eisenberg. Notably, we can see some distinct features extracted from the eyes, mouth, and nostrils. Additional extracted features we can expect from other images in our datasest include the hair and ears.\n",
    "\n",
    "One unexpected feature comes from the background of the current image. This could be removed my improving the cropping in the preprocessing of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:30.908773Z",
     "iopub.status.busy": "2023-04-12T14:16:30.908383Z",
     "iopub.status.idle": "2023-04-12T14:16:31.108500Z",
     "shell.execute_reply": "2023-04-12T14:16:31.107479Z",
     "shell.execute_reply.started": "2023-04-12T14:16:30.908738Z"
    }
   },
   "outputs": [],
   "source": [
    "SIFTExtractor.drawKeypoints(gray_jesse_1, keypoints_jesse[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the features of two different images of Jesse Eisenberg. We see that the eye and nostril feature descriptors are identified as matching by the SIFT extractor in both image representations. We do however see that the extractor matches one feature incorrectly. This may be evidence of the feature extractor being too local. Nonetheless, the features are identified as specific to Jesse Eisenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:33.356083Z",
     "iopub.status.busy": "2023-04-12T14:16:33.355378Z",
     "iopub.status.idle": "2023-04-12T14:16:33.562701Z",
     "shell.execute_reply": "2023-04-12T14:16:33.561565Z",
     "shell.execute_reply.started": "2023-04-12T14:16:33.356043Z"
    }
   },
   "outputs": [],
   "source": [
    "SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_jesse_2, keypoints_jesse[1], descriptors_jesse[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the features of Jesse Eisenberg and Mila Kunis. We see that no features are identified as matching between these two images. This suggests that the feature representations are discriminative as they are distinguishable between two different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:35.049183Z",
     "iopub.status.busy": "2023-04-12T14:16:35.048479Z",
     "iopub.status.idle": "2023-04-12T14:16:35.254630Z",
     "shell.execute_reply": "2023-04-12T14:16:35.253619Z",
     "shell.execute_reply.started": "2023-04-12T14:16:35.049142Z"
    }
   },
   "outputs": [],
   "source": [
    "SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_mila_1, keypoints_mila[0], descriptors_mila[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A true test of our feature recognition system is comparing the features of two 'lookalikes'. For this we visualize the features of Jesse Eisenberg and Michael Cera. The SIFT extractor matches a feature on Jesse and Michael's eye which shows that the matcher is not as discriminative with lookalikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:36.768881Z",
     "iopub.status.busy": "2023-04-12T14:16:36.767385Z",
     "iopub.status.idle": "2023-04-12T14:16:36.972559Z",
     "shell.execute_reply": "2023-04-12T14:16:36.971555Z",
     "shell.execute_reply.started": "2023-04-12T14:16:36.768830Z"
    }
   },
   "outputs": [],
   "source": [
    "SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_michael_1, keypoints_michael_and_sarah[0], descriptors_michael_and_sarah[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100596,
     "end_time": "2021-03-08T07:59:05.775686",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.67509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.2. T-SNE Plot\n",
    "\n",
    "The features we extracted with SIFT are still located in a high-dimensional space, such that it is hard to visualize the feature representations of our training images. In order to visualize them, we have to postprocess the features with a **T-SNE** or **t-Distributed Stochastic Neighbor Embedding**, which is a method used to visualie high dimensional data in fewer dimensions.\n",
    "\n",
    "Here we visualize the good descriptors of Jesse Eisenberg and Mila Kunis. A descriptor is deemed good if it is similar to other descriptors in the same locality of the image. Similarity here is measured with euclidean distance.\n",
    "\n",
    "In this section we will use the t-SNE framework from sklearn. We first initialize a TSNE object with the parameters `n_components=2`, `perplexity=20`, `early_exaggeration=20` and the optional parameters `learning_rate='auto'`, `init='random'` and  `n_iter=2000`.\n",
    "\n",
    "We choose 2 for the number of components because it more intuitive for visualization purposes. Perplexity is generally a value between 5 and 50 that represents the balance between local and global features in our feature space. We choose 50 for this value as we estimate 50 near neighbors in each class. A value of 20 is chosen for `early_exaggeration` to accentuate the distance between classes.\n",
    "\n",
    "T-SNE uses a factor of randomization to create the plot, thus each run may result in a slightly different distribution. Nonetheless we can see that the features for Jesse and Mila form two distinct classes. However, in some runs the distance between these two classes is not very big. This is expected as the feature representations for facial features may be similar even from person to person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:16:59.427569Z",
     "iopub.status.busy": "2023-04-12T14:16:59.426856Z",
     "iopub.status.idle": "2023-04-12T14:17:00.546487Z",
     "shell.execute_reply": "2023-04-12T14:17:00.545544Z",
     "shell.execute_reply.started": "2023-04-12T14:16:59.427501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the 'good' descriptors of all images of mila and jesse. A descriptor is deemed good if it is similar to other descriptors in the same locality of the image where similarity is measured with euclidean distance.\n",
    "good_mila_descriptors, _ = SIFTExtractor.getGoodFeatures(descriptors_mila[0], descriptors_mila, keypoints_mila)\n",
    "good_jesse_descriptors, _ = SIFTExtractor.getGoodFeatures(descriptors_jesse[0], descriptors_jesse, keypoints_jesse)\n",
    "\n",
    "# plot the tsne\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=50, n_iter=2000, verbose=1, early_exaggeration=20)\n",
    "z_jesse = tsne.fit_transform(np.array(good_jesse_descriptors))\n",
    "z_mila = tsne.fit_transform(np.array(good_mila_descriptors))\n",
    "\n",
    "sns.scatterplot(x=z_jesse[:,0], y=z_jesse[:,1], label='jesse')\n",
    "sns.scatterplot(x=z_mila[:,0], y=z_mila[:,1], label='mila')\n",
    "plt.grid()\n",
    "plt.axhline(0, color=\"black\")\n",
    "plt.axvline(0, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Discussion\n",
    "In this section we observed that the handcrafted SIFT features are discriminative. From both the visualized matching keypoints and the TSNE plot we observe that SIFT effectively separates features of two different people, namely Jesse Eisenberg and Mila Kunis.\n",
    "\n",
    "In regard to robustness, the handcrafted features perform less effectively in this domain. While the SIFT extractor generally extracts a few features effectively (such as the eye and nostril features in the first image of Jesse Eisenberg), only a few 'good' features are extracted for each image. A more robust system would extract features from the eyes, nose, mouth, and ears for each image regardless of the image conditions. The system can be made more robust by improving the preprocessing of images before extracting features. While the SIFT extractor applies a guassian to images before grabbing features, additional steps such as normalizing contrast and brightness could improve the definition of features.\n",
    "\n",
    "Compared to the previous grabbing task in the individual assignment, the SIFT extractor applied in facial recognition must be more local. What is meant by this is that distinguishing features from person to person is a more difficult task than object recognition seen in the previous assignment. Because of this the SIFT extractor needs to be well tuned to distinguish slight the intricacies of facial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101426,
     "end_time": "2021-03-08T07:59:05.978236",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.87681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method to extract features out of image data that we explore is principal component analysis. __Principal Component Analysis__ (PCA) is a technique used to extract highly variant components (set of points) from data. In our case, we will apply it to faces in order to extract those components that make up the variability of the faces. Ideally, we would discretize or isolate components containing facial features that can be later used to discriminate the faces in our dataset. Most of the work in applying this method relies in picking informative components and the least amount of background noise as well as aligning the faces to a reference point to reduce variance generated only based on the location of the face.\n",
    "\n",
    "For PCA, we first convert each image in our training set into a one-dimensional array by flattening the images. If color images are used, we flatten each color channel and concatenate them. For simplicity, we discuss the case of performing PCA on square grasycale images, each having the same size $K\\times K$ (with, by default, $224\\times 224$ for our training set). After flattening, we hence have a matrix $M$, often called the __data matrix__ of size $Z \\times K^2$, with $Z$ the number of training examples. The columns (flattened pixels) then represent random variables, while the rows indicate different samples of those random variables. In our case, the matrix hence has size $80 \\times 10 \\ 000$. Given this data matrix, we zero-mean the dataset. That is, we compute the mean of each column and subtract it from all training examples. The reason behind centering the data is that the covariance matrix is sensitive to the mean of the data: without centering, the covariance matrix will reflect the location of the mean of the data (the bias in the training data) as well as its variability, which makes for a worse PCA performance.\n",
    "\n",
    "There are then two ways to proceed with PCA. The first option is to compute the covariance matrix of this data matrix $M$. This will give a covariance matrix $C$ of size $K^2 \\times K^2$, of which we compute the eigenvalue decomposition (EVD). The principal components are then determined by the largest eigenvalues and their corresponding eigenvectors of this covariance matrix, since they indicate the directions of highest variance. By keeping $q$ principal components, we compute a projection matrix of size $q \\times K^2$ which stores the corresponding eigenvectors as columns. Given a flattened image $x$ with shape $K^2 \\times 1$, we can compute a lower-dimensional representation $z = W\\cdot x$, and $z$ is often referred to as the __feature vector__. The time complexity of computing the EVD on a $n\\times n$ matrix is $\\mathcal{O}(n^3)$.\n",
    "\n",
    "The other approach is to use **singular value decomposition** (SVD). It generalizes the concept of eigenvalue decomposition, which can only be applied to square matrices, to matrices of any shape. Similar to the EVD case, here we keep the $q$ largest singular values and their corresponding vectors as the principal components. The time complexity of computing the SVD on a $m\\times n$ matrix is $\\mathcal{O}\\left( \\text{min}\\left( mn^2, m^2 n \\right) \\right)$.\n",
    "\n",
    "In our case, the matrix of interest has shape $80 \\times 10 \\ 000$. Due to this dimensionality, computing the SVD of this matrix is much more efficient than the EVD. During our experiments, we noted that computing the EVD on the covariance matrix could easily take more than 10 minutes, whereas the SVD takes only a few seconds. We did not see any significant advantage for the EVD in terms of performance. As such, we decided to stick to the SVD in this implementation. Our implementation makes use of scikit-learn's PCA, which in turn calls numpy's SVD optimised function.\n",
    "\n",
    "One challenge with PCA is choosing the number of principal components. One way to choose this value is by looking at the eigenvalues/singular values $\\lambda_i$, $i = 1, ..., N$ of the data matrix, ordered in decreasing magnitude. (For EVD, $N$ equals $K^2$, while for SVD, $N$ equals $Z$). We can then decide the number of principal components $q$ by requiring that these components explain a certain percentage $p$ of the variance in the dataset, *i.e.* choose $q$ such that\n",
    "\\begin{equation*}  \n",
    "    \\frac{\\sum_{i=1}^q \\lambda_i}{\\sum_{i=1}^N \\lambda_i} \\geq p \\, .\n",
    "\\end{equation*}  \n",
    "Another approach could be to investigate the average reconstruction loss on the training images. In the end, the number of principal components is another hyperparameter of the classifier, and one needs to carefully tune this hyperparameter, such as via a knee plot. \n",
    "\n",
    "Lastly, although already mentioned briefly, it is worth explaining the decision behind doing the mean subtraction. It is a common preprocessing step in PCA, since it improves the resulting covariance matrix, due to its sensitivity to the mean of the data. Commonalities on every face are not informative, so with a minimum information loss we can focus on the variations around the mean. In addition to that, interpreting the meaning of the principal components gets facilitated, since they get aligned with the directions of maximum variation. At the same time, although talking about PCA on square grayscale images where it might not be that powerful, mean subtraction can remove some bias due to lighting as it would subtract the mean pixel intensity making it easier to focus on the relevant variations.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:17:08.331784Z",
     "iopub.status.busy": "2023-04-12T14:17:08.331399Z",
     "iopub.status.idle": "2023-04-12T14:17:08.346024Z",
     "shell.execute_reply": "2023-04-12T14:17:08.344943Z",
     "shell.execute_reply.started": "2023-04-12T14:17:08.331748Z"
    }
   },
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \"\"\"\n",
    "    PCA feature extractor which, given training data, finds the ideal set of principal components. Inspired by assignment 3 of the artificial neural networks course.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=20, use_color=True):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            n_components (int/float, optional): Number of components to keep. Either an integer or a float. When a float, this is the percentage\n",
    "            of the total variance that has to be explained by the principal components. Defaults to 20.\n",
    "            use_color (bool, optional): Whether to use all color channels for PCA, or to work with grayscale images. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_components = n_components\n",
    "        # ^ the number of principal components to be computed\n",
    "        self.model = None\n",
    "        # ^ the PCA model, will be initialized in fit\n",
    "        self.meanface = None\n",
    "        # ^ the mean of all the input training data\n",
    "        self.use_color = use_color\n",
    "        # ^ whether we train PCA on color images or on grayscale images\n",
    "\n",
    "        # We will save the original shape of the input data for convenience, used inside this class\n",
    "        self.width     = None\n",
    "        self.height    = None\n",
    "        self.shape     = None\n",
    "\n",
    "    def preprocess_data(self, X):\n",
    "        \"\"\"Preprocess the images, such as converting to grayscale and flattening.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Data matrix containing several images of the same shape.\n",
    "\n",
    "        Returns:\n",
    "            None: no return\n",
    "        \"\"\"\n",
    "        # Save the original shape for later on\n",
    "        self.shape = X.shape[1:]\n",
    "        \n",
    "        # In case we use grayscale, drop color channels\n",
    "        if not self.use_color:\n",
    "            self.shape = self.shape[:-1]\n",
    "        self.width, self.height = self.shape[0], self.shape[1]\n",
    "\n",
    "        if self.use_color:\n",
    "            self.data_matrix = np.array([img.flatten() for img in X])\n",
    "        else:\n",
    "            # Convert images to grayscale\n",
    "            X_gray = np.mean(X, axis=3)\n",
    "            # Flatten images\n",
    "            self.data_matrix = np.array([img.flatten() for img in X_gray])\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the PCA on the training data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): The training data, consisting of several images of the same shape.\n",
    "        \"\"\"\n",
    "        self.model = PCA(self.n_components, svd_solver=\"full\", whiten=True)\n",
    "        self.preprocess_data(X)\n",
    "        self.model.fit(self.data_matrix)\n",
    "        # Also save the mean face as a 2D image, in color or grayscale\n",
    "        self.meanface = rescale_intensity(self.model.mean_.reshape(self.shape), out_range=(0, 255)).astype(np.uint8)\n",
    "        # Save the eigenfaces for convenience of plotting later on:\n",
    "        self.eigenfaces = np.array([rescale_intensity(face.reshape(self.shape), out_range=(0, 255)).astype(np.uint8) for face in self.model.components_])\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute feature vector of the given sequence of images according to the fitted PCA, i.e. project on principal components.\n",
    "        NOTE has to be an array of images, so for a single image, put it in an array of one element\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Sequence of images to be converted to their latent/feature representation.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Features of the sequence of provided images.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Simply call the transform of scikit-learn's PCA, but reshape the matrix in desired shape\n",
    "        X = np.array([img.flatten() for img in X])\n",
    "\n",
    "        return self.model.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Reconstruct images (i.e. convert back to their original shape) based on their latent feature representation.\n",
    "        NOTE has to be an array of images, so for a single image, put it in an array of one element\n",
    "        Args:\n",
    "            X (np.array): Feature representation of the images.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Reconstructed images.\n",
    "        \"\"\"\n",
    "        X = np.array([img.flatten() for img in X])\n",
    "\n",
    "        # Go from the latent space back to original space\n",
    "        reconstructed = self.model.inverse_transform(X)\n",
    "        # Reshape back into a 2D image (works for both color or gray)\n",
    "        reconstructed = np.array([img.reshape(self.shape) for img in reconstructed])\n",
    "        # Normalize the values:\n",
    "        reconstructed = rescale_intensity(reconstructed, out_range=(0, 255)).astype(np.uint8)\n",
    "\n",
    "        return reconstructed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate our discussion above on \"explained variance\" as determined by the size of the eigenvalues/singular values, we can plot the singular values, ordered according to decreasing magnitude. This is stored in scikit-learn's PCA object (for more information, consult [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:17:10.079149Z",
     "iopub.status.busy": "2023-04-12T14:17:10.078057Z",
     "iopub.status.idle": "2023-04-12T14:17:13.237847Z",
     "shell.execute_reply": "2023-04-12T14:17:13.236795Z",
     "shell.execute_reply.started": "2023-04-12T14:17:10.079097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize PCA, but DO NOT set number of components to have explained variance\n",
    "pca = PCAFeatureExtractor(n_components=None)\n",
    "pca.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:17:30.225422Z",
     "iopub.status.busy": "2023-04-12T14:17:30.224856Z",
     "iopub.status.idle": "2023-04-12T14:17:30.432762Z",
     "shell.execute_reply": "2023-04-12T14:17:30.431788Z",
     "shell.execute_reply.started": "2023-04-12T14:17:30.225385Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    plt.plot(pca.model.singular_values_, \"-o\", color=\"red\")\n",
    "    plt.xlabel(\"Singular value index\")\n",
    "    plt.ylabel(\"Singular value magnitude\")\n",
    "    plt.title(\"Singular values for training data\")\n",
    "    plt.grid()\n",
    "    plt.axhline(0, color=\"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one singular value that is equal to zero in this case. We can also plot ratio of the cumulative sum of these singular values, which gives the explained vario, over the total sum of all singular values to determine the ratio of the equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:18:02.800367Z",
     "iopub.status.busy": "2023-04-12T14:18:02.799745Z",
     "iopub.status.idle": "2023-04-12T14:18:03.036534Z",
     "shell.execute_reply": "2023-04-12T14:18:03.035529Z",
     "shell.execute_reply.started": "2023-04-12T14:18:02.800329Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    # Get the cumulative sum of the explained variance\n",
    "    cumsum = np.cumsum(pca.model.explained_variance_ratio_)\n",
    "\n",
    "    # Plot the cumulative sum\n",
    "    plt.plot([i+1 for i in range(len(cumsum))], cumsum, \"-o\", color=\"red\", zorder=100)\n",
    "\n",
    "    # For visualization, determine when we have 95% explained variance\n",
    "    for i, value in enumerate(cumsum):\n",
    "        if value >= 0.95:\n",
    "            break\n",
    "    plt.axhline(0.95, color=\"black\")\n",
    "    plt.axvline(i + 1, color=\"black\", label = \"95% explained variance\")\n",
    "    plt.xlabel(\"Singular value index\")\n",
    "    plt.ylabel(\"Singular value magnitude\")\n",
    "    plt.title(\"Singular values for training data\")\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the discussion below, we will initialize a default PCA feature extractor which uses 20 principal components. We will work in color, which might give worse results than using grayscale images since we have three times more \"pixel variables\", but working with color images allows for better visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:18:06.046910Z",
     "iopub.status.busy": "2023-04-12T14:18:06.045861Z",
     "iopub.status.idle": "2023-04-12T14:18:09.007156Z",
     "shell.execute_reply": "2023-04-12T14:18:09.005673Z",
     "shell.execute_reply.started": "2023-04-12T14:18:06.046863Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCAFeatureExtractor()\n",
    "pca.fit(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100881,
     "end_time": "2021-03-08T07:59:06.392861",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.29198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.1. Eigenface Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While fitting our PCA feature extractor to the training data, we also saved the __mean face__. Remember that PCA flattens the images into arrays of size $K^2$, and views the pixels as random variables. We can hence take the mean of each pixel over our training set, and end up with an array containing the mean of all pixels. By reshaping this mean array into an image of size $K\\times K$, we can reconstruct this average as an image, and this average is called the mean face: it gives an average representation of a face, according to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:18:19.019775Z",
     "iopub.status.busy": "2023-04-12T14:18:19.019307Z",
     "iopub.status.idle": "2023-04-12T14:18:19.388612Z",
     "shell.execute_reply": "2023-04-12T14:18:19.387369Z",
     "shell.execute_reply.started": "2023-04-12T14:18:19.019734Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    plt.imshow(pca.meanface, cmap=\"Greys_r\")\n",
    "    plt.title(\"Mean face from training data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fitting the PCA feature extractor, we keep a certain number of \"principal components\" which are either eigenvectors of the covariance matrix or vectors corresponding to singular values, according to the decomposition used in the PCA. Similar to the mean vector, we can reshape these eigenvectors into an image of size $K\\times K$ and visualize them. When the training data consists of faces, these eigenvectors are often referred to as __eigenfaces__, since (when visualized) they resemble faces. These faces (vectors) are used to project images of faces onto a lower-dimensional representation, and due to their order, they maximize the amount of information they retain of the original faces. Below, we plot some of these eigenfaces, in order of decreasing singular values (hence, most important to least important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:18:23.735473Z",
     "iopub.status.busy": "2023-04-12T14:18:23.734875Z",
     "iopub.status.idle": "2023-04-12T14:18:31.175069Z",
     "shell.execute_reply": "2023-04-12T14:18:31.173662Z",
     "shell.execute_reply.started": "2023-04-12T14:18:23.735434Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_eigenfaces_to_show = pca.n_components\n",
    "nb_per_row = 10\n",
    "if SHOW_PLOTS:\n",
    "    plot_image_sequence(pca.eigenfaces[0:nb_eigenfaces_to_show], imgs_per_row=nb_per_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101263,
     "end_time": "2021-03-08T07:59:06.797448",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.696185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.2. Feature Space Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the projection of a data point by PCA on a lower-dimensional representation is reversible, we can compare an original image with its \"reconstruction\". That is, we project an image on its lower-dimensional representation, and apply the reverse transformation to get back an image of the original size. This can essentially be seen as a compression or reconstruction, as the new image is created from less information. The number of principal components determines the quality of the reconstructed image. Below, we show an example of such a reconstruction using 20 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:19:16.252447Z",
     "iopub.status.busy": "2023-04-12T14:19:16.251958Z",
     "iopub.status.idle": "2023-04-12T14:19:16.697496Z",
     "shell.execute_reply": "2023-04-12T14:19:16.696440Z",
     "shell.execute_reply.started": "2023-04-12T14:19:16.252402Z"
    },
    "papermill": {
     "duration": 0.101801,
     "end_time": "2021-03-08T07:59:07.000598",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.898797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take an example face to reconstruct\n",
    "example_face = train_X[0]\n",
    "# If we don't use color, convert to grayscale\n",
    "if not pca.use_color:\n",
    "    example_face = np.mean(example_face, axis=2)\n",
    "\n",
    "# Plot the original face\n",
    "if SHOW_PLOTS:\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "    ax0.imshow(example_face.reshape(pca.shape), cmap=\"Greys_r\")\n",
    "    ax0.set_title(\"Original face\")\n",
    "    # Get reconstruction: first, project onto lower dimensonional repr\n",
    "    reduced = pca.transform([example_face])\n",
    "    # Then, reverse projection and reconstruct original\n",
    "    reconstructed = pca.inverse_transform([reduced])[0]\n",
    "    ax1.imshow(reconstructed, cmap=\"Greys_r\")\n",
    "    ax1.set_title(f\"Reconstructed face (q = {pca.n_components})\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the quality of this reconstruction depends on the amount of principal components to keep. Above, there are clear deviations between the original and reconstructed images. Below, we vary over the amount of principal components, below denoted by $q$, we use in the representation. This clearly shows that, while we clearly see only the mean face for low $q$, the reconstruction gradually uses more information of the face of interest, until the final images, with a high value of $q$, show hardly any difference with the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:23:32.772426Z",
     "iopub.status.busy": "2023-04-12T14:23:32.771447Z",
     "iopub.status.idle": "2023-04-12T14:24:05.862382Z",
     "shell.execute_reply": "2023-04-12T14:24:05.861519Z",
     "shell.execute_reply.started": "2023-04-12T14:23:32.772385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify which components we are going to look at\n",
    "max_nb_components = 50\n",
    "step_size = 5\n",
    "n_components_list = np.arange(step_size, max_nb_components+1,step_size)\n",
    "nb_per_row = 5\n",
    "\n",
    "if SHOW_PLOTS:\n",
    "    fig, axs = plt.subplots(len(n_components_list)//nb_per_row, nb_per_row, figsize = (15, 10))\n",
    "    for i, n_components in enumerate(n_components_list):\n",
    "        # Get the current axis\n",
    "        ax = axs[i // nb_per_row, i % nb_per_row]\n",
    "        # Set number of principal components and recompute the projection matrix\n",
    "        pca.n_components = n_components\n",
    "        pca.fit(train_X)\n",
    "        # Compute the reconstruction\n",
    "        reconstructed = pca.inverse_transform(pca.transform([example_face]))[0]\n",
    "        # Plot it\n",
    "        ax.imshow(reconstructed, cmap=\"Greys_r\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"q = {n_components}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, while we visually see that the difference between the original image and its reconstruction decreases, we want to have a method of quantifying this difference. One possible measure between the two images is the **root-mean-square deviation** (RMSD) between the two images, viewed as two arrays $\\boldsymbol{x}$ and $\\hat{\\boldsymbol{x}}$ of size $K^2$. The RMSD measure is then defined as\n",
    "\\begin{equation*}\n",
    "    RMSD(\\boldsymbol{x}, \\hat{\\boldsymbol{x}}) = \\sqrt{\\frac{\\sum_{i=1}^{K^2} (x_i - \\hat{x}_i)}{K^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:24:45.543843Z",
     "iopub.status.busy": "2023-04-12T14:24:45.543005Z",
     "iopub.status.idle": "2023-04-12T14:24:45.550654Z",
     "shell.execute_reply": "2023-04-12T14:24:45.549414Z",
     "shell.execute_reply.started": "2023-04-12T14:24:45.543799Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmsd(x: np.array, x_hat: np.array):\n",
    "    \"\"\"\n",
    "    Computes root mean square deviation between two Numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sqrt(np.mean((x - x_hat)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will again iterate over the number of principal components $q$ we keep in the PCA, and compute for each value of $q$ the average RMSD on the training set. This could give us insights into how much information is being retained by the PCA projection, and could *e.g.* inform us on which value of $q$ would be the most ideal to use in our classification pipeline.\n",
    "\n",
    "*Note*: the following cell can take some time to evaluate, hence we \"jump\" in the number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:25:43.380464Z",
     "iopub.status.busy": "2023-04-12T14:25:43.380019Z",
     "iopub.status.idle": "2023-04-12T14:26:21.859639Z",
     "shell.execute_reply": "2023-04-12T14:26:21.858513Z",
     "shell.execute_reply.started": "2023-04-12T14:25:43.380423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the RMSD errors in a list\n",
    "errors = []\n",
    "\n",
    "# Check whether we train with grayscale or color images\n",
    "if pca.use_color:\n",
    "    original_data = train_X\n",
    "else:\n",
    "    # If we use grayscale, convert all the training images to grayscale as well\n",
    "    original_data = np.array([np.mean(train_face, axis=2) for train_face in train_X])\n",
    "\n",
    "for i in tqdm(range(len(n_components_list))):\n",
    "    n_components = n_components_list[i]\n",
    "    # Set number of principal components and recompute the projection matrix\n",
    "    pca.n_components = n_components\n",
    "    pca.fit(train_X)\n",
    "    # Compute the reconstruction\n",
    "    reconstructed_faces = pca.inverse_transform(pca.transform(original_data))\n",
    "    error = np.mean([rmsd(original_data, reconstructed_faces)])\n",
    "    # Append to list, make sure to reshape\n",
    "    errors.append(error)\n",
    "\n",
    "if SHOW_PLOTS:\n",
    "    plt.plot(n_components_list, errors, '-o', color='red')\n",
    "    plt.ylabel(\"RMSD\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.title(\"Reconstruction error for varying number of principal components\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to choose the number of principal components to keep for the classifier later on. For instance, we can consider the \"gain\" in RMSD when we increase the number of principal components to keep. Below, we plot for each value of $q$ the difference between the RMSD value with $q$ components and $q+1$ components. Together with the previous plot, we see that we have the best results for $q=40$ or $q=50$, so it would make sense to try different values of $q$ in the classification pipeline later on to tune this hyperparameter. We expect that, while the reconstruction loss goes steadily down for increasing number of principal components, having simpler features can be beneficial for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:26:26.911005Z",
     "iopub.status.busy": "2023-04-12T14:26:26.910079Z",
     "iopub.status.idle": "2023-04-12T14:26:27.141438Z",
     "shell.execute_reply": "2023-04-12T14:26:27.140513Z",
     "shell.execute_reply.started": "2023-04-12T14:26:26.910950Z"
    }
   },
   "outputs": [],
   "source": [
    "errors = np.array(errors)\n",
    "next_errors = np.roll(errors, -1)\n",
    "differences = errors - next_errors\n",
    "if SHOW_PLOTS:\n",
    "    plt.plot([i+1 for i in range(len(differences[:-1]))], differences[:-1], '-o', color='red')\n",
    "    plt.ylabel(\"RMSD gain\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.title(\"RMSD gain for varying number of components\")\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantitative measure such as the RMSD is also a great way to compare our different face detectors and choose which preprocessed data is more suited for our use case here. It can also be used to tune the detectors: recall for instance that we defined an auxiliary function to modify the bounding boxes around faces: the hyperparameters involved in the detections can be tuned by their reconstruction error. However, this is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face-Feature Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great aspect about PCA is that we can create so-called **face-feature plots**, which can also be seen for instance in [this tutorial](https://sandipanweb.wordpress.com/2018/01/06/eigenfaces-and-a-simple-face-detector-with-pca-svd-in-python/). In a face-feature plot, we plot the different faces of the training set at locations that are determined by their features obtained from the PCA projection. For visualization purposes, we therefore restrict ourselves to $q = 2$, such that there are only two eigenfaces of interest. Applying the PCA projection on each image then gives us a 2D representation of the faces, which determine the location in the grid where we plot each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:26:32.092675Z",
     "iopub.status.busy": "2023-04-12T14:26:32.092285Z",
     "iopub.status.idle": "2023-04-12T14:26:34.955975Z",
     "shell.execute_reply": "2023-04-12T14:26:34.954445Z",
     "shell.execute_reply.started": "2023-04-12T14:26:32.092639Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.n_components = 2\n",
    "pca.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:26:35.840194Z",
     "iopub.status.busy": "2023-04-12T14:26:35.839836Z",
     "iopub.status.idle": "2023-04-12T14:26:37.180123Z",
     "shell.execute_reply": "2023-04-12T14:26:37.177412Z",
     "shell.execute_reply.started": "2023-04-12T14:26:35.840161Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    # Parameters to play with to increase/decrease the boxes of the faces in the plot\n",
    "    delta_image = 0.25\n",
    "    delta_plot = 0.5\n",
    "    # Obtain the latent representations, i.e. after projecting with the PCA\n",
    "    latent_reps = np.array([pca.transform([image]).flatten() for image in original_data])\n",
    "\n",
    "    # Plot each image at their latent representation\n",
    "    for i in range(train_X.shape[0]):\n",
    "        image = train_X[i]\n",
    "        latent = latent_reps[i]\n",
    "        ax.imshow(image, interpolation='nearest', extent=(latent[0]-delta_image, latent[0]+delta_image, latent[1]-delta_image, latent[1]+delta_image), zorder=100)\n",
    "\n",
    "    # Add annotations\n",
    "    plt.xlim(latent_reps[:, 0].min()-delta_plot, latent_reps[:, 0].max()+delta_plot)\n",
    "    plt.ylim(latent_reps[:, 1].min()-delta_plot, latent_reps[:, 1].max()+delta_plot)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Face-feature plot')\n",
    "    plt.axhline(0, color=\"black\")\n",
    "    plt.axvline(0, color=\"black\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of plotting the faces, it is perhaps more informative to plot the class labels of the faces we extracted. This is done in the plot below, where black stands for class label 0, red for class label 1 and blue for class label 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:26:44.288297Z",
     "iopub.status.busy": "2023-04-12T14:26:44.287925Z",
     "iopub.status.idle": "2023-04-12T14:26:45.856701Z",
     "shell.execute_reply": "2023-04-12T14:26:45.855584Z",
     "shell.execute_reply.started": "2023-04-12T14:26:44.288262Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    # Parameters to play with to increase/decrease the plot window\n",
    "    delta_plot = 0.5\n",
    "    # Obtain the latent representations, i.e. after projecting with the PCA\n",
    "    latent_reps = np.array([pca.transform([image]).flatten() for image in original_data])\n",
    "\n",
    "    # Use colors instead of the faces\n",
    "    colors = [\"black\", \"red\", \"blue\"]\n",
    "\n",
    "    for i in range(original_data.shape[0]):\n",
    "        image = original_data[i]\n",
    "        latent = latent_reps[i]\n",
    "        plt.scatter(latent[0], latent[1], color = colors[train_y[i]])\n",
    "\n",
    "    # Annotate plot\n",
    "    plt.xlim(latent_reps[:, 0].min()-delta_plot, latent_reps[:, 0].max()+delta_plot)\n",
    "    plt.ylim(latent_reps[:, 1].min()-delta_plot, latent_reps[:, 1].max()+delta_plot)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Class labels plotted in PCA space')\n",
    "    plt.axhline(0, color=\"black\")\n",
    "    plt.axvline(0, color=\"black\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from some data points, we see that the red and blue classes are well separated. However, the lookalikes, denoted by the black dots, are spread throughout the PCA space, which is expected since they likely have very similar features as Jesse and Mila. However, we have take into account that this is a PCA representation based on only 2 principal components, which is very low and hence we expect the separation between classes to be quite poor. By increasing the amount of principal components, we can make the PCA features more discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102099,
     "end_time": "2021-03-08T07:59:07.204783",
     "exception": false,
     "start_time": "2021-03-08T07:59:07.102684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick recap, in this section we have seen how does PCA work, which options it provides to choose from depending on the starting data and what results it gives at every step of the process. One of the most critical choices that has to be made is the number of principal components, and for that, the **root-mean-square deviation (RMSD)** was used. This measure gives us a tool to calculate for every iteration on the number of principal components the *loss* of the reconstruction. For us, based on that procedure, the clear result was to avoid any values above 40, since there was no much information gained with higher numbers. In that way, the dimensionality of the feature space gets reduced while still being pretty informative.\n",
    "\n",
    "Additionally, throughout the whole section different plots and graphs have been shown at each step to better understand how do the components of PCA work. Just before this discussion the face-feature plot is shown, and while the lookalikes are distributed over the whole PCA space, it is a normal behaviour since we are just using 2 principal components. The other classes, on the other hand, are clearly separated between them. Apart from that, another representative plot can be seen in the projection of the reconstructions of a face as the number of principal components increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.10088,
     "end_time": "2021-03-08T07:59:07.406787",
     "exception": false,
     "start_time": "2021-03-08T07:59:07.305907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics used throughout the assignment are measures that give us the performance of the face recognition system as a whole. While there are other ones that provide us with the similarity or distance between predicted and true identities that might be used internally in different implemented methods or packages, we decided to just focus on those first ones. Below, we discuss these measures in the context of a binary classification problem for simplicity, with the two classes called positives and negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one, which was already provided in the template, was **accuracy**. Accuracy gives us the ratio between the number of correct predictions and the total amount of predictions:\n",
    "\\begin{equation*}\n",
    "    \\text{accuracy}  = \\frac{\\text{TP} +\\text{TN} }{\\text{TP} +\\text{TN} +\\text{FP}+\\text{FN} }\n",
    "\\end{equation*}\n",
    "where TP stands for *true positives*, TN for *true negatives*, FP for *false positives* and FN for *false negatives*.\n",
    "\n",
    "Although it is used a lot in classification, and it will also be used here, it has a couple of clear *disadvantages* that force us to use other measure as well. Its clearest flaw is its misleadingness when the data is imbalanced. When a class has many more examples than others, a classifier that always predicts the majority class would get a really high accuracy while in reality it would be quite poor if it cannot properly label any other class. However, accuracy it is still a decent starting point to measure the performance of a classifier. In our case, our class distributions are indeed not evenly balanced, but the degree of imbalance is relatively moderate and accuracy gives a rough estimate of a classifier's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** tells us the performance of the system as the ratio between the true positives to the sum of those same ones and the false positives. It can be defined as\n",
    "\\begin{equation*}\n",
    "    \\text{recall}  = \\frac{\\text{TP} }{\\text{TP} +\\text{FN} }\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "his measure excels in situations where identifying all instance of a class is important and false negatives have notable consequences. A high score would indicate that the system is able to identify correctly most of the relevant instance of a class. However, in a similar fashion to the previous one, it has its flaws, and its main one is the tendency to give high scores to models that give a high number of false positives. That is why it is normally used in conjunction with other metrics such as the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score is the harmonic mean of precision and recall, which provides a balance between these two, and therefore, it gives us a better overview of the performance of the system together with the previously mentioned recall. It is calculated as follows:\n",
    "\\begin{equation*}\n",
    "    f1 = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall} }{\\text{precision} +\\text{recall}} = \\frac{2  \\text{TP}}{2  \\text{TP}+\\text{FP}+\\text{FN}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score ranges between 0 and 1, as for the two previous ones as well, where 1 would mean perfect precision and recall. When there are no specific preferences towards better precision or better recall, the F1-score can be used to easily compare different models. The one that has the higher F1-score being the better-performing one. Therefore, it is really useful in our case when trying to come up with different classifiers and different ways of getting feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous discussion relied on a binary classification problem as example, our face recognition application is actually a multi-class classification problem where there are three possible labels: *1* for *Jesse*, *2* for *Mila* and *0* for their lookalikes. For multi-class classification problems, confusion matrices are convenient to estimate the performance of a classifier. A **confusion matrix** is a table where the differences between the true labels and the predicted ones for each class can be seen. Therefore, it is one of the most useful metrics/tools can we can use. Its structure would be like this:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Actual Label 1</th>\n",
    "    <th>Actual Label 2</th>\n",
    "    <th>Actual Label 3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Predicted Label 1</strong></td>\n",
    "    <td>TP for Label 1</td>\n",
    "    <td>FP for Label 1</td>\n",
    "    <td>FP for Label 1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Predicted Label 2</strong></td>\n",
    "    <td>FN for Label 2</td>\n",
    "    <td>TP for Label 2</td>\n",
    "    <td>FP for Label 2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Predicted Label 3</strong></td>\n",
    "    <td>FN for Label 3</td>\n",
    "    <td>FN for Label 3</td>\n",
    "    <td>TP for Label 3</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is where the previous metrics get calculated from, but it is also a great tool of evaluating the different models on its own. You can easily tell for each case what the main problems and strengths might be if you are getting low or high scores in any of the previous measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.103749,
     "end_time": "2021-03-08T07:59:08.894358",
     "exception": false,
     "start_time": "2021-03-08T07:59:08.790609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Classifiers\n",
    "\n",
    "In the previous section, we extracted features out of the image data of the faces of the training set. These features can then serve as input for various classification methods, which then try to learn to identify the different classes (recognize the different faces) based solely on their feature representations. In this section, we will explore classifying our faces with our handcrafted SIFT and PCA features with various classification methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many possible classifiers that can be used for our classification problem. Below, we use a support vector machine (SVM) classifier for the SIFT and PCA features that we extracted above. For another tutorial on how to use an SVM for face recognition, see [here](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html). We also briefly explore a fully-connected neural network as alternative to the SVM. Finally, we will explore VGG-16 as a classifier, which achieves a superior performance due to the fact that this architecture was trained on much more data, and moreover, uses features obtained using deep learning on a larger training set. \n",
    "\n",
    "First, we define a general pipeline that can be used by any feature extractor and any classifier trained on those features. The function takes a preprocessed image as an input (for the test data, we preprocessed the data before and saved a path to the location of all the preprocessed data), as well as a feature extractor and a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:58:05.131986Z",
     "iopub.status.busy": "2023-04-12T14:58:05.131391Z",
     "iopub.status.idle": "2023-04-12T14:58:05.138799Z",
     "shell.execute_reply": "2023-04-12T14:58:05.137767Z",
     "shell.execute_reply.started": "2023-04-12T14:58:05.131941Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_pipeline(feature_extractor, classifier, path=test_prep_X_loc):\n",
    "    \"\"\"\n",
    "    Simple prediction pipeline in case we load in the preprocessed test data.\n",
    "    Args:\n",
    "        feature_extractor (Any): The feature extractor used to extract features out of faces.\n",
    "        classifier (Any): The classifier trained to classify based on the features extracted by feature_extractor.\n",
    "        path (str, optional): Location of the directory containing the preprocessed test data to load and classify.\n",
    "    \"\"\"\n",
    "    # Path points to a directory where preprocessed test data is located\n",
    "    y_pred = []\n",
    "    \n",
    "    # Get the number of files\n",
    "    nb_of_files = len(os.listdir(path))\n",
    "    for i in tqdm(range(nb_of_files)):\n",
    "        # Load data\n",
    "        img = np.load(os.path.join(path, f\"test_{i}.npy\"))\n",
    "        # Convert from BGR to RGB (warning! cv2.cvtColor gives error)\n",
    "        img = img[...,::-1lor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Get the features using the extractor\n",
    "        features = feature_extractor([img])\n",
    "        # Make the predictions\n",
    "        new_pred = classifier(features)\n",
    "        y_pred.append(new_pred)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an auxiliary function that takes care of converting the predictions we made above to a CSV file for submitting to the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:49:02.110911Z",
     "iopub.status.busy": "2023-04-12T14:49:02.110262Z",
     "iopub.status.idle": "2023-04-12T14:49:02.119378Z",
     "shell.execute_reply": "2023-04-12T14:49:02.118244Z",
     "shell.execute_reply.started": "2023-04-12T14:49:02.110869Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prep_X_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:28:45.568874Z",
     "iopub.status.busy": "2023-04-12T14:28:45.567441Z",
     "iopub.status.idle": "2023-04-12T14:28:45.574839Z",
     "shell.execute_reply": "2023-04-12T14:28:45.573782Z",
     "shell.execute_reply.started": "2023-04-12T14:28:45.568825Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictions_to_csv(predictions: np.array, name: str  = 'submission.csv'):\n",
    "    \"\"\"\n",
    "    Auxiliary function that creates a CSV submission file based on predictions made for the test set.\n",
    "    Args:\n",
    "        predictions (np.array): Predictions made on the test set.\n",
    "        name (str, optional): Name of the submission CSV file. Defaults to submission.csv\n",
    "    \"\"\"\n",
    "    submission = test.copy().drop('img', axis = 1)\n",
    "    submission['class'] = predictions\n",
    "    submission.to_csv(name)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM Classifier based on PCA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with an SVM classifier based on the PCA features discussed above. An SVM, or Support Vector Machine, is a classifier that maximizes the distance between two classes when they are linearly separable. When data is not linearly separable, a kernel is applied to transform the data in such a way that a linear decision boundary can still be found. Our aim is that with the SIFT and PCA features, we can find a linear separation between the features of Jesse Eisenberg and Mila Kunis. For that purpose, we use a radial basis function (RBF) kernel (also known as Gaussian kernel) below.\n",
    "\n",
    "For a more general info on the use of SVMs in the context of face recognition, see [this tutorial](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:28:53.686402Z",
     "iopub.status.busy": "2023-04-12T14:28:53.685453Z",
     "iopub.status.idle": "2023-04-12T14:28:53.695097Z",
     "shell.execute_reply": "2023-04-12T14:28:53.693937Z",
     "shell.execute_reply.started": "2023-04-12T14:28:53.686362Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM_PCA_classifier:\n",
    "    \"\"\"\n",
    "    Implements an SVM classifier using the PCA features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # The model will be fitted layer on\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Fit the SVM with the given data.\n",
    "        Args:\n",
    "            X (np.array): Input data for the classifier.\n",
    "            y (np.array): Output data of the classifier.\n",
    "            verbose (bool, optional): Show optimal SVM parameters. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the best SVM parameters for a \n",
    "        param_grid = {\n",
    "            \"C\": loguniform(1e-1, 1e5),\n",
    "            \"gamma\": loguniform(1e-7, 1e2),\n",
    "        }\n",
    "        clf = RandomizedSearchCV(\n",
    "            SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=500\n",
    "        )\n",
    "        clf = clf.fit(X, y)\n",
    "        if verbose:\n",
    "            print(clf.best_estimator_)\n",
    "\n",
    "        # Save the best model\n",
    "        self.model = clf.best_estimator_\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate the workings of the classifier. First, we transform the input data to obtain their features, after which we perform a train-test-split to have a validation set to check the results of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:28:55.145131Z",
     "iopub.status.busy": "2023-04-12T14:28:55.144454Z",
     "iopub.status.idle": "2023-04-12T14:28:58.346238Z",
     "shell.execute_reply": "2023-04-12T14:28:58.344672Z",
     "shell.execute_reply.started": "2023-04-12T14:28:55.145091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a PCA extractor\n",
    "n_components = 40\n",
    "pca = PCAFeatureExtractor(n_components = n_components, use_color=True)\n",
    "pca.fit(train_X)\n",
    "# Obtain feature representations\n",
    "train_X_pca = pca.transform(train_X)\n",
    "# Perform train test split\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    train_X_pca, train_y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the above training data to fit the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:01.897549Z",
     "iopub.status.busy": "2023-04-12T14:29:01.897174Z",
     "iopub.status.idle": "2023-04-12T14:29:06.357335Z",
     "shell.execute_reply": "2023-04-12T14:29:06.355757Z",
     "shell.execute_reply.started": "2023-04-12T14:29:01.897515Z"
    }
   },
   "outputs": [],
   "source": [
    "svm_pca_classifier = SVM_PCA_classifier()\n",
    "svm_pca_classifier.fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can check the results by looking at the performance on the hold-out set. Below we see the results on the PCA features in the form of a **confusion matrix**. In all of our results we note the confusion matrix rather than just the accuracy as in a classification task, accuracy can be misleading for imbalanced classes. Rather we look at the f1-score, which is the average of the precision and recall. In our case, the training data that we will be using our classification methods on have balanced classes, but the confusion matrix allows us to see the classification f1-score on each class individually. This will allow us to determine if our features or our model are more distinguishable on a specific class.\n",
    "\n",
    "Below we see that the SVM Classifier on the PCA features have a fairly evenly distributed f1-score between all 3 classes. The average f1-score on this classifier is .82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:18.061547Z",
     "iopub.status.busy": "2023-04-12T14:29:18.061191Z",
     "iopub.status.idle": "2023-04-12T14:29:18.370346Z",
     "shell.execute_reply": "2023-04-12T14:29:18.369380Z",
     "shell.execute_reply.started": "2023-04-12T14:29:18.061513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make the predictions on the test set (validation set)\n",
    "pred_y = svm_pca_classifier.predict(X_test_pca)\n",
    "\n",
    "# Print a classification report and show confusion matrix\n",
    "cr = classification_report(y_test_pca, pred_y)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"SVM with PCA\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SVM Classifier based on SIFT Features\n",
    "\n",
    "Similarly, we use an SVM Classifier on the handcrafted SIFT Features.\n",
    "\n",
    "In order to perform classification on the SIFT Features, we must first create a 'bag of visual words' model. In this scenario, we use K-means clustering to form clusters of features that are distinguishable from each other. The visual features are analogous to words as with this model we create a dictionary where we assign one word to each cluster. Thus each distinguishable feature should be mapped to a specific cluster which will ideally allow the SVM model to group clusters together according to who they identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:26.291784Z",
     "iopub.status.busy": "2023-04-12T14:29:26.291259Z",
     "iopub.status.idle": "2023-04-12T14:29:26.303361Z",
     "shell.execute_reply": "2023-04-12T14:29:26.302260Z",
     "shell.execute_reply.started": "2023-04-12T14:29:26.291744Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM_SIFT_Classifier:\n",
    "    \"\"\"\n",
    "    SVM classifier using the SIFT features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        Args:\n",
    "            k (int): Hyperparameter for mini batch k means.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.hist = None\n",
    "        self.clp = None\n",
    "        self.kmeans = None\n",
    "\n",
    "    def flatten(self, arr):\n",
    "        \"\"\"\n",
    "        Flatten descriptors to two dimensions\n",
    "        Args:\n",
    "            arr (np.array): The SIFT descriptors\n",
    "        \"\"\"\n",
    "        all_descriptors = []\n",
    "        for img_descriptors in arr:\n",
    "            # get each feature from image\n",
    "            for descriptor in img_descriptors:\n",
    "                all_descriptors.append(descriptor)\n",
    "        all_descriptors = np.stack(all_descriptors)\n",
    "        return all_descriptors\n",
    "\n",
    "    def fit(self, desc):\n",
    "        \"\"\"\n",
    "        Fit data with mini batch k means\n",
    "        Args:\n",
    "            desc (np.array): SIFT descriptors\n",
    "        \"\"\"\n",
    "        flattened_desc = self.flatten(desc)\n",
    "        kmeans = MiniBatchKMeans(n_clusters=self.k, batch_size=20).fit(flattened_desc)\n",
    "        self.kmeans = kmeans\n",
    "\n",
    "    def histogram(self, all_kp, all_desc):\n",
    "        \"\"\"\n",
    "        Cluster data into histograms based on prediction.\n",
    "        Args:\n",
    "            all_kp (np.array): All keypoints for SIFT\n",
    "            all_desc (np.array): All descriptors for SIFT\n",
    "        \"\"\"\n",
    "        hist = []\n",
    "        for i in range(len(all_desc)):\n",
    "            hist_i = np.zeros(self.k)\n",
    "            nkp = np.size(all_kp[i])\n",
    "\n",
    "            for d in all_desc[i]:\n",
    "                idx = self.kmeans.predict([d])\n",
    "                hist_i[idx] +=1/nkp\n",
    "            hist.append(hist_i)\n",
    "        self.hist = hist\n",
    "        return hist\n",
    "\n",
    "    def train(self, train_hist, train_y, verbose = True):\n",
    "        \"\"\"\n",
    "        Train the classifier\n",
    "        Args:\n",
    "            train_hist (np.array): Histogram\n",
    "            train_y (np.array): Targets\n",
    "            verbose (bool, optional): Print progress. Defaults to False.\n",
    "        \"\"\"\n",
    "        # Perform grid search to find optimal SVM parameters\n",
    "        param_grid = {\n",
    "            \"C\": loguniform(1e-1, 1e5),\n",
    "            \"gamma\": loguniform(1e-7, 1e2),\n",
    "        }\n",
    "        clf = RandomizedSearchCV(\n",
    "            SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=500)\n",
    "        start = time.time()\n",
    "        clf = clf.fit(np.array(train_hist), train_y)\n",
    "        end = time.time()\n",
    "        if verbose:\n",
    "            print(\"Done in %0.3fs\" % (end - start))\n",
    "            print(\"Best estimator found by grid search:\")\n",
    "            print(clf.best_estimator_)\n",
    "        # Save best classifier\n",
    "        self.clf = clf\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we first collect the SIFT Features for our classification problem and perform a train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:31.442920Z",
     "iopub.status.busy": "2023-04-12T14:29:31.442174Z",
     "iopub.status.idle": "2023-04-12T14:29:36.953001Z",
     "shell.execute_reply": "2023-04-12T14:29:36.951780Z",
     "shell.execute_reply.started": "2023-04-12T14:29:31.442881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create instance of the classifier\n",
    "classifier = SVM_SIFT_Classifier(10)\n",
    "\n",
    "# split training data\n",
    "X_train_sift, X_test_sift, y_train_sift, y_test_sift = train_test_split(\n",
    "    train_X, train_y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# get descriptors and keypoints of train data\n",
    "train_des, train_kp = SIFTExtractor.detect_and_compute(X_train_sift)\n",
    "classifier.fit(train_des)\n",
    "# cluster training points by their keypoints\n",
    "train_hist = classifier.histogram(train_kp, train_des)\n",
    "# train\n",
    "clf = classifier.train(train_hist, y_train_sift)\n",
    "\n",
    "# get desscriptors and keypoints of test data\n",
    "test_des, test_kp = SIFTExtractor.detect_and_compute(X_test_sift)\n",
    "# cluster test data by keypoints\n",
    "test_hist = classifier.histogram(test_kp, test_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now again turn to the prediction stage. We note that the SIFT Features result in lower performance with SVM than the PCA Features. The average F1-score is .5\n",
    "\n",
    "This lower score can either be attributed to the fact that an SVM classsifier may not be the best model for these features, or that the SIFT Features are less distinguishable than the PCA Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:39.538831Z",
     "iopub.status.busy": "2023-04-12T14:29:39.538070Z",
     "iopub.status.idle": "2023-04-12T14:29:39.809503Z",
     "shell.execute_reply": "2023-04-12T14:29:39.808513Z",
     "shell.execute_reply.started": "2023-04-12T14:29:39.538785Z"
    },
    "papermill": {
     "duration": 0.108542,
     "end_time": "2021-03-08T07:59:09.525054",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.416512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "pred_y = classifier.clf.predict(test_hist)\n",
    "\n",
    "# Show report\n",
    "cr = classification_report(y_test_sift, pred_y)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_sift, pred_y, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"SVM with SIFT\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 RandomForest Classifer\n",
    "\n",
    "Next we try a random forest classifier from sklearn. SVM Classifiers are more performant on binary classification tasks. Since our problem is multi-task, we expect the RandomForest Classifier to perform better.\n",
    "\n",
    "A random forest classifier is an ensemble method of classification such that it combines predictions from multiple decision trees. Each decision tree uses a random subset of the training data and features, hence the name 'random' forest. In this way, the random forest classifier should account for overfitting and lead to better performance especially on small datasets where overfitting is common.\n",
    "\n",
    "We tune the `max_depth` parameter in order to obtain the best results for our data. Selecting a large number can lead to overfitting, while a small number can lead to underfitting.\n",
    "\n",
    "Afterwards, we use the random forest classifier for both the SIFT and PCA feature representations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:29:41.522127Z",
     "iopub.status.busy": "2023-04-12T14:29:41.521457Z",
     "iopub.status.idle": "2023-04-12T14:29:41.528356Z",
     "shell.execute_reply": "2023-04-12T14:29:41.527104Z",
     "shell.execute_reply.started": "2023-04-12T14:29:41.522087Z"
    }
   },
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = RandomForestClassifier(max_depth=6, random_state=1)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 RandomForestClassifier based on SIFT Features\n",
    "\n",
    "We start by modeling the classier on the SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:30:34.869952Z",
     "iopub.status.busy": "2023-04-12T14:30:34.869557Z",
     "iopub.status.idle": "2023-04-12T14:30:34.991860Z",
     "shell.execute_reply": "2023-04-12T14:30:34.990880Z",
     "shell.execute_reply.started": "2023-04-12T14:30:34.869919Z"
    }
   },
   "outputs": [],
   "source": [
    "sift_search_classifier = BestModel()\n",
    "sift_search_classifier.fit(np.array(train_hist), y_train_sift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the predictions that follow, we note that the average f1-score did not improve but the metrics for Class 1 and Class 2 improved. These are the classes for Jesse Eisenberg and Mila Kunis which are the two classes that we wish to most accurately discriminate. However the lower score on Class 0 means that the random forest classsifier classifies 'lookalikes' less accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:30:45.716629Z",
     "iopub.status.busy": "2023-04-12T14:30:45.715965Z",
     "iopub.status.idle": "2023-04-12T14:30:46.092834Z",
     "shell.execute_reply": "2023-04-12T14:30:46.091349Z",
     "shell.execute_reply.started": "2023-04-12T14:30:45.716568Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = sift_search_classifier.predict(test_hist)\n",
    "\n",
    "# Show report\n",
    "cr = classification_report(y_test_sift, pred_y)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_sift, pred_y, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"random forest with SIFT\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Random Forest Classifer based on PCA features\n",
    "\n",
    "Next we use the classifier on the PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:30:59.197916Z",
     "iopub.status.busy": "2023-04-12T14:30:59.197546Z",
     "iopub.status.idle": "2023-04-12T14:30:59.321972Z",
     "shell.execute_reply": "2023-04-12T14:30:59.320975Z",
     "shell.execute_reply.started": "2023-04-12T14:30:59.197883Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_classifier = BestModel()\n",
    "aml = pca_classifier.fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to SVM, our RandomForest classifer performs slighlty worse. Additionally, with the random forest classifier we still observe that the PCA classifier performs better than the SIFT classifier. Thus it can be concluded that the PCA features are more discriminative than the SIFT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:31:09.920406Z",
     "iopub.status.busy": "2023-04-12T14:31:09.919449Z",
     "iopub.status.idle": "2023-04-12T14:31:10.274600Z",
     "shell.execute_reply": "2023-04-12T14:31:10.273656Z",
     "shell.execute_reply.started": "2023-04-12T14:31:09.920367Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = pca_classifier.model.predict(X_test_pca)\n",
    "\n",
    "# Show report\n",
    "cr = classification_report(y_test_pca, pred_y)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"random forest with PCA\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training dataset is so small with only 85 examples, we believe our classifier may be dropping in performance due to overfitting. Another method to prevent overfitting is cross validation. Cross validation splits the datatset into partitions and performs a train_test split on each partition and takes an average of all of the results. We note that cross validation improves the performance of the RandomForest Classifier on PCA features. The average f1-score seen below is .85 Cross validation was also experiemnted on the SIFT features, however it did not affect the scores so it is not shown in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:33:42.936270Z",
     "iopub.status.busy": "2023-04-12T14:33:42.935809Z",
     "iopub.status.idle": "2023-04-12T14:33:43.576559Z",
     "shell.execute_reply": "2023-04-12T14:33:43.575552Z",
     "shell.execute_reply.started": "2023-04-12T14:33:42.936230Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pca_classifier.model, train_X_pca, train_y, cv=3)\n",
    "cr = classification_report(train_y, y_pred)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(train_y, y_pred, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"random forest with PCA, cross-validation\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Ensemble Classification\n",
    "\n",
    "A final method to improve on our SVM classification methods discussed above is to combine the predictions from both the SIFT classifier and the PCA classifier in a so-called **ensemble method**. The SIFT and PCA features may highlight different identifiers for each person. Thus creating this ensemble classifier allows us to combine both feature types to improve on our classification. Our ensemble classifier results in an f1-score of .73\n",
    "\n",
    "From the results that follow, we see that our ensemble classifier performs the best on class 2. This means that the PCA and SIFT classifiers agreed on the most images of Mila Kunis. While the ensemble classifier performs the worst on class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:33:54.278905Z",
     "iopub.status.busy": "2023-04-12T14:33:54.277975Z",
     "iopub.status.idle": "2023-04-12T14:33:54.569639Z",
     "shell.execute_reply": "2023-04-12T14:33:54.568530Z",
     "shell.execute_reply.started": "2023-04-12T14:33:54.278852Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_proba_sift = sift_search_classifier.model.predict_proba(np.array(test_hist))\n",
    "pred_proba_pca = pca_classifier.model.predict_proba(X_test_pca)\n",
    "\n",
    "combined_pred_proba = (pred_proba_pca + pred_proba_sift) /2\n",
    "combined_pred = np.argmax(combined_pred_proba, axis=1)\n",
    "\n",
    "cr = classification_report(y_test_pca, combined_pred)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_pca, combined_pred, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"ensemble method\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Simple feedforward MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we turn our attention to the final and most effective classifier, the VGG network, we discuss a simple neural network architecture as build-up. That is, we use a fully-connected (dense) **feedforward multilayer perceptron** (MLP), which takes our handcrafted features as input features, and propagates them through a few hidden layers to compute an output. In our case, the output will consist of three nodes that represent the probabilities that a given feature representation corresponds to the given classes 0, 1, 2. We will demonstrate the inner workings of neural networks using the PyTorch library. For our purpose, one can choose whether to convert the targets, *i.e.* the class labels, to 3D vectors for this purpose using the auxiliary function defined below, although PyTorch is able to handle both representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:33:59.366128Z",
     "iopub.status.busy": "2023-04-12T14:33:59.365392Z",
     "iopub.status.idle": "2023-04-12T14:33:59.372110Z",
     "shell.execute_reply": "2023-04-12T14:33:59.371056Z",
     "shell.execute_reply.started": "2023-04-12T14:33:59.366089Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vector_data(labels: np.array, size: int=3):\n",
    "    \"\"\"\n",
    "    Turns an array of class labels into an array of 3D vectors representing probabilities.\n",
    "    Args:\n",
    "        labels (np.array): Class labels (target data)\n",
    "        size (int, optional): Number of different class labels. Defaults to 3 for our application.\n",
    "    Returns:\n",
    "        np.array: New labels, representing the distributions.\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        # Put zeroes everywhere\n",
    "        new_label = np.zeros(size)\n",
    "        # Put a one in the index location corresponding to the label\n",
    "        new_label[label] = 1\n",
    "        new_labels.append(new_label)\n",
    "        \n",
    "    return np.array(new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch library allows user to create their own datasets to process data in PyTorch networks. Hence, we define a custom dataset class that is specifically catered towards our face recognition application. Note that, when loading in the data, we normalize the input data, which is good practice especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:26.756412Z",
     "iopub.status.busy": "2023-04-12T14:34:26.756049Z",
     "iopub.status.idle": "2023-04-12T14:34:26.765840Z",
     "shell.execute_reply": "2023-04-12T14:34:26.764541Z",
     "shell.execute_reply.started": "2023-04-12T14:34:26.756378Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom data set to process the input features and output labels of our data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features: np.array, labels: np.array, to_vector: bool = False, normalization_function: Callable = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the class by separating data into features and labels.\n",
    "        Args:\n",
    "            features (np.array): Array containing the input data.\n",
    "            labels (np.array): Array containing the output data.\n",
    "            to_vector (bool, optional): Indicate whether we train with class labels as targets or convert to probabilities. Defaults to False.\n",
    "            normalization_function (Callable, optional): Function that normalizes the input features, for instance with scikit-learn. Defaults to None (no normalization performed).\n",
    "        \"\"\"\n",
    "        # Normalize input data if given a normalization function to perform\n",
    "        if normalization_function is not None:\n",
    "            features = normalization_function(features)\n",
    "\n",
    "        # Convert features to Torch tensors for PyTorch\n",
    "        features = torch.from_numpy(features)\n",
    "        # Turn into probabilities if desired (not recommended according to PyTorch docs)\n",
    "        if to_vector:\n",
    "            labels = create_vector_data(labels)\n",
    "        # Convert labels to Torch tensors\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        # Save as instance variables to the dataloader\n",
    "        self.features = features\n",
    "        self.labels   = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Gets an item from the dataset based on a specified index.\n",
    "        Args:\n",
    "            idx: (int): Index used to fetch the item.\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Tuple of feature and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Get the feature, but normalized\n",
    "        feature = self.features[idx]\n",
    "\n",
    "        # Get the label\n",
    "        label = self.labels[idx]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we will use the features extracted by a PCA analysis (see above for the extraction and train test split). As already mentioned, we will normalize the data through scikit-learn's StandardScaler. We fit the scaler on the training data and use the same transformation to scale the test data. Normalizing the input features is crucial to ensure that the network trains efficiently and to improve the stability of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:39.831461Z",
     "iopub.status.busy": "2023-04-12T14:34:39.831054Z",
     "iopub.status.idle": "2023-04-12T14:34:39.849039Z",
     "shell.execute_reply": "2023-04-12T14:34:39.848042Z",
     "shell.execute_reply.started": "2023-04-12T14:34:39.831419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# Convert to PyTorch Datasets as we defined them, normalize with same transformation as on training data\n",
    "train_dataset = CustomDataset(X_train_pca, y_train_pca, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = CustomDataset(X_test_pca, y_test_pca, normalization_function = scaler.transform)\n",
    "# Then we create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define our architecture. The architecture can easily accomodate for various setups of the hidden layers and their sizes. The user can also specify different activation functions, with the default one being the rectified linear unit (ReLU), which is often used. The output of the network uses a **softmax layer**, which models a multinomial probability distribution and is hence the ideal activation function for a multiclass classification problems. Applying an argmax operation to the output of the network can turn these probabilities into a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:42.109397Z",
     "iopub.status.busy": "2023-04-12T14:34:42.109042Z",
     "iopub.status.idle": "2023-04-12T14:34:42.120876Z",
     "shell.execute_reply": "2023-04-12T14:34:42.119882Z",
     "shell.execute_reply.started": "2023-04-12T14:34:42.109364Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network for our multiclass classification problem of face recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_of_inputs: int, nb_of_outputs: int = 3, h: list = [200, 200, 200], activation_function: Callable = torch.nn.ReLU) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        Args:\n",
    "            nb_of_inputs (int): Number of input nodes for the network, i.e. the number of features.\n",
    "            nb_of_outputs (int, optional): Number of output nodes for the softmax layer. Defaults to 3.\n",
    "            h (list, optional): List representing the setup of the hidden layers by specifying the number of hidden neurons per layer. Defaults to [200, 200, 200].\n",
    "            activation_function (Callable, optional): Activation function for the hidden layers. Defaults to the ReLU activation function.  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.h = h\n",
    "        # Call the super constructor first\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        \n",
    "        # Add visible layers as well to get all layer sizes\n",
    "        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n",
    "\n",
    "        # Define the layers:\n",
    "        for i in range(len(self.h_augmented)-1):\n",
    "            if i == len(self.h_augmented)-2:\n",
    "                # For the final output layer, apply softmax\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=False))\n",
    "                setattr(self, f\"softmax\", nn.Softmax())  \n",
    "            else:\n",
    "                # For intermediate layers, apply the specified activation function\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n",
    "                setattr(self, f\"activation{i+1}\", activation_function())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a single forward step given the input x.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input for the neural network.\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            # The first module is the whole network, so continue\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # For each of our defined layers, \"apply\" the layer\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the architecture is defined, we will define a **train loop**, in wich the neural networks' parameters get adjusted, and a **test loop**, where the weights are frozen and we compute the loss on the train and test set to monitor the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:43.673304Z",
     "iopub.status.busy": "2023-04-12T14:34:43.672946Z",
     "iopub.status.idle": "2023-04-12T14:34:43.681770Z",
     "shell.execute_reply": "2023-04-12T14:34:43.680601Z",
     "shell.execute_reply.started": "2023-04-12T14:34:43.673272Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader: DataLoader, model: FeedForwardNet, loss_fn: Callable, optimizer: Callable) -> None:\n",
    "    \"\"\"\n",
    "    Does one epoch of the training loop.\n",
    "    Args:\n",
    "        dataloader (DataLoader): Torch DataLoader object, containing training data.\n",
    "        model (FeedForwardNet): An instance of our neural network.\n",
    "        loss_fn (Callable): The loss function used during training.\n",
    "        optimizer (Callable): The optimizer used during training.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    # Go over the data in the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction \n",
    "        prediction = model(X)\n",
    "        # Compute loss \n",
    "        loss = loss_fn(prediction, y)\n",
    "\n",
    "        # Do the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader: DataLoader, model: FeedForwardNet, loss_fn: Callable) -> float:\n",
    "    \"\"\"\n",
    "    The testing loop to compute the loss.\n",
    "    Args:\n",
    "        dataloader (DataLoader): Torch DataLoader object, containing training data.\n",
    "        model (FeedForwardNet): An instance of our neural network.\n",
    "        loss_fn (Callable): The loss function used during training.\n",
    "        optimizer (Callable): The optimizer used during training.\n",
    "    Returns:\n",
    "        float: Loss computed on the provided data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    # Initialize loss\n",
    "    test_loss = 0\n",
    "\n",
    "    # Predict and compute loss, add to total loss\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            prediction = model(X)\n",
    "            test_loss += loss_fn(prediction, y).item()\n",
    "\n",
    "    # Return the average of the loss over the number of batches\n",
    "    return test_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data can disrupt the training and effectiveness of the classifier. While the training data is not too imbalanced, we will take the class distributions into account when training the MLP, since that will be easy to achieve with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:44.936474Z",
     "iopub.status.busy": "2023-04-12T14:34:44.935883Z",
     "iopub.status.idle": "2023-04-12T14:34:45.019366Z",
     "shell.execute_reply": "2023-04-12T14:34:45.018221Z",
     "shell.execute_reply.started": "2023-04-12T14:34:44.936434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get class distributions of the training data\n",
    "counts = np.array([len(train_improved[train_improved[\"class\"] == i]) for i in [0,1,2]])\n",
    "probabilities = counts/np.sum(counts)\n",
    "probabilities = torch.from_numpy(probabilities)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a class which unites all training aspects and also saves the losses obtained during training to conveniently plot the training afterwards. The train method implemented in this class also uses **early stopping**, a commonly used regularization method, and implements a simple DIY adaptive learning rate scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:46.331661Z",
     "iopub.status.busy": "2023-04-12T14:34:46.330732Z",
     "iopub.status.idle": "2023-04-12T14:34:46.347317Z",
     "shell.execute_reply": "2023-04-12T14:34:46.346262Z",
     "shell.execute_reply.started": "2023-04-12T14:34:46.331609Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class that implements all training aspects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: FeedForwardNet, learning_rate: float, train_dataloader: DataLoader, test_dataloader: DataLoader, \n",
    "                 loss_fn: Callable, optimizer: Callable) -> None:\n",
    "        \"\"\"\n",
    "        Initialization of class.\n",
    "        Args:\n",
    "            model (FeedForwardNet): An instance of our network.\n",
    "            learning_rate (float): Initial learning rate for the network.\n",
    "            train_dataloader (DataLoader): DataLoader for the training data.\n",
    "            test_dataloader (DataLoader): DataLoader for the test data.\n",
    "            loss_fn (Callable): The loss function used during training.\n",
    "            optimizer (Callable): The optimizer used during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # Save everything as class variables\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.model = model\n",
    "        self.best_model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        # Create empty lists to store the train and test lossees\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        \n",
    "\n",
    "    def train(self, number_of_epochs: int = 500, patience: int = 10, patience_delta: float = 0.01, \n",
    "              adaptation_threshold: float = 0.9995, adaptation_multiplier: float = 0.5, verbose: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Train the network using early stopping and with an adaptive learning rate.\n",
    "        Args:\n",
    "            number_of_epochs (int, optional): Maximum number of epochs to train the network. Defaults to 500.\n",
    "            patience (int, optional): Number of epochs before we exit training due to early stopping criterion. Defaults to 10.\n",
    "            patience_delta (float, optional): Threshold when comparing test loss for early stopping criterion. Defaults to 0.01.\n",
    "            adaptation_threshold (float, optional): Threshold for the adaptive learning rate. Defaults to 0.9995.\n",
    "            adaptation_multiplier (float, optional): Multiplier reducing the learning rate if desired. Defaults to 0.5.\n",
    "            verbose (bool, optional): Print epochs and losses during training. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        best_loss = np.inf\n",
    "        # Counter for early stopping will have a \"burn in\" period\n",
    "        stopping_counter = -10\n",
    "\n",
    "        # Epoch counter for this specific training session\n",
    "        epoch_counter = 0  \n",
    "        # In case we continue training, the total number of epochs depends on train losses already saved\n",
    "        total_epoch_counter = len(self.train_losses) + 1 \n",
    "\n",
    "        # The counter makes sure we do not update the learning rate too often\n",
    "        # if the network was not trained yet, first 5 epochs we don't change the learning rate\n",
    "        if len(self.train_losses) == 0:\n",
    "            counter = -5\n",
    "        else:\n",
    "            counter = 0\n",
    "\n",
    "        # Keep on continuing the training until we hit max number of epochs\n",
    "        while epoch_counter < number_of_epochs:\n",
    "            # Train the network\n",
    "            train_loop(self.train_dataloader, self.model, self.loss_fn, self.optimizer)\n",
    "            # Test on the training data to get performance\n",
    "            average_train_loss = test_loop(self.train_dataloader, self.model, self.loss_fn)\n",
    "            self.train_losses.append(average_train_loss)\n",
    "            # Test on testing data to get performance\n",
    "            average_test_loss = test_loop(self.test_dataloader, self.model, self.loss_fn)\n",
    "            self.test_losses.append(average_test_loss)\n",
    "            \n",
    "            # Print progress (if desired)\n",
    "            if verbose:\n",
    "                print(f\"--- Epoch {epoch_counter} ---\")\n",
    "                print(f\"Train loss: {average_train_loss}\")\n",
    "                print(f\"Test   loss: {average_test_loss}\")\n",
    "\n",
    "            ## Adaptive learning rate\n",
    "            # Adapt the learning rate after 10 epochs (burn-in period)\n",
    "            if counter >= 10:\n",
    "                # Compare previous and recent train losses\n",
    "                current = np.min(self.train_losses[-5:])\n",
    "                previous = np.min(self.train_losses[-10:-5])\n",
    "\n",
    "                # If we did not improve the test loss sufficiently, adapt learning rate\n",
    "                if current / previous >= adaptation_threshold:\n",
    "                    # Reset counter (note: we will increment later, so set to -1 st it becomes 0)\n",
    "                    counter = -1\n",
    "                    self.learning_rate = adaptation_multiplier * self.learning_rate\n",
    "                    # Change optimizer\n",
    "                    optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "                    self.optimizer = optimizer\n",
    "                    \n",
    "            ## Early stopping\n",
    "            # Check if the validation loss has improved\n",
    "            if average_test_loss < best_loss:\n",
    "                best_loss = average_test_loss\n",
    "                self.best_model = self.model\n",
    "                stopping_counter = 0\n",
    "            else:\n",
    "                # Perform early stopping if there was no improvement after a certain number of timesteps\n",
    "                if average_test_loss > best_loss + patience_delta:\n",
    "                    stopping_counter += 1\n",
    "                    if stopping_counter >= patience:\n",
    "                        print('Early stopping after {} epochs'.format(epoch_counter + 1))\n",
    "                        break\n",
    "\n",
    "            # Another epoch passed - increment overall counters\n",
    "            counter += 1\n",
    "            epoch_counter += 1\n",
    "\n",
    "        print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train an instance of our MLP classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:48.569999Z",
     "iopub.status.busy": "2023-04-12T14:34:48.569626Z",
     "iopub.status.idle": "2023-04-12T14:34:48.585381Z",
     "shell.execute_reply": "2023-04-12T14:34:48.583917Z",
     "shell.execute_reply.started": "2023-04-12T14:34:48.569958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define an instance of our network, make sure the input matches the number of PCA components\n",
    "net = FeedForwardNet(nb_of_inputs = pca.n_components, h=[100, 20]).double()\n",
    "print(net)\n",
    "# Initialize optimizer and Trainer\n",
    "learning_rate = 0.001\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=probabilities)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
    "trainer = Trainer(net, learning_rate, train_dataloader, test_dataloader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:34:51.022034Z",
     "iopub.status.busy": "2023-04-12T14:34:51.021674Z",
     "iopub.status.idle": "2023-04-12T14:35:17.986836Z",
     "shell.execute_reply": "2023-04-12T14:35:17.985649Z",
     "shell.execute_reply.started": "2023-04-12T14:34:51.022001Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train(number_of_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the losses to monitor the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:35:22.081369Z",
     "iopub.status.busy": "2023-04-12T14:35:22.080672Z",
     "iopub.status.idle": "2023-04-12T14:35:22.304555Z",
     "shell.execute_reply": "2023-04-12T14:35:22.303529Z",
     "shell.execute_reply.started": "2023-04-12T14:35:22.081329Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_PLOTS:\n",
    "    plt.plot([i+1 for i in range(len(trainer.train_losses))], trainer.train_losses, '-o', color=\"red\", label=\"Train\")\n",
    "    plt.plot([i+1 for i in range(len(trainer.test_losses))], trainer.test_losses, '-o', color=\"blue\", label=\"Test\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training of feedforward MLP\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can put everything into a classifier class for the prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:35:26.126549Z",
     "iopub.status.busy": "2023-04-12T14:35:26.126188Z",
     "iopub.status.idle": "2023-04-12T14:35:26.135208Z",
     "shell.execute_reply": "2023-04-12T14:35:26.133919Z",
     "shell.execute_reply.started": "2023-04-12T14:35:26.126516Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardClassifier:\n",
    "    \"\"\"\n",
    "    Implements a classifier with a simple feedforward MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net: FeedForwardNet, scaler: StandardScaler):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with a network and a normalizer.\n",
    "        Args:\n",
    "            net (FeedForwardNet): A trained feedforward MLP classifier with softmax output layer.\n",
    "            scaler (StandardScaler): StandardScaler which normalizes the input data.\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Classifier takes already fitted model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        # Disable torch's gradient (backpropagation) for predictions\n",
    "        with torch.no_grad():\n",
    "            # Perform normalization\n",
    "            X = self.scaler.transform(X)\n",
    "            # Make the prediction\n",
    "            y_pred = self.net(torch.from_numpy(X))\n",
    "            y_pred = torch.argmax(y_pred, axis=1).numpy()\n",
    "            # In case we have only a single value, convert array to int\n",
    "            if len(y_pred) == 1:\n",
    "                y_pred = y_pred[0]\n",
    "        return y_pred\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again analyze the performance with the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:35:28.501891Z",
     "iopub.status.busy": "2023-04-12T14:35:28.500880Z",
     "iopub.status.idle": "2023-04-12T14:35:28.790980Z",
     "shell.execute_reply": "2023-04-12T14:35:28.789841Z",
     "shell.execute_reply.started": "2023-04-12T14:35:28.501840Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = FeedForwardClassifier(net, scaler)\n",
    "# Make the predictions on the test set (validation set)\n",
    "pred_y = classifier(X_test_pca)\n",
    "\n",
    "# Print a classification report and show confusion matrix\n",
    "cr = classification_report(y_test_pca, pred_y)\n",
    "print(cr)\n",
    "if SHOW_PLOTS:\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n",
    "    # Add title and tidy up\n",
    "    classifier_name = \"MLP with PCA\"\n",
    "    plt.title(\"Confusion matrix for \" + classifier_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Baseline 2: Transfer learning CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - add discussion and documentation. **Note**, for actually using it, I suggest we skip the discussion on the train data etc and just show how to directly use it on the test data since we can just explain why it's going to improve upon the methods above to finish the whole notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pre-trained weights and the architecture of a neural network trained on face images. We append an additional layer to the network to classify the 4 classes of our dataset serving as feature extractor + classifier integrated in one model. We could also use the embeddings from the top layer of the network to calculate face similaries but we preffered to fine-tune the network and therefore perform transfer-learning based on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:10:15.635383Z",
     "iopub.status.busy": "2023-04-12T16:10:15.634781Z",
     "iopub.status.idle": "2023-04-12T16:10:15.646254Z",
     "shell.execute_reply": "2023-04-12T16:10:15.645203Z",
     "shell.execute_reply.started": "2023-04-12T16:10:15.635340Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGGFaceFeatureExtractorAndClassifier:\n",
    "    \"\"\"\n",
    "    Implements a feature extractor and classifier based on VGG\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load\n",
    "        # vgg = VGG16(include_top=False, input_shape=FACE_SIZE + (3,))\n",
    "        \n",
    "        # Load the same pretrained model we are using in previous steps of the pipeline\n",
    "        # so we do not have to re-download the weights\n",
    "        vgg = DeepFace.VGGFace.loadModel()\n",
    "\n",
    "        # vgg.trainable = False\n",
    "\n",
    "        # for layer in vgg.layers:\n",
    "        #     if layer.name in ['block5_conv1', 'block5_conv2', 'block5_conv3']:\n",
    "        #         layer.trainable = True\n",
    "\n",
    "        # Beware: a higher number of neurons would overfit on the training dataset\n",
    "        classifier = Dense(32, activation='relu')(vgg.layers[-1].output)\n",
    "        \n",
    "        # Modify top layers to our classification problem\n",
    "        output_prob = Dense(3, activation='softmax')(classifier)\n",
    "\n",
    "        # Save modified model with the new layers\n",
    "        self.model = Model(inputs=vgg.inputs, outputs=output_prob)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Use the same preprocessing as the pretrained model\n",
    "        X_train = vgg16_preprocess_input(X)\n",
    "\n",
    "        # Compile model and set a small learning rate (we resume where the pretrained model stopped and we dont want to overshoot on the weights)\n",
    "        self.model.compile(run_eagerly=True, optimizer=Adam(\n",
    "            learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                \"best_model\", save_best_only=True, monitor='val_sparse_categorical_accuracy', save_format='tf',\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Fit model, should converge from 6 to 8 epochs. It will stop automatically when it reaches 100% accuracy on the training set.\n",
    "        self.model.fit(X_train, y, epochs=8,\n",
    "                       batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(vgg16_preprocess_input(X), verbose=0)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an instance of the above network. We can print a summary that showcases the architecture behind the model (this output is a bit lengthy so we ignore it for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:10:15.649640Z",
     "iopub.status.busy": "2023-04-12T16:10:15.648523Z",
     "iopub.status.idle": "2023-04-12T16:10:17.218632Z",
     "shell.execute_reply": "2023-04-12T16:10:17.217594Z",
     "shell.execute_reply.started": "2023-04-12T16:10:15.649600Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_fe_cls = VGGFaceFeatureExtractorAndClassifier()\n",
    "## Remove comment to see architecture\n",
    "# vgg_fe_cls.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply **transfer learning**, we have to finetune this model on our training set such that it predicts the right classes. This is computationally very expensive, but we can use a GPU in Kaggle to speed up the process.\n",
    "\n",
    "*Note:* we can load in the preprocessed training data here, in case one wants to immediately see the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:10:17.221200Z",
     "iopub.status.busy": "2023-04-12T16:10:17.220915Z",
     "iopub.status.idle": "2023-04-12T16:10:17.790274Z",
     "shell.execute_reply": "2023-04-12T16:10:17.789119Z",
     "shell.execute_reply.started": "2023-04-12T16:10:17.221172Z"
    }
   },
   "outputs": [],
   "source": [
    "## load preprocessed data\n",
    "prep_path = '/kaggle/working/prepped_data/'\n",
    "if os.path.exists(prep_path):\n",
    "    print(\"Loading preprocessed data\")\n",
    "    train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n",
    "    train_y = np.load(os.path.join(prep_path, 'train_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:10:17.792479Z",
     "iopub.status.busy": "2023-04-12T16:10:17.792061Z",
     "iopub.status.idle": "2023-04-12T16:10:44.986214Z",
     "shell.execute_reply": "2023-04-12T16:10:44.985169Z",
     "shell.execute_reply.started": "2023-04-12T16:10:17.792431Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_fe_cls.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102942,
     "end_time": "2021-03-08T07:59:09.730342",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.6274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Experiments\n",
    "\n",
    "TODO - add documentation and discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. Example: basic pipeline\n",
    "\n",
    "A demonstration of the pipeline is shown for the feedforward MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `net` and `classifier` we defined above to create an instance of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = FeedForwardClassifier(net, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine this MLP classifier with the PCA feature extractor to predict on the test set using our final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = prediction_pipeline(pca, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CSV for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_csv(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.103853,
     "end_time": "2021-03-08T07:59:10.903341",
     "exception": false,
     "start_time": "2021-03-08T07:59:10.799488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Publishing best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - discuss the best results\n",
    "\n",
    "The best results were obtained with the VGG classifier. Since the architecture has its own preprocessing method, we do not rely on the `prediction_pipeline` function we defined above, but we slightly modify that for loop to work with the VGG classifier we imported and trained earlier on. To obtain the predictions on the test set, recall that we load in the preprocessed images we obtained with DeepFace, which is at the following file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:07:11.176517Z",
     "iopub.status.busy": "2023-04-12T16:07:11.175899Z",
     "iopub.status.idle": "2023-04-12T16:07:11.185536Z",
     "shell.execute_reply": "2023-04-12T16:07:11.184283Z",
     "shell.execute_reply.started": "2023-04-12T16:07:11.176478Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Loading preprocessed data from {test_prep_X_loc}\")\n",
    "print(os.path.exists(test_prep_X_loc))  # must evaluate to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T16:11:21.428872Z",
     "iopub.status.busy": "2023-04-12T16:11:21.427825Z",
     "iopub.status.idle": "2023-04-12T16:14:23.646267Z",
     "shell.execute_reply": "2023-04-12T16:14:23.645290Z",
     "shell.execute_reply.started": "2023-04-12T16:11:21.428817Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Path points to a directory where preprocessed test data is located\n",
    "y_pred = []\n",
    "\n",
    "# Avoid errors here:\n",
    "if os.path.exists(test_prep_X_loc):\n",
    "    # Get the number of files\n",
    "    nb_of_files = len(os.listdir(test_prep_X_loc))\n",
    "    for i in tqdm(range(nb_of_files)):\n",
    "        # Load the image\n",
    "        img = np.load(os.path.join(test_prep_X_loc, f\"test_{i}.npy\"))\n",
    "        # Convert from BGR to RGB (warning! cv2.cvtColor gives error)\n",
    "        #img = img[...,::-1]\n",
    "        # VGG wants a specific shape as input, reshape\n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        # Make the predictions: turn softmax into a prediction through argmax\n",
    "        softmax = vgg_fe_cls(img)\n",
    "        new_pred = np.argmax(softmax)\n",
    "        y_pred.append(new_pred)\n",
    "\n",
    "    os.path.exists(\"/kaggle/working\")\n",
    "    predictions_to_csv(y_pred, name=\"/kaggle/working/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.116655,
     "end_time": "2021-03-08T07:59:11.577703",
     "exception": false,
     "start_time": "2021-03-08T07:59:11.461048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Discussion\n",
    "\n",
    "In summary we contributed the following: \n",
    "* We enhanced and augmented the training dataset by manually cropping poorly cropped images or images that showed the wrong person.\n",
    "* We experimented with three preprocessing strategies, namely HAAR, MTCNN, and RetinaFace. Our final pipeline uses the RetinaFace preprocessing images because it combines face detection, landmark localization and face bounding box regression that result in the best accuracy in preprocessing faces.\n",
    "* We implemented SIFT to extract handcrafted features that are invariant to scale. We visualize these features and demonstrate how they are matched for facial recognition.\n",
    "* We implemented PCA to extract features and demonstrate how we can reconstruct an image with varying number of principal components.\n",
    "* We implemented various classification methods, namely SVM, RandomForest, and a FeedForwardNet on both SIFT and PCA features. We discover that the PCA features are more robust with a best average f1-score of .86. This may be due to the fact that the SIFT method extracts fewer good features, and thus may result in more variability during classification.\n",
    "* We also enhance our classifiers with cross validation and an ensemble method to classify data based on both SIFT and PCA features. These methods are especially important to tackle overfitting seen with such a small training dataset.\n",
    "* We used transfer learning to employ state of the art VGG Face Feature Extraction with results in 0.91 accuracy on the test data in the Kaggle competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
